{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximators\n",
    "\n",
    "> Models that approximate a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# import logging\n",
    "# logging_level = logging.DEBUG\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Dict, Literal\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# TODO: Merge with base from RL networks to avoid duplicaiotn of code\n",
    "\n",
    "class BaseModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def select_activation(activation):\n",
    "        \"\"\" Select the activation function based on input string \"\"\"\n",
    "        activation = activation.lower()  # Convert input to lowercase for consistency\n",
    "        if activation == \"relu\":\n",
    "            return nn.ReLU\n",
    "        elif activation == \"sigmoid\":\n",
    "            return nn.Sigmoid\n",
    "        elif activation == \"tanh\":\n",
    "            return nn.Tanh\n",
    "        elif activation == \"elu\":\n",
    "            return nn.ELU\n",
    "        elif activation == \"leakyrelu\":\n",
    "            return nn.LeakyReLU\n",
    "        elif activation == \"identity\":\n",
    "            return nn.Identity\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function {activation} not recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LinearModel(BaseModule):\n",
    "    \"\"\"Linear regression model\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "            input_size: int, # number of features\n",
    "            output_size: int, # number of outputs/actions\n",
    "            relu_output: bool = False): # whether to apply ReLU activation to the output\n",
    "        super().__init__()\n",
    "        self.l1=nn.Linear(input_size, output_size)\n",
    "        if relu_output:\n",
    "            self.final_activation = nn.ReLU()\n",
    "        else:\n",
    "            self.final_activation = nn.Identity()\n",
    "            \n",
    "    def forward(self,x):\n",
    "        out=self.l1(x)\n",
    "        out=self.final_activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLP(BaseModule):\n",
    "\n",
    "    \"\"\" Multilayer perceptron model \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                    input_size: int, # number of features\n",
    "                    output_size: int, # number of outputs/actions\n",
    "                    hidden_layers: list, # list of number of neurons in each hidden layer\n",
    "                    drop_prob: float = 0.0, # dropout probability\n",
    "                    batch_norm: bool = False, # whether to apply batch normalization\n",
    "                    relu_output: bool = False): # whether to apply ReLU activation to the output\n",
    "        super().__init__()\n",
    "\n",
    "        # List of layers\n",
    "        layers = []\n",
    "\n",
    "        last_size = input_size\n",
    "        for num_neurons in hidden_layers:\n",
    "            layers.append(nn.Linear(last_size, num_neurons))\n",
    "            layers.append(nn.ReLU())\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(num_neurons))\n",
    "            layers.append(nn.Dropout(p=drop_prob))\n",
    "            last_size = num_neurons\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(last_size, output_size))\n",
    "        if relu_output:\n",
    "            layers.append(nn.ReLU()) # output is non-negative\n",
    "        else:\n",
    "            layers.append(nn.Identity())\n",
    "\n",
    "        # Combine layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transformer(BaseModule):\n",
    "\n",
    "    \"\"\" Multilayer perceptron model \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                    input_size: int, # number of (time steps, features)\n",
    "                    output_size: int, # number of outputs/actions\n",
    "\n",
    "                    max_context_length: int = 128, #  maximum context lenght during inference\n",
    "                    n_layer: int = 3, # number of layers in the transformer\n",
    "                    n_head: int = 8, # number of heads per layer\n",
    "                    n_embd_per_head: int = 32, # number of embedding per head\n",
    "                    rope_scaling: Dict | None = None, # whether to use rope scaling, not implemented yet\n",
    "\n",
    "                    min_multiple = 256, # minimum multiple for neurons in the MLP block of the transformer\n",
    "                    gating = True, # Whether to apply the gating mechanism from the original Llama model (used in LagLlama)\n",
    "\n",
    "                    drop_prob: float = 0.0, # dropout probability\n",
    "                    final_activation: Literal[\"relu\", \"sigmoid\", \"tanh\", \"elu\", \"leakyrelu\", \"identity\"] = \"identity\" # final activation function\n",
    "                    ): # whether to apply ReLU activation to the output\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        block_size = max_context_length\n",
    "        input_size = input_size[1] # we only consider the number of features\n",
    "\n",
    "        self.param_proj = nn.Linear(n_embd_per_head * n_head, output_size) # final projection layer for output\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Linear(\n",
    "                    input_size, n_embd_per_head * n_head # Initial projection from input to embedding space\n",
    "                ),\n",
    "                h=nn.ModuleList([Block(n_embd_per_head, n_head, block_size, drop_prob, min_multiple = min_multiple, gating=gating) for _ in range(n_layer)]),\n",
    "                ln_f=RMSNorm(n_embd_per_head * n_head),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.final_activation = self.select_activation(final_activation)()\n",
    "\n",
    "        # not _init_weights used since we are using the default initialization.\n",
    "\n",
    "    def forward(    self,\n",
    "                    x: torch.Tensor,) -> torch.Tensor:\n",
    "\n",
    "        (B, T, C) = x.size()\n",
    "\n",
    "        x = self.transformer.wte(\n",
    "            x\n",
    "        )\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        output = self.param_proj(\n",
    "            x\n",
    "        ) \n",
    "\n",
    "        output = self.final_activation(output)\n",
    "\n",
    "        output = output[:, -1, :] # we use the last time dimension as the output\n",
    "             \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LlamaRotaryEmbedding(torch.nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Rotary positional embeddings (RoPE) based on https://arxiv.org/abs/2104.09864\n",
    "    Code following the implementation in https://github.com/time-series-foundation-models/lag-llama\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (\n",
    "            self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)\n",
    "        )\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings,\n",
    "            device=self.inv_freq.device,\n",
    "            dtype=torch.get_default_dtype(),\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(\n",
    "            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype\n",
    "        )\n",
    "\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\n",
    "            \"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False\n",
    "        )\n",
    "\n",
    "    def forward(self, device, dtype, seq_len=None):\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=device, dtype=dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, ...].to(dtype=dtype),\n",
    "            self.sin_cached[:, :, :seq_len, ...].to(dtype=dtype),\n",
    "        )\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    \"\"\" Causeal self-attention module\n",
    "    Based on the implementation in https://github.com/time-series-foundation-models/lag-llama,\n",
    "    without usage of kv_cache since we always make a prediction for only the next step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd_per_head, n_head, block_size, dropout) -> None:\n",
    "        super().__init__()\n",
    "        # query projections for all heads, but in a batch\n",
    "        self.q_proj = nn.Linear(\n",
    "            n_embd_per_head * n_head,\n",
    "            n_embd_per_head * n_head,\n",
    "            bias=False,\n",
    "        )\n",
    "        # key, value projections\n",
    "        self.kv_proj = nn.Linear(\n",
    "            n_embd_per_head * n_head,\n",
    "            2 * n_embd_per_head * n_head,\n",
    "            bias=False,\n",
    "        )\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(\n",
    "            n_embd_per_head * n_head,\n",
    "            n_embd_per_head * n_head,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.n_embd_per_head = n_embd_per_head\n",
    "        self.block_size = block_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.rope_scaling=None\n",
    "\n",
    "        self._init_rope()\n",
    "\n",
    "    def _init_rope(self):\n",
    "        if self.rope_scaling is None:\n",
    "            self.rotary_emb = LlamaRotaryEmbedding(\n",
    "                self.n_embd_per_head, max_position_embeddings=self.block_size\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\"RoPE scaling is not yet implemented\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q = self.q_proj(x)\n",
    "        k, v = self.kv_proj(x).split(self.n_embd_per_head * self.n_head, dim=2)\n",
    "\n",
    "        \n",
    "        k = k.view(B, -1, self.n_head, self.n_embd_per_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        q = q.view(B, -1, self.n_head, self.n_embd_per_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.n_head, self.n_embd_per_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "\n",
    "        if self.rotary_emb is not None:\n",
    "            cos, sin = self.rotary_emb(device=v.device, dtype=v.dtype, seq_len=T)\n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids=None)\n",
    "\n",
    "\n",
    "        y = F.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True\n",
    "        )\n",
    "        \n",
    "        # # debug\n",
    "        # if not torch.isfinite(y).all():\n",
    "        #     print(\"y is not finite\")\n",
    "        #     print(y)\n",
    "        #     print(q)\n",
    "        #     print(k)\n",
    "        #     print(v)\n",
    "\n",
    "        # re-assemble all head outputs side by side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "def find_multiple(n: int, k: int) -> int:\n",
    "    if n % k == 0:\n",
    "        return n\n",
    "    return n + k - (n % k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MLP_block(nn.Module):\n",
    "    def __init__(self, n_embd_per_head, n_head, min_multiple = 256, gating = True) -> None:\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * n_embd_per_head * n_head\n",
    "        n_hidden = int(2 * hidden_dim / 3)\n",
    "        self.gating = gating\n",
    "        \n",
    "        n_hidden = find_multiple(n_hidden, min_multiple)\n",
    "\n",
    "        self.c_fc1 = nn.Linear(\n",
    "            n_embd_per_head * n_head, n_hidden, bias=False\n",
    "        )\n",
    "        if gating:\n",
    "            self.c_fc2 = nn.Linear(\n",
    "                n_embd_per_head * n_head, n_hidden, bias=False\n",
    "            )\n",
    "        \n",
    "        self.c_proj = nn.Linear(\n",
    "            n_hidden, n_embd_per_head * n_head, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.gating:\n",
    "            x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n",
    "        else:\n",
    "            x = F.silu(self.c_fc1(x))\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization as implemented in https://github.com/time-series-foundation-models/lag-llama.\n",
    "\n",
    "    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
    "    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(size))\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        norm_x = x.to(torch.float32).pow(2).mean(dim=self.dim, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(norm_x + self.eps)\n",
    "        output = (self.scale * x_normed).type_as(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd_per_head, n_head, block_size, dropout, min_multiple = 256, gating=True) -> None:\n",
    "        super().__init__()\n",
    "        self.rms_1 = RMSNorm(n_embd_per_head * n_head)\n",
    "        self.attn = CausalSelfAttention(n_embd_per_head, n_head, block_size, dropout)\n",
    "        self.rms_2 = RMSNorm(n_embd_per_head * n_head)\n",
    "        self.mlp = MLP_block(n_embd_per_head, n_head, min_multiple = min_multiple, gating=gating)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.rms_1(x))\n",
    "        y = x + self.mlp(self.rms_2(x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
