{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML utils\n",
    "\n",
    "> Some helper functions for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.ml_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import  List, Tuple, Literal\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LRSchedulerPerStep():\n",
    "    \"\"\"\n",
    "    Learning rate scheduler from Attention is all you need paper (https://arxiv.org/abs/1706.03762)\n",
    "    One ajustment: Added base LR as tunable parameter rather than setting it automated based on model dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                optimizer: torch.optim.Optimizer, # Optimizer to adjust learning rate for\n",
    "                base_learning_rate: float = 0.0001,\n",
    "                warmup: int =4000):\n",
    "\n",
    "        # Ensure optimizer is a PyTorch optimizer\n",
    "        if not isinstance(optimizer, torch.optim.Optimizer):\n",
    "            raise ValueError('Optimizer must be a PyTorch optimizer')\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.basic = base_learning_rate\n",
    "        self.warm = warmup**-1.5\n",
    "        self.scaling_factor = 1/warmup**-0.5 # ensures that the peak realtive to the base lr is always 1\n",
    "\n",
    "        self.step_num = 0   \n",
    "        self.step()\n",
    "        \n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self.basic * self.scaling_factor * min(self.step_num**-0.5, self.step_num*self.warm)\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
