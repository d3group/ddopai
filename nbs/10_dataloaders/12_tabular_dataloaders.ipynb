{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular dataloaders\n",
    "\n",
    "> Dataloaders for tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataloaders.tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Tuple, List\n",
    "import pandas as pd\n",
    "\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XYDataLoader(BaseDataLoader):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    A class for datasets with the typicall X, Y structure. Both X\n",
    "    and Y are numpy arrays. X may be of shape (datapoints, features) or (datapoints, sequence_length, features) \n",
    "    if lag features are used. The prep_lag_features can be used to create those lag features. Y is of shape\n",
    "    (datapoints, units).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        val_index_start: Union[int, None] = None, \n",
    "        test_index_start: Union[int, None] = None, \n",
    "        lag_window_params: Union[dict] = None, # default: {'lag_window': 0, 'include_y': False, 'pre_calc': False}\n",
    "        normalize_features: Union[dict] = None, # default: {'normalize': True, 'ignore_one_hot': True}\n",
    "    ):\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.val_index_start = val_index_start\n",
    "        self.test_index_start = test_index_start\n",
    "\n",
    "        # train index ends either at the start of the validation set, the start of the test set or at the end of the dataset\n",
    "        if self.val_index_start is not None:\n",
    "            self.train_index_end = self.val_index_start-1\n",
    "        elif self.test_index_start is not None:\n",
    "            self.train_index_end = self.test_index_start-1\n",
    "        else:\n",
    "            self.train_index_end = len(Y)-1\n",
    "\n",
    "        self.dataset_type = \"train\"\n",
    "\n",
    "        normalize_features = normalize_features or {'normalize': True, 'ignore_one_hot': True}\n",
    "        lag_window_params = lag_window_params or {'lag_window': 0, 'include_y': False, 'pre_calc': False}\n",
    "\n",
    "        self.normalize_features(**normalize_features, initial_normalization=True)\n",
    "        self.prep_lag_features(**lag_window_params)\n",
    "\n",
    "        # X must at least have datapoint and feature dimension\n",
    "        if len(X.shape) == 1:\n",
    "            self.X = X.reshape(-1, 1)\n",
    "        \n",
    "        # Y must have at least datapoint and unit dimension (even if only one unit is present)\n",
    "        if len(Y.shape) == 1:\n",
    "            self.Y = Y.reshape(-1, 1)\n",
    "\n",
    "        assert len(X) == len(Y), 'X and Y must have the same length'\n",
    "\n",
    "        self.num_units = Y.shape[1] # shape 0 is alsways time, shape 1 is the number of units (e.g., SKUs)\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def normalize_features(self,\n",
    "        normalize: bool = True,\n",
    "        ignore_one_hot: bool = True,\n",
    "        initial_normalization=False # Flag if it is set before having added lag features\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Normalize features using a standard scaler. If ignore_one_hot is true, one-hot encoded features are not normalized.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if normalize:\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            if initial_normalization:\n",
    "\n",
    "                if len(self.X.shape) == 3:\n",
    "                    raise ValueError('Normalization not possible with lag features. Please set initial_normalization=False')\n",
    "            \n",
    "                scaler.fit(self.X[:self.train_index_end+1]) # +1 to include the last training point\n",
    "                scaler.transform(self.X)\n",
    "\n",
    "                if initial_normalization:\n",
    "                    return\n",
    "                else:\n",
    "                    raise NotImplementedError('Normalization after lag features have been set not implemented yet')\n",
    "\n",
    "                    # Idea:\n",
    "                        # remove time dimension\n",
    "                        # normalize features\n",
    "                        # add time_dimension back\n",
    "                    # Problem:\n",
    "                        # usage of prep_lag_features needs to ensure y is not added a second time\n",
    "\n",
    "    def prep_lag_features(self,\n",
    "        lag_window: int = 0, # length of the lage window\n",
    "        include_y: bool = False, # if lag demand shall be included as feature\n",
    "        pre_calc: bool = False # if all lags are pre-calculated for the entire dataset\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Create lag feature for the dataset. If \"inlcude_y\" is true, then a lag-1 of of the target variable is added as a feature.\n",
    "        If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag\n",
    "        window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features\n",
    "        including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce\n",
    "        computation time later on at the expense of increases memory usage. \n",
    "\n",
    "        \"\"\"\n",
    "        # to be discussed: Do we need option to only provide lag demand wihtout lag features?\n",
    "        self.lag_window = lag_window\n",
    "        self.pre_calc = pre_calc\n",
    "        self.include_y = include_y\n",
    "        \n",
    "        if self.pre_calc:\n",
    "            if self.include_y:\n",
    "                # add additional column to X with demand shifted by 1\n",
    "                self.X = np.concatenate((self.X, np.roll(self.Y, 1, axis=0)), axis=1)\n",
    "                self.X = self.X[1:] # remove first row\n",
    "                self.Y = self.Y[1:] # remove first row\n",
    "                \n",
    "                self.val_index_start = self.val_index_start-1\n",
    "                self.test_index_start = self.test_index_start-1\n",
    "                self.train_index_end  = self.train_index_end-1\n",
    "        \n",
    "            if self.lag_window is not None and self.lag_window > 0:\n",
    "\n",
    "                # add lag features as dimention 2 to X (making it dimension (datapoints, sequence_length, features))\n",
    "                X_lag = np.zeros((self.X.shape[0], self.lag_window+1, self.X.shape[1]))\n",
    "                for i in range(self.lag_window+1):\n",
    "                    if i == 0:\n",
    "                        features = self.X\n",
    "                    else:    \n",
    "                        features = self.X[:-i, :]\n",
    "                    X_lag[i:, self.lag_window-i, :] = features\n",
    "                self.X = X_lag[self.lag_window:]\n",
    "                self.Y = self.Y[self.lag_window:]\n",
    "\n",
    "                self.val_index_start = self.val_index_start-self.lag_window\n",
    "                self.test_index_start = self.test_index_start-self.lag_window\n",
    "                self.train_index_end  = self.train_index_end-self.lag_window\n",
    "\n",
    "        else:\n",
    "            self.lag_window = None\n",
    "            self.include_y = False\n",
    "            # add time dimension to X\n",
    "\n",
    "    def update_lag_features(self,\n",
    "        lag_window: int,\n",
    "        ):\n",
    "\n",
    "        \"\"\" Update lag window parameters for dataloader object that is already initialized \"\"\"\n",
    "\n",
    "        raise NotImplementedError('Not implemented yet')\n",
    "\n",
    "        # Problem: updating lag_features naively would shorten the dataset each time it is called\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "\n",
    "        \"\"\" get item by index, depending on the dataset type (train, val, test)\"\"\"\n",
    "\n",
    "        if self.dataset_type == \"train\":\n",
    "            if idx > self.train_index_end:\n",
    "                raise IndexError(f'index {idx} out of range{self.train_index_end}')\n",
    "            idx = idx\n",
    "\n",
    "        elif self.dataset_type == \"val\":\n",
    "            idx = idx + self.val_index_start\n",
    "            \n",
    "            if idx >= self.test_index_start:\n",
    "                raise IndexError(f'index{idx} out of range{self.test_index_start}')\n",
    "            \n",
    "        elif self.dataset_type == \"test\":\n",
    "            idx = idx + self.test_index_start\n",
    "            \n",
    "            if idx >= len(self.X):\n",
    "                raise IndexError(f'index{idx} out of range{len(self.X)}')\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('dataset_type not set')\n",
    "\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    @property\n",
    "    def X_shape(self):\n",
    "        return self.X.shape\n",
    "    \n",
    "    @property\n",
    "    def Y_shape(self):\n",
    "        return self.Y.shape\n",
    "\n",
    "    @property\n",
    "    def len_train(self):\n",
    "        return self.train_index_end+1\n",
    "\n",
    "    @property\n",
    "    def len_val(self):\n",
    "        if self.val_index_start is None:\n",
    "            raise ValueError('no validation set defined')\n",
    "        return self.test_index_start-self.val_index_start\n",
    "\n",
    "    @property\n",
    "    def len_test(self):\n",
    "        if self.test_index_start is None:\n",
    "            raise ValueError('no test set defined')\n",
    "        return len(self.Y)-self.test_index_start\n",
    "\n",
    "    def get_all_X(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        \"\"\"\n",
    "        Returns the entire features dataset.\n",
    "        Return either the train, val, test, or all data.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset_type == 'train':\n",
    "            return self.X[:self.val_index_start].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'val':\n",
    "            return self.X[self.val_index_start:self.test_index_start].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'test':\n",
    "            return self.X[self.test_index_start:].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'all':\n",
    "            return self.X.copy() if self.X is not None else None\n",
    "        else:\n",
    "            raise ValueError('dataset_type not recognized')\n",
    "\n",
    "    def get_all_Y(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        \"\"\"\n",
    "        Returns the entire target dataset.\n",
    "        Return either the train, val, test, or all data.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset_type == 'train':\n",
    "            return self.Y[:self.val_index_start].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'val':\n",
    "            return self.Y[self.val_index_start:self.test_index_start].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'test':\n",
    "            return self.Y[self.test_index_start:].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'all':\n",
    "            return self.Y.copy() if self.Y is not None else None\n",
    "        else:\n",
    "            raise ValueError('dataset_type not recognized')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## XYDataLoader\n",
       "\n",
       ">      XYDataLoader (X:numpy.ndarray, Y:numpy.ndarray,\n",
       ">                    val_index_start:Optional[int]=None,\n",
       ">                    test_index_start:Optional[int]=None,\n",
       ">                    lag_window_params:dict=None, normalize_features:dict=None)\n",
       "\n",
       "*A class for datasets with the typicall X, Y structure. Both X\n",
       "and Y are numpy arrays. X may be of shape (datapoints, features) or (datapoints, sequence_length, features) \n",
       "if lag features are used. The prep_lag_features can be used to create those lag features. Y is of shape\n",
       "(datapoints, units).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | ndarray |  |  |\n",
       "| Y | ndarray |  |  |\n",
       "| val_index_start | Optional | None |  |\n",
       "| test_index_start | Optional | None |  |\n",
       "| lag_window_params | dict | None | default: {'lag_window': 0, 'include_y': False, 'pre_calc': False} |\n",
       "| normalize_features | dict | None | default: {'normalize': True, 'ignore_one_hot': True} |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## XYDataLoader\n",
       "\n",
       ">      XYDataLoader (X:numpy.ndarray, Y:numpy.ndarray,\n",
       ">                    val_index_start:Optional[int]=None,\n",
       ">                    test_index_start:Optional[int]=None,\n",
       ">                    lag_window_params:dict=None, normalize_features:dict=None)\n",
       "\n",
       "*A class for datasets with the typicall X, Y structure. Both X\n",
       "and Y are numpy arrays. X may be of shape (datapoints, features) or (datapoints, sequence_length, features) \n",
       "if lag features are used. The prep_lag_features can be used to create those lag features. Y is of shape\n",
       "(datapoints, units).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | ndarray |  |  |\n",
       "| Y | ndarray |  |  |\n",
       "| val_index_start | Optional | None |  |\n",
       "| test_index_start | Optional | None |  |\n",
       "| lag_window_params | dict | None | default: {'lag_window': 0, 'include_y': False, 'pre_calc': False} |\n",
       "| normalize_features | dict | None | default: {'normalize': True, 'ignore_one_hot': True} |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L111){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.prep_lag_features\n",
       "\n",
       ">      XYDataLoader.prep_lag_features (lag_window:int=0, include_y:bool=False,\n",
       ">                                      pre_calc:bool=False)\n",
       "\n",
       "*Create lag feature for the dataset. If \"inlcude_y\" is true, then a lag-1 of of the target variable is added as a feature.\n",
       "If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag\n",
       "window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features\n",
       "including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce\n",
       "computation time later on at the expense of increases memory usage.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| lag_window | int | 0 | length of the lage window |\n",
       "| include_y | bool | False | if lag demand shall be included as feature |\n",
       "| pre_calc | bool | False | if all lags are pre-calculated for the entire dataset |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L111){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.prep_lag_features\n",
       "\n",
       ">      XYDataLoader.prep_lag_features (lag_window:int=0, include_y:bool=False,\n",
       ">                                      pre_calc:bool=False)\n",
       "\n",
       "*Create lag feature for the dataset. If \"inlcude_y\" is true, then a lag-1 of of the target variable is added as a feature.\n",
       "If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag\n",
       "window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features\n",
       "including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce\n",
       "computation time later on at the expense of increases memory usage.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| lag_window | int | 0 | length of the lage window |\n",
       "| include_y | bool | False | if lag demand shall be included as feature |\n",
       "| pre_calc | bool | False | if all lags are pre-calculated for the entire dataset |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.prep_lag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.__getitem__\n",
       "\n",
       ">      XYDataLoader.__getitem__ (idx)\n",
       "\n",
       "*get item by index, depending on the dataset type (train, val, test)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.__getitem__\n",
       "\n",
       ">      XYDataLoader.__getitem__ (idx)\n",
       "\n",
       "*get item by index, depending on the dataset type (train, val, test)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L226){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_X\n",
       "\n",
       ">      XYDataLoader.get_all_X (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire features dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L226){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_X\n",
       "\n",
       ">      XYDataLoader.get_all_X (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire features dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.get_all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L246){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_Y\n",
       "\n",
       ">      XYDataLoader.get_all_Y (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire target dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L246){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_Y\n",
       "\n",
       ">      XYDataLoader.get_all_Y (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire target dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.get_all_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of ```XYDataLoader``` for simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: [-1.63994308  0.13549824] [-3.38753308]\n",
      "sample shape Y: (1,)\n",
      "length: 100\n"
     ]
    }
   ],
   "source": [
    "X = np.random.standard_normal((100, 2))\n",
    "Y = np.random.standard_normal((100, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X = X, Y = Y)\n",
    "\n",
    "sample_X, sample_Y = dataloader[0]\n",
    "print(\"sample:\", sample_X, sample_Y)\n",
    "print(\"sample shape Y:\", sample_Y.shape)\n",
    "\n",
    "print(\"length:\", len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of ```XYDataLoader``` on how to handle train, val, and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train: 6 length val: 2 length test: 2\n",
      "\n",
      "### Data from train set ###\n",
      "idx: 0 data: [0.18776023 1.66839591] [4.28944588]\n",
      "idx: 1 data: [-0.27719155  0.40190162] [0.15596495]\n",
      "idx: 2 data: [-0.71553719  0.08537152] [-1.57621627]\n",
      "idx: 3 data: [ 1.25414844 -1.15060407] [-0.87212214]\n",
      "idx: 4 data: [-0.13077158 -0.64079997] [-3.36943747]\n",
      "idx: 5 data: [2.40752771 0.15625665] [4.71208213]\n",
      "\n",
      "### Data from val set ###\n",
      "idx: 0 data: [ 1.3429748  -0.38041163] [1.14371727]\n",
      "idx: 1 data: [-0.77508115 -0.79383888] [-3.29342162]\n",
      "\n",
      "### Data from test set ###\n",
      "idx: 0 data: [-1.56573895 -1.19523184] [-5.87876387]\n",
      "idx: 1 data: [0.08372153 1.31012091] [4.52378226]\n",
      "\n",
      "### Data from train set again ###\n",
      "idx: 0 data: [0.18776023 1.66839591] [4.28944588]\n",
      "idx: 1 data: [-0.27719155  0.40190162] [0.15596495]\n",
      "idx: 2 data: [-0.71553719  0.08537152] [-1.57621627]\n",
      "idx: 3 data: [ 1.25414844 -1.15060407] [-0.87212214]\n",
      "idx: 4 data: [-0.13077158 -0.64079997] [-3.36943747]\n",
      "idx: 5 data: [2.40752771 0.15625665] [4.71208213]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.standard_normal((10, 2))\n",
    "Y = np.random.standard_normal((10, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X = X, Y = Y, val_index_start=6, test_index_start=8)\n",
    "\n",
    "sample_X, sample_Y = dataloader[0]\n",
    "\n",
    "print(\"length train:\", dataloader.len_train, \"length val:\", dataloader.len_val, \"length test:\", dataloader.len_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.val()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from val set ###\")\n",
    "for i in range(dataloader.len_val):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.test()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from test set ###\")\n",
    "for i in range(dataloader.len_test):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.train()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set again ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.56573895, -1.19523184],\n",
       "       [ 0.08372153,  1.31012091]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "dataloader.get_all_X('all')\n",
    "dataloader.get_all_X('train')\n",
    "dataloader.get_all_X('val')\n",
    "dataloader.get_all_X('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.87876387],\n",
       "       [ 4.52378226]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "dataloader.get_all_Y('all')\n",
    "dataloader.get_all_Y('train')\n",
    "dataloader.get_all_Y('val')\n",
    "dataloader.get_all_Y('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of ```XYDataLoader``` on how to include lag features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train: 4 length val: 2 length test: 2\n",
      "\n",
      "### Data from train set ###\n",
      "idx: 0 data: [[ 0.12064842  0.92218769  3.22653523]\n",
      " [ 0.83660435 -0.88768222  1.2122358 ]] [-1.3309468]\n",
      "idx: 1 data: [[ 0.83660435 -0.88768222  1.2122358 ]\n",
      " [ 0.67280815  0.07891949 -1.3309468 ]] [2.32867388]\n",
      "idx: 2 data: [[ 0.67280815  0.07891949 -1.3309468 ]\n",
      " [-0.1513143   0.03342441  2.32867388]] [0.22463874]\n",
      "idx: 3 data: [[-0.1513143   0.03342441  2.32867388]\n",
      " [ 0.15557226 -0.02865607  0.22463874]] [0.88872031]\n",
      "\n",
      "### Data from val set ###\n",
      "idx: 0 data: [[ 0.15557226 -0.02865607  0.22463874]\n",
      " [-0.8775118  -0.59691176  0.88872031]] [-4.1931668]\n",
      "idx: 1 data: [[-0.8775118  -0.59691176  0.88872031]\n",
      " [-1.3087275  -1.47004448 -4.1931668 ]] [-6.13630354]\n",
      "\n",
      "### Data from test set ###\n",
      "idx: 0 data: [[-1.3087275  -1.47004448 -4.1931668 ]\n",
      " [-0.54701461 -0.89519827 -6.13630354]] [-4.72374999]\n",
      "idx: 1 data: [[-0.54701461 -0.89519827 -6.13630354]\n",
      " [-0.42618545  0.449891   -4.72374999]] [-0.30863742]\n",
      "\n",
      "### Data from train set again ###\n",
      "idx: 0 data: [[ 0.12064842  0.92218769  3.22653523]\n",
      " [ 0.83660435 -0.88768222  1.2122358 ]] [-1.3309468]\n",
      "idx: 1 data: [[ 0.83660435 -0.88768222  1.2122358 ]\n",
      " [ 0.67280815  0.07891949 -1.3309468 ]] [2.32867388]\n",
      "idx: 2 data: [[ 0.67280815  0.07891949 -1.3309468 ]\n",
      " [-0.1513143   0.03342441  2.32867388]] [0.22463874]\n",
      "idx: 3 data: [[-0.1513143   0.03342441  2.32867388]\n",
      " [ 0.15557226 -0.02865607  0.22463874]] [0.88872031]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.standard_normal((10, 2))\n",
    "Y = np.random.standard_normal((10, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "\n",
    "lag_window_params = {'lag_window': 1, 'include_y': True, 'pre_calc': True}\n",
    "\n",
    "dataloader = XYDataLoader(X = X, Y = Y, val_index_start=6, test_index_start=8, lag_window_params=lag_window_params)\n",
    "\n",
    "sample_X, sample_Y = dataloader[0]\n",
    "\n",
    "print(\"length train:\", dataloader.len_train, \"length val:\", dataloader.len_val, \"length test:\", dataloader.len_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.val()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from val set ###\")\n",
    "for i in range(dataloader.len_val):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.test()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from test set ###\")\n",
    "for i in range(dataloader.len_test):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.train()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set again ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiShapeLoader(BaseDataLoader):\n",
    "\n",
    "    \"\"\"\n",
    "    A class designed for comlex datasets with mutlipe feature types. The class is more\n",
    "    memory-efficient than the XYDataLoader, as it separate the storeage of SKU-specific\n",
    "    feature, time-specific features, and time-SKU-specific features. The class works generically\n",
    "    as long as those feature classes are provided during pre-processing. The class is designed \n",
    "    to handle classic learning, but able to work in a meta-learning pipeline where no SKU-dimension\n",
    "    is present and the model needs to make prediction on SKU-time level without knowhing the\n",
    "    specific SKU.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        demand: pd.DataFrame, # Demand data of shape time x SKU\n",
    "        SKU_features: pd.DataFrame, # Features constant over time of shape SKU x SKU_features\n",
    "        time_features: pd.DataFrame, # Features constant over SKU of shape time x time_features\n",
    "        time_SKU_features: pd.DataFrame, # Features varying over time and SKU of shape time x (time_SKU_features*SKU) with double index\n",
    "        mask: pd.DataFrame, # Mask of shape time x SKU telling which SKUs are available at which time (can be used as mask during trainig or added to features)\n",
    "        \n",
    "        val_index_start: Union[int, None] = None, # Validation index start on the time dimension\n",
    "        test_index_start: Union[int, None] = None, # Test index start on the time dimension\n",
    "        in_sample_val_test_SKUs: List = None, # SKUs in the training set to be used for validation and testing, out-of-sample w.r.t. time dimension\n",
    "        out_of_sample_val_SKUs: List = None, # SKUs to be hold-out for validation (can be same as test if no validation on out-of-sample SKUs required)\n",
    "        out_of_sample_test_SKUs: List = None, # SKUs to be hold-out for testing\n",
    "        lag_window_params: Union[dict] = None, # default: {'lag_window': 0, 'include_y': False, 'pre_calc': True}\n",
    "        normalize_features: Union[dict] = None, # default: {'normalize': True, 'ignore_one_hot': True}\n",
    "        engineered_SKU_features: Union[dict] = None, # default: [\"mean_demand\", \"std_demand\", \"kurtosis_demand\", \"skewness_demand\", \"percentile_10_demand\", \"percentile_30_demand\", \"median_demand\", \"percentile_70_demand\", \"percentile_90_demand\", \"inter_quartile_range\"]\n",
    "        include_non_available: bool = False, # if timestep/SKU combination where the SKU was not available for sale shall be included. If included, it will be used as feature, otherwise as mask.\n",
    "        train_subset: int = False ,# if only a subset of SKUs is used for training. Will always contain in_sample_val_test_SKUs and then fills the rest with random SKUs\n",
    "        train_subset_SKUs: List = None, # if train_subset is set, specific SKUs can be provided\n",
    "        SKU_as_batch: bool = False # if get_index during training gets an index for the time dimension (in batch) or from time*SKU dimension\n",
    "    ):\n",
    "\n",
    "        normalize_features = normalize_features or {'normalize': True, 'ignore_one_hot': True}\n",
    "        lag_window_params = lag_window_params or {'lag_window': 0, 'include_y': False, 'pre_calc': False}\n",
    "        self.lag_window_params = lag_window_params\n",
    "        self.train_index_start = self.lag_window_params[\"lag_window\"] # start index for training data\n",
    "        self.train_index_start += self.lag_window_params[\"include_y\"] # if lag demand is included as feature need one more timestep\n",
    "        engineered_SKU_features = engineered_SKU_features or [\"mean_demand\", \"std_demand\", \"kurtosis_demand\", \"skewness_demand\", \"percentile_10_demand\", \"percentile_30_demand\", \"median_demand\", \"percentile_70_demand\", \"percentile_90_demand\", \"inter_quartile_range\"]\n",
    "\n",
    "        self.demand = demand\n",
    "        self.SKU_features = SKU_features\n",
    "        self.time_features = time_features\n",
    "        self.time_SKU_features = time_SKU_features\n",
    "        self.mask = mask\n",
    "        self.num_time_SKU_features = len(self.time_SKU_features.columns.get_level_values(0).unique())\n",
    "        self.num_units = len(self.demand.columns)\n",
    "        self.num_features = len(self.SKU_features.columns) + len(self.time_features.columns) + self.num_time_SKU_features\n",
    "        if engineered_SKU_features is not None:\n",
    "            self.num_features += len(engineered_SKU_features)\n",
    "        if lag_window_params[\"include_y\"]:\n",
    "            self.num_features += 1\n",
    "        if include_non_available:\n",
    "            self.num_features += 1\n",
    "\n",
    "        self.normalized_in_sample_SKUs = False\n",
    "        self.normalized_out_of_sample_val_SKUs = False\n",
    "        self.normalized_out_of_sample_test_SKUs = False\n",
    "\n",
    "        self.include_non_available = include_non_available\n",
    "        self.train_subset = train_subset\n",
    "        self.train_subset_SKUs = train_subset_SKUs\n",
    "        self.SKU_as_batch = SKU_as_batch\n",
    "\n",
    "        self.SKU_type = \"in_sample\" # or \"out_of_sample_val\" or \"out_of_sample_test\" # affecting the SKU-dimension\n",
    "        self.dataset_type = \"train\" # or \"val\" or \"test\", affecting the time-dimension\n",
    "\n",
    "        logging.info(\"Setting indices for validation and test set\")\n",
    "        self.val_index_start = val_index_start\n",
    "        self.test_index_start = test_index_start\n",
    "\n",
    "        # train index ends either at the start of the validation set, the start of the test set or at the end of the dataset\n",
    "        if self.val_index_start is not None:\n",
    "            self.train_index_end = self.val_index_start-1\n",
    "        elif self.test_index_start is not None:\n",
    "            self.train_index_end = self.test_index_start-1\n",
    "        else:\n",
    "            self.train_index_end = len(self.demand)-1\n",
    "        \n",
    "        logging.info(\"Setting out-of-sample SKUs\")\n",
    "        # print(\"Number of SKUs in dataset:\", self.demand.shape[1])\n",
    "        for sku, attr_suffix in [(out_of_sample_val_SKUs, 'val'), (out_of_sample_test_SKUs, 'test')]:\n",
    "            if sku is not None:\n",
    "                setattr(self, f'demand_out_of_sample_{attr_suffix}', self.demand.loc[:, sku])\n",
    "                setattr(self, f'SKU_features_out_of_sample_{attr_suffix}', self.SKU_features.loc[sku])\n",
    "                setattr(self, f'time_SKU_features_out_of_sample_{attr_suffix}', # here SKU are in columns on index level 2\n",
    "                        self.time_SKU_features.loc[:, pd.IndexSlice[:, sku]])   \n",
    "                setattr(self, f'mask_out_of_sample_{attr_suffix}', self.mask.loc[:, sku])\n",
    "                # time_features are independent of SKU\n",
    "\n",
    "                self.demand.drop(columns=sku, inplace=True)\n",
    "                self.SKU_features.drop(index=sku, inplace=True)\n",
    "                for single_sku in sku if isinstance(sku, list) else [sku]:\n",
    "                    columns_to_drop = self.time_SKU_features.columns.get_loc_level(single_sku, level=1)\n",
    "                    self.time_SKU_features.drop(columns=self.time_SKU_features.columns[columns_to_drop[0]], inplace=True)\n",
    "                self.mask.drop(columns=sku, inplace=True)\n",
    "        self.in_sample_val_test_SKUs = in_sample_val_test_SKUs\n",
    "        \n",
    "        logging.info(\"Identifying training SKUs\")\n",
    "        self.identify_train_SKUs()\n",
    "\n",
    "        logging.info(\"Creating engineered SKU features for training data\")\n",
    "        engineered_SKU_features = self.build_engineered_SKU_features(engineered_SKU_features, self.demand.iloc[:self.train_index_end+1]) # only for training data initially\n",
    "        self.SKU_features = pd.concat([self.SKU_features, engineered_SKU_features.transpose()], axis=1)\n",
    "        \n",
    "        logging.info(\"Normalizing in-sample SKU features (based on training timesteps)\")\n",
    "        self.normalize_features_in_sample(**normalize_features, initial_normalization=True)\n",
    "\n",
    "        # store row and column indices of demand, SKU_features time_features mask and then convert to numpy array\n",
    "\n",
    "        self.demand_indices = self.save_indices(self.demand)\n",
    "        self.SKU_features_indices = self.save_indices(self.SKU_features)\n",
    "        self.time_features_indices = self.save_indices(self.time_features)\n",
    "        self.time_SKU_features_indices = self.save_indices(self.time_SKU_features)\n",
    "        self.mask_indices = self.save_indices(self.mask)\n",
    "\n",
    "        self.demand = self.demand.to_numpy()\n",
    "        self.SKU_features = self.SKU_features.to_numpy()\n",
    "        self.time_features = self.time_features.to_numpy()\n",
    "        self.time_SKU_features = self.time_SKU_features.to_numpy()\n",
    "        self.mask = self.mask.to_numpy()\n",
    "\n",
    "        self.len_train_time = self.train_index_end-self.train_index_start+1\n",
    "        if SKU_as_batch:\n",
    "            logging.info(\"Creating time-SKU index for training data\")\n",
    "            self.sku_time_index = [(i, j) for i in range(self.train_SKUs_indices.shape[0]) for j in range(self.len_train_time)]\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def identify_train_SKUs(self):\n",
    "        \"\"\" determine which SKUs are used for training, validation and testing \"\"\"\n",
    "\n",
    "        if self.train_subset:\n",
    "\n",
    "            if self.train_subset_SKUs is not None:\n",
    "                if len(self.train_subset_SKUs) != self.train_subset:\n",
    "                    raise ValueError('train_subset_SKUs must have the same length as train_subset')\n",
    "                train_SKUs = self.train_subset_SKUs\n",
    "                # check that all train_SKUs are in demand.collumns\n",
    "                if not set(train_SKUs).issubset(self.demand.columns):\n",
    "                    raise ValueError('train_subset_SKUs must be a subset of all training SKUs')\n",
    "                if self.in_sample_val_test_SKUs is not None:\n",
    "                    if not set(self.in_sample_val_test_SKUs).issubset(train_SKUs):\n",
    "                        raise ValueError('train_subset_SKUs must contain in_sample_val_test_SKUs')\n",
    "            else:\n",
    "                if self.in_sample_val_test_SKUs is not None and self.train_subset <= len(self.in_sample_val_test_SKUs):\n",
    "                    raise ValueError('train_subset must be equal or larger than the number of in_sample_val_test_SKUs')\n",
    "                train_SKUs = self.in_sample_val_test_SKUs if self.in_sample_val_test_SKUs is not None else []\n",
    "                remaining_SKUs = self.demand.columns.difference(train_SKUs)\n",
    "                additional_SKUs = np.random.choice(remaining_SKUs, self.train_subset-len(train_SKUs), replace=False)\n",
    "                train_SKUs = np.concatenate((train_SKUs, additional_SKUs))\n",
    "    \n",
    "        else:\n",
    "            train_SKUs = self.demand.columns # val and test SKUs have been removed before, only training SKUs remain\n",
    "        \n",
    "        self.train_SKUs = train_SKUs\n",
    "\n",
    "        self.train_SKUs_indices = self.demand.columns.get_indexer(self.train_SKUs)\n",
    "        if self.in_sample_val_test_SKUs is not None:\n",
    "            self.in_sample_val_test_SKUs_indices = self.demand.columns.get_indexer(self.in_sample_val_test_SKUs)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def build_engineered_SKU_features(engineered_SKU_features: List, demand: pd.DataFrame):\n",
    "\n",
    "        \"\"\"\n",
    "        Create engineered features for each SKU\n",
    "        \"\"\"\n",
    "\n",
    "        feature_names = []\n",
    "        feature_values = []\n",
    "\n",
    "        for feature in engineered_SKU_features:\n",
    "\n",
    "            if feature == \"mean_demand\":\n",
    "                mean_demand = demand.mean(axis=0)\n",
    "            elif feature == \"std_demand\":\n",
    "                std_demand = demand.std(axis=0)\n",
    "            elif feature == \"kurtosis_demand\":\n",
    "                kurtosis_demand = demand.kurtosis(axis=0)\n",
    "            elif feature == \"skewness_demand\":\n",
    "                skewness_demand = demand.skew(axis=0)\n",
    "            elif feature == \"percentile_10_demand\":\n",
    "                percentile_10_demand = demand.quantile(0.1, axis=0)\n",
    "            elif feature == \"percentile_30_demand\":\n",
    "                percentile_30_demand = demand.quantile(0.3, axis=0)\n",
    "            elif feature == \"median_demand\":\n",
    "                median_demand = demand.median(axis=0)\n",
    "            elif feature == \"percentile_70_demand\":\n",
    "                percentile_70_demand = demand.quantile(0.7, axis=0)\n",
    "            elif feature == \"percentile_90_demand\":\n",
    "                percentile_90_demand = demand.quantile(0.9, axis=0)\n",
    "            elif feature == \"inter_quartile_range\":\n",
    "                inter_quartile_range = demand.quantile(0.75, axis=0) - demand.quantile(0.25, axis=0)\n",
    "            else:  \n",
    "                raise ValueError(f'Feature {feature} not recognized')\n",
    "            \n",
    "            feature_names.append(feature)\n",
    "            feature_values.append(locals()[feature])\n",
    "\n",
    "        return pd.DataFrame(feature_values, columns=demand.columns, index=feature_names)\n",
    "    \n",
    "    def normalize_features_in_sample(self,\n",
    "        normalize: bool = True,\n",
    "        ignore_one_hot: bool = True,\n",
    "        initial_normalization = False # Flag if it is set before having added lag features\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Normalize features using a standard scaler. If ignore_one_hot is true, one-hot encoded features are not normalized.\n",
    "        \"\"\"\n",
    "\n",
    "        if normalize:\n",
    "\n",
    "            if self.normalized_in_sample_SKUs:\n",
    "                raise ValueError('Features already normalized')\n",
    "\n",
    "            self.scaler_demand = StandardScaler()\n",
    "            self.scaler_SKU_features = StandardScaler()\n",
    "            self.scaler_time_features = StandardScaler()\n",
    "            self.scaler_time_SKU_features= [StandardScaler() for _ in range(self.num_time_SKU_features)]\n",
    "\n",
    "            if initial_normalization:\n",
    "            \n",
    "                logging.info(\"--Normalizing demand\")\n",
    "                self.scaler_demand.fit(self.demand[:self.train_index_end+1])\n",
    "                transformed_demand = self.scaler_demand.transform(self.demand)\n",
    "                self.demand.iloc[:,:] = transformed_demand\n",
    "\n",
    "                logging.info(\"--Normalizing SKU features\")\n",
    "                continuous_features = [col for col in self.SKU_features.columns if not self.is_one_hot(self.SKU_features[col])]\n",
    "                if len(continuous_features) > 0:\n",
    "                    self.scaler_SKU_features.fit(self.SKU_features[continuous_features]) # SKU features are already calculated based on training index\n",
    "                    transformed_SKU_features = self.scaler_SKU_features.transform(self.SKU_features[continuous_features])\n",
    "                    self.SKU_features[continuous_features] = transformed_SKU_features\n",
    "\n",
    "                logging.info(\"--Normalizing time features\")\n",
    "                continuous_features = [col for col in self.time_features.columns if not self.is_one_hot(self.time_features[col])]\n",
    "                if len(continuous_features) > 0:\n",
    "                    self.scaler_time_features.fit(self.time_features.loc[:self.train_index_end+1,continuous_features]) # each column to be normalized\n",
    "                    transformed_time_features = self.scaler_time_features.transform(self.time_features.loc[:,continuous_features])\n",
    "                    self.time_features.loc[:,continuous_features] = transformed_time_features\n",
    "\n",
    "                logging.info(\"--Normalizing time-SKU features\")\n",
    "                # Normalize time-SKU features (double-indexed)\n",
    "                for i, feature in enumerate(self.time_SKU_features.columns.get_level_values(0).unique()):\n",
    "                    # Select all columns corresponding to the current feature in level 0\n",
    "                    feature_df = self.time_SKU_features.xs(key=feature, axis=1, level=0)\n",
    "                    if not self.is_one_hot_across_skus(feature_df):\n",
    "                        self.scaler_time_SKU_features[i].fit(feature_df[:self.train_index_end+1])\n",
    "                        transformed_feature_df = self.scaler_time_SKU_features[i].transform(feature_df)\n",
    "                        self.time_SKU_features.loc[:, (feature, slice(None))] = transformed_feature_df\n",
    "            \n",
    "                self.normalized_in_sample_SKUs = True\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError('Training data can only normalized during initialization - later normlization not implemented yet')\n",
    "\n",
    "    def update_lag_features(self,\n",
    "        lag_window: int,\n",
    "        ):\n",
    "\n",
    "        \"\"\" Update lag window parameters for dataloader object that is already initialized \"\"\"\n",
    "\n",
    "        raise NotImplementedError('Not implemented yet')\n",
    "\n",
    "        # Problem: updating lag_features naively would shorten the dataset each time it is called\n",
    "\n",
    "    def get_time_SKU_idx(self, idx):\n",
    "\n",
    "        \"\"\" get time and SKU index by index, depending on the dataset type (train, val, test)\"\"\"\n",
    "\n",
    "        if self.dataset_type == \"train\":\n",
    "\n",
    "            if self.SKU_as_batch:\n",
    "                if idx >= len(self.sku_time_index):\n",
    "                    raise IndexError(f'index {idx} out of range{len(self.sku_time_index)}')\n",
    "                idx_sku, idx_time, = self.sku_time_index[idx]\n",
    "                idx_skus = [idx_sku]\n",
    "\n",
    "                print(idx_time, idx_sku)\n",
    "\n",
    "            else:\n",
    "                if idx+self.train_index_start > self.train_index_end:\n",
    "                    raise IndexError(f'index {idx} out of range{self.train_index_end-self.train_index_start}')\n",
    "                idx_skus = self.train_SKUs_indices\n",
    "                idx_time = idx\n",
    "            idx_time += self.train_index_start\n",
    "\n",
    "        elif self.dataset_type == \"val\":\n",
    "            idx_time = idx + self.val_index_start\n",
    "            if self.in_sample_val_test_SKUs is not None:\n",
    "                idx_skus = self.in_sample_val_test_SKUs_indices\n",
    "            else:\n",
    "                idx_skus = self.train_SKUs_indices\n",
    "            \n",
    "            if idx >= self.test_index_start:\n",
    "                raise IndexError(f'index{idx} out of range{self.test_index_start}')\n",
    "        elif self.dataset_type == \"test\":\n",
    "            idx_time = idx + self.test_index_start\n",
    "            if self.in_sample_val_test_SKUs is not None:\n",
    "                idx_skus = self.in_sample_val_test_SKUs_indices\n",
    "            else:\n",
    "                idx_skus = self.train_SKUs_indices\n",
    "            \n",
    "            if idx >= len(self.demand):\n",
    "                raise IndexError(f'index{idx} out of range{len(self.demand)}')\n",
    "        else:\n",
    "            raise ValueError('dataset_type not set')\n",
    "\n",
    "        return idx_time, idx_skus\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \"\"\" get item by index, depending on the dataset type (train, val, test)\"\"\"\n",
    "\n",
    "        lag_window = self.lag_window_params[\"lag_window\"]\n",
    "        include_y = self.lag_window_params[\"include_y\"]\n",
    "\n",
    "        idx_time, idx_skus = self.get_time_SKU_idx(idx)\n",
    "        num_skus = len(idx_skus)\n",
    "\n",
    "        print(idx_time, idx_skus)\n",
    "        demand = self.demand[idx_time, idx_skus]\n",
    "\n",
    "        item = np.empty((1,lag_window+1, self.num_features, num_skus))\n",
    "\n",
    "        for t in range(lag_window+1):\n",
    "\n",
    "            item_t = np.empty((1, self.num_features, num_skus))\n",
    "            idx_time_t = idx_time-t\n",
    "\n",
    "            if include_y:\n",
    "                assert idx_time_t-1 >= 0\n",
    "                lag_demand = self.demand[idx_time_t-1, idx_skus]\n",
    "\n",
    "            SKU_features = self.SKU_features[idx_skus].transpose()\n",
    "            time_features = self.time_features[idx_time_t]\n",
    "            # repeate time_SKU_features for all SKUs with SKU as last dimension \n",
    "            time_features = np.repeat(time_features[:, np.newaxis], num_skus, axis=1)\n",
    "\n",
    "            time_SKU_features = np.empty((self.num_time_SKU_features, num_skus))\n",
    "            for i, idx_sku in enumerate(idx_skus):\n",
    "                SKU_indices = [len(self.train_SKUs)*i+idx_sku for i in range(self.num_time_SKU_features)]\n",
    "                time_SKU_features[:,i] = self.time_SKU_features[idx_time_t, SKU_indices]\n",
    "            len_SKU_features = len(SKU_features)\n",
    "            len_time_features = len(time_features)\n",
    "            \n",
    "            item_t[:,:len(SKU_features),:] = SKU_features\n",
    "            item_t[:,len_SKU_features:(len_SKU_features+len_time_features),:] = time_features\n",
    "            item_t[:,(len_SKU_features+len_time_features):(len_SKU_features+len_time_features+self.num_time_SKU_features),:] = time_SKU_features\n",
    "\n",
    "            if self.include_non_available:\n",
    "                if self.include_y:\n",
    "                    item_t[:,-2,:] = lag_demand\n",
    "                item_t[:,-1,:] = self.mask[idx_time,idx_skus]\n",
    "            else:\n",
    "                item_t[:,-1,:] = lag_demand\n",
    "\n",
    "            item[:,t,:,:] = item_t\n",
    "        \n",
    "        if lag_window == 0:\n",
    "            if item.shape[1] == 1:\n",
    "                item = item.squeeze(1) \n",
    "            else:\n",
    "                raise ValueError('Lag window is 0, but item has more than one time dimension')\n",
    "        if self.SKU_as_batch:\n",
    "            if item.shape[-1] == 1:\n",
    "                item = item.squeeze(-1) \n",
    "            else:\n",
    "                raise ValueError('SKU as batch, but item has more than one SKU dimension')\n",
    "    \n",
    "        return item, demand\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.demand)\n",
    "    \n",
    "    @property\n",
    "    def X_shape(self):\n",
    "        return (len_train_time, self.num_features, self.num_units)\n",
    "    \n",
    "    @property\n",
    "    def Y_shape(self):\n",
    "        return (len_train_time, self.num_units)\n",
    "\n",
    "    @property\n",
    "    def len_train(self):\n",
    "        if self.SKU_as_batch:\n",
    "            return len(self.sku_time_index)\n",
    "        else:\n",
    "            return self.len_train_time\n",
    "\n",
    "    @property\n",
    "    def len_val(self):\n",
    "        if self.val_index_start is None:\n",
    "            raise ValueError('no validation set defined')\n",
    "        return self.test_index_start-self.val_index_start\n",
    "\n",
    "    @property\n",
    "    def len_test(self):\n",
    "        if self.test_index_start is None:\n",
    "            raise ValueError('no test set defined')\n",
    "        return len(self.demand)-self.test_index_start\n",
    "\n",
    "    def get_all_X(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        raise NotImplementedError('Not implemented yet')\n",
    "\n",
    "        # \"\"\"\n",
    "        # Returns the entire features dataset.\n",
    "        # Return either the train, val, test, or all data.\n",
    "        # \"\"\"\n",
    "\n",
    "        # if dataset_type == 'train':\n",
    "        #     return self.X[:self.val_index_start].copy() if self.X is not None else None\n",
    "        # elif dataset_type == 'val':\n",
    "        #     return self.X[self.val_index_start:self.test_index_start].copy() if self.X is not None else None\n",
    "        # elif dataset_type == 'test':\n",
    "        #     return self.X[self.test_index_start:].copy() if self.X is not None else None\n",
    "        # elif dataset_type == 'all':\n",
    "        #     return self.X.copy() if self.X is not None else None\n",
    "        # else:\n",
    "        #     raise ValueError('dataset_type not recognized')\n",
    "\n",
    "    def get_all_Y(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        # \"\"\"\n",
    "        # Returns the entire target dataset.\n",
    "        # Return either the train, val, test, or all data.\n",
    "        # \"\"\"\n",
    "\n",
    "        raise NotImplementedError('Not implemented yet')\n",
    "\n",
    "        # if dataset_type == 'train':\n",
    "        #     return self.Y[:self.val_index_start].copy() if self.Y is not None else None\n",
    "        # elif dataset_type == 'val':\n",
    "        #     return self.Y[self.val_index_start:self.test_index_start].copy() if self.Y is not None else None\n",
    "        # elif dataset_type == 'test':\n",
    "        #     return self.Y[self.test_index_start:].copy() if self.Y is not None else None\n",
    "        # elif dataset_type == 'all':\n",
    "        #     return self.Y.copy() if self.Y is not None else None\n",
    "        # else:\n",
    "        #     raise ValueError('dataset_type not recognized')\n",
    "\n",
    "    @staticmethod\n",
    "    def is_one_hot(column):\n",
    "        return set(column.unique()) <= {0, 1}\n",
    "\n",
    "    @staticmethod\n",
    "    def is_one_hot_across_skus(feature_df):\n",
    "        \"\"\"\n",
    "        Check if the set of unique values in a feature across all SKU_ids is {0, 1}.\n",
    "        feature_df: DataFrame slice for a specific feature with SKU_ids as columns.\n",
    "        \"\"\"\n",
    "        flattened_values = feature_df.values.flatten()\n",
    "        \n",
    "        # Check if the unique values in this flattened array are exactly {0, 1}\n",
    "        unique_values = set(flattened_values)\n",
    "        \n",
    "        return unique_values <= {0, 1}\n",
    "\n",
    "    @staticmethod\n",
    "    def save_indices(df):\n",
    "        \"\"\"\n",
    "        Saves the row and column indices of a DataFrame.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'rows': df.index,\n",
    "            'columns': df.columns\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using existing data from disk\n",
      "INFO:root:Importing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Preprocessing data\n",
      "INFO:root:--Creating catogory mapping and features\n",
      "INFO:root:--Preparing sales time series data\n",
      "INFO:root:--Preparing calendric information\n",
      "INFO:root:--Preparing snap features\n",
      "INFO:root:--Preparing price information\n",
      "INFO:root:--Creating indicator table if products are available for purchase\n",
      "INFO:root:--Preparing final outputs and ensure consistency of time and feature dimensions\n"
     ]
    }
   ],
   "source": [
    "from ddopnew.datasets.kaggle_m5 import KaggleM5DatasetLoader\n",
    "\n",
    "data_path = \"/Users/magnus/Documents/02_PhD/Reinforcement_Learning/general_purpose_drl/Newsvendor/kaggle_data\" # For testing purposes, please specify the path to the data on your machine\n",
    "if data_path is not None:\n",
    "    loader = KaggleM5DatasetLoader(data_path, overwrite=False, product_as_feature=False)\n",
    "    demand, SKU_features, time_features, time_SKU_features, mask = loader.load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting indices for validation and test set\n",
      "INFO:root:Setting out-of-sample SKUs\n",
      "INFO:root:Identifying training SKUs\n",
      "INFO:root:Creating engineered SKU features for training data\n",
      "INFO:root:Normalizing in-sample SKU features (based on training timesteps)\n",
      "INFO:root:--Normalizing demand\n",
      "INFO:root:--Normalizing SKU features\n",
      "INFO:root:--Normalizing time features\n",
      "INFO:root:--Normalizing time-SKU features\n",
      "INFO:root:Creating time-SKU index for training data\n"
     ]
    }
   ],
   "source": [
    "val_index_start = len(demand)-300\n",
    "test_index_start = len(demand)-100\n",
    "\n",
    "out_of_sample_val_SKUs = [\"HOBBIES_1_002_CA_1\", \"HOBBIES_1_003_CA_1\"]\n",
    "out_of_sample_test_SKUs = [\"HOBBIES_1_005_CA_1\", \"FOODS_3_819_WI_3\"]\n",
    "\n",
    "dataloader = MultiShapeLoader(\n",
    "    demand.copy(),\n",
    "    SKU_features.copy(),\n",
    "    time_features.copy(),\n",
    "    time_SKU_features.copy(),\n",
    "    mask.copy(),\n",
    "    val_index_start=val_index_start,\n",
    "    test_index_start=test_index_start,\n",
    "    # in_sample_val_test_SKUs=[\"FOODS_3_825_WI_3\"],\n",
    "    out_of_sample_val_SKUs=out_of_sample_val_SKUs,\n",
    "    out_of_sample_test_SKUs=out_of_sample_test_SKUs,\n",
    "    lag_window_params = {'lag_window': 5, 'include_y': True, 'pre_calc': False},\n",
    "    # train_subset=300,\n",
    "    # train_subset_SKUs=[\"HOBBIES_1_001_CA_1\", \"HOBBIES_1_012_CA_1\"],\n",
    "    SKU_as_batch = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634 30485\n",
      "1640 [30485]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00, -2.03228586e-01,\n",
       "          -4.43128224e-02, -2.07535661e-01, -1.26581336e-01,\n",
       "          -5.62582701e-02, -1.35647453e-01, -2.35129336e-01,\n",
       "          -3.53059783e-01, -1.48281536e-01, -4.23038458e-01,\n",
       "           1.62981158e+00,  1.72888660e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.77635684e-15,  0.00000000e+00,\n",
       "           1.81258722e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00, -2.03228586e-01,\n",
       "          -4.43128224e-02, -2.07535661e-01, -1.26581336e-01,\n",
       "          -5.62582701e-02, -1.35647453e-01, -2.35129336e-01,\n",
       "          -3.53059783e-01, -1.48281536e-01, -4.23038458e-01,\n",
       "           1.62981158e+00,  1.72677692e+00,  0.00000000e+00,\n",
       "           1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.77635684e-15,  0.00000000e+00,\n",
       "           3.42268075e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00, -2.03228586e-01,\n",
       "          -4.43128224e-02, -2.07535661e-01, -1.26581336e-01,\n",
       "          -5.62582701e-02, -1.35647453e-01, -2.35129336e-01,\n",
       "          -3.53059783e-01, -1.48281536e-01, -4.23038458e-01,\n",
       "           1.62981158e+00,  1.72466723e+00,  1.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.77635684e-15,  0.00000000e+00,\n",
       "           1.81258722e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00, -2.03228586e-01,\n",
       "          -4.43128224e-02, -2.07535661e-01, -1.26581336e-01,\n",
       "          -5.62582701e-02, -1.35647453e-01, -2.35129336e-01,\n",
       "          -3.53059783e-01, -1.48281536e-01, -4.23038458e-01,\n",
       "           1.62981158e+00,  1.72255755e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.77635684e-15,  0.00000000e+00,\n",
       "           3.42268075e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00, -2.03228586e-01,\n",
       "          -4.43128224e-02, -2.07535661e-01, -1.26581336e-01,\n",
       "          -5.62582701e-02, -1.35647453e-01, -2.35129336e-01,\n",
       "          -3.53059783e-01, -1.48281536e-01, -4.23038458e-01,\n",
       "           1.62981158e+00,  1.72044786e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.77635684e-15,  0.00000000e+00,\n",
       "          -3.92891495e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.00000000e+00, -2.03228586e-01,\n",
       "          -4.43128224e-02, -2.07535661e-01, -1.26581336e-01,\n",
       "          -5.62582701e-02, -1.35647453e-01, -2.35129336e-01,\n",
       "          -3.53059783e-01, -1.48281536e-01, -4.23038458e-01,\n",
       "           1.62981158e+00,  1.71833818e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.77635684e-15,  0.00000000e+00,\n",
       "           2.54774679e+00]]]),\n",
       " array([2.54774679]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.__getitem__(49844609) #986 with non-zero lag demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
