{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular dataloaders\n",
    "\n",
    "> Dataloaders for tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataloaders.tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Tuple, List, Literal\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XYDataLoader(BaseDataLoader):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    A class for datasets with the typicall X, Y structure. Both X\n",
    "    and Y are numpy arrays. X may be of shape (datapoints, features) or (datapoints, sequence_length, features) \n",
    "    if lag features are used. The prep_lag_features can be used to create those lag features. Y is of shape\n",
    "    (datapoints, units).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        val_index_start: Union[int, None] = None, \n",
    "        test_index_start: Union[int, None] = None, \n",
    "        lag_window_params: Union[dict] = None, # default: {'lag_window': 0, 'include_y': False, 'pre_calc': False}\n",
    "        normalize_features: Union[dict] = None, # default: {'normalize': True, 'ignore_one_hot': True}\n",
    "    ):\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.val_index_start = val_index_start\n",
    "        self.test_index_start = test_index_start\n",
    "\n",
    "        # train index ends either at the start of the validation set, the start of the test set or at the end of the dataset\n",
    "        if self.val_index_start is not None:\n",
    "            self.train_index_end = self.val_index_start-1\n",
    "        elif self.test_index_start is not None:\n",
    "            self.train_index_end = self.test_index_start-1\n",
    "        else:\n",
    "            self.train_index_end = len(Y)-1\n",
    "\n",
    "        self.dataset_type = \"train\"\n",
    "\n",
    "        normalize_features = normalize_features or {'normalize': True, 'ignore_one_hot': True}\n",
    "        lag_window_params = lag_window_params or {'lag_window': 0, 'include_y': False, 'pre_calc': False}\n",
    "\n",
    "        self.normalize_features(**normalize_features, initial_normalization=True)\n",
    "        self.prep_lag_features(**lag_window_params)\n",
    "\n",
    "        # X must at least have datapoint and feature dimension\n",
    "        if len(X.shape) == 1:\n",
    "            self.X = X.reshape(-1, 1)\n",
    "        \n",
    "        # Y must have at least datapoint and unit dimension (even if only one unit is present)\n",
    "        if len(Y.shape) == 1:\n",
    "            self.Y = Y.reshape(-1, 1)\n",
    "\n",
    "        assert len(X) == len(Y), 'X and Y must have the same length'\n",
    "\n",
    "        self.num_units = Y.shape[1] # shape 0 is alsways time, shape 1 is the number of units (e.g., SKUs)\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def normalize_features(self,\n",
    "        normalize: bool = True,\n",
    "        ignore_one_hot: bool = True,\n",
    "        initial_normalization=False # Flag if it is set before having added lag features\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Normalize features using a standard scaler. If ignore_one_hot is true, one-hot encoded features are not normalized.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if normalize:\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            if initial_normalization:\n",
    "\n",
    "                if len(self.X.shape) == 3:\n",
    "                    raise ValueError('Normalization not possible with lag features. Please set initial_normalization=False')\n",
    "            \n",
    "                scaler.fit(self.X[:self.train_index_end+1]) # +1 to include the last training point\n",
    "                scaler.transform(self.X)\n",
    "\n",
    "                if initial_normalization:\n",
    "                    return\n",
    "                else:\n",
    "                    raise NotImplementedError('Normalization after lag features have been set not implemented yet')\n",
    "\n",
    "                    # Idea:\n",
    "                        # remove time dimension\n",
    "                        # normalize features\n",
    "                        # add time_dimension back\n",
    "                    # Problem:\n",
    "                        # usage of prep_lag_features needs to ensure y is not added a second time\n",
    "\n",
    "    def prep_lag_features(self,\n",
    "        lag_window: int = 0, # length of the lage window\n",
    "        include_y: bool = False, # if lag demand shall be included as feature\n",
    "        pre_calc: bool = False # if all lags are pre-calculated for the entire dataset\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Create lag feature for the dataset. If \"inlcude_y\" is true, then a lag-1 of of the target variable is added as a feature.\n",
    "        If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag\n",
    "        window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features\n",
    "        including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce\n",
    "        computation time later on at the expense of increases memory usage. \n",
    "\n",
    "        \"\"\"\n",
    "        # to be discussed: Do we need option to only provide lag demand wihtout lag features?\n",
    "        self.lag_window = lag_window\n",
    "        self.pre_calc = pre_calc\n",
    "        self.include_y = include_y\n",
    "        \n",
    "        if self.pre_calc:\n",
    "            if self.include_y:\n",
    "                # add additional column to X with demand shifted by 1\n",
    "                self.X = np.concatenate((self.X, np.roll(self.Y, 1, axis=0)), axis=1)\n",
    "                self.X = self.X[1:] # remove first row\n",
    "                self.Y = self.Y[1:] # remove first row\n",
    "                \n",
    "                self.val_index_start = self.val_index_start-1\n",
    "                self.test_index_start = self.test_index_start-1\n",
    "                self.train_index_end  = self.train_index_end-1\n",
    "        \n",
    "            if self.lag_window is not None and self.lag_window > 0:\n",
    "\n",
    "                # add lag features as dimention 2 to X (making it dimension (datapoints, sequence_length, features))\n",
    "                X_lag = np.zeros((self.X.shape[0], self.lag_window+1, self.X.shape[1]))\n",
    "                for i in range(self.lag_window+1):\n",
    "                    if i == 0:\n",
    "                        features = self.X\n",
    "                    else:    \n",
    "                        features = self.X[:-i, :]\n",
    "                    X_lag[i:, self.lag_window-i, :] = features\n",
    "                self.X = X_lag[self.lag_window:]\n",
    "                self.Y = self.Y[self.lag_window:]\n",
    "\n",
    "                self.val_index_start = self.val_index_start-self.lag_window\n",
    "                self.test_index_start = self.test_index_start-self.lag_window\n",
    "                self.train_index_end  = self.train_index_end-self.lag_window\n",
    "\n",
    "        else:\n",
    "            self.lag_window = None\n",
    "            self.include_y = False\n",
    "            # add time dimension to X\n",
    "\n",
    "    def update_lag_features(self,\n",
    "        lag_window: int,\n",
    "        ):\n",
    "\n",
    "        \"\"\" Update lag window parameters for dataloader object that is already initialized \"\"\"\n",
    "\n",
    "        raise NotImplementedError('Not implemented yet')\n",
    "\n",
    "        # Problem: updating lag_features naively would shorten the dataset each time it is called\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "\n",
    "        \"\"\" get item by index, depending on the dataset type (train, val, test)\"\"\"\n",
    "\n",
    "        if self.dataset_type == \"train\":\n",
    "            if idx > self.train_index_end:\n",
    "                raise IndexError(f'index {idx} out of range{self.train_index_end}')\n",
    "            idx = idx\n",
    "\n",
    "        elif self.dataset_type == \"val\":\n",
    "            idx = idx + self.val_index_start\n",
    "            \n",
    "            if idx >= self.test_index_start:\n",
    "                raise IndexError(f'index{idx} out of range{self.test_index_start}')\n",
    "            \n",
    "        elif self.dataset_type == \"test\":\n",
    "            idx = idx + self.test_index_start\n",
    "            \n",
    "            if idx >= len(self.X):\n",
    "                raise IndexError(f'index{idx} out of range{len(self.X)}')\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('dataset_type not set')\n",
    "\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    @property\n",
    "    def X_shape(self):\n",
    "        return self.X.shape\n",
    "    \n",
    "    @property\n",
    "    def Y_shape(self):\n",
    "        return self.Y.shape\n",
    "\n",
    "    @property\n",
    "    def len_train(self):\n",
    "        return self.train_index_end+1\n",
    "\n",
    "    @property\n",
    "    def len_val(self):\n",
    "        if self.val_index_start is None:\n",
    "            raise ValueError('no validation set defined')\n",
    "        return self.test_index_start-self.val_index_start\n",
    "\n",
    "    @property\n",
    "    def len_test(self):\n",
    "        if self.test_index_start is None:\n",
    "            raise ValueError('no test set defined')\n",
    "        return len(self.Y)-self.test_index_start\n",
    "\n",
    "    def get_all_X(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        \"\"\"\n",
    "        Returns the entire features dataset.\n",
    "        Return either the train, val, test, or all data.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset_type == 'train':\n",
    "            return self.X[:self.val_index_start].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'val':\n",
    "            return self.X[self.val_index_start:self.test_index_start].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'test':\n",
    "            return self.X[self.test_index_start:].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'all':\n",
    "            return self.X.copy() if self.X is not None else None\n",
    "        else:\n",
    "            raise ValueError('dataset_type not recognized')\n",
    "\n",
    "    def get_all_Y(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        \"\"\"\n",
    "        Returns the entire target dataset.\n",
    "        Return either the train, val, test, or all data.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset_type == 'train':\n",
    "            return self.Y[:self.val_index_start].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'val':\n",
    "            return self.Y[self.val_index_start:self.test_index_start].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'test':\n",
    "            return self.Y[self.test_index_start:].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'all':\n",
    "            return self.Y.copy() if self.Y is not None else None\n",
    "        else:\n",
    "            raise ValueError('dataset_type not recognized')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## XYDataLoader\n",
       "\n",
       ">      XYDataLoader (X:numpy.ndarray, Y:numpy.ndarray,\n",
       ">                    val_index_start:Optional[int]=None,\n",
       ">                    test_index_start:Optional[int]=None,\n",
       ">                    lag_window_params:dict=None, normalize_features:dict=None)\n",
       "\n",
       "*A class for datasets with the typicall X, Y structure. Both X\n",
       "and Y are numpy arrays. X may be of shape (datapoints, features) or (datapoints, sequence_length, features) \n",
       "if lag features are used. The prep_lag_features can be used to create those lag features. Y is of shape\n",
       "(datapoints, units).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | ndarray |  |  |\n",
       "| Y | ndarray |  |  |\n",
       "| val_index_start | Optional | None |  |\n",
       "| test_index_start | Optional | None |  |\n",
       "| lag_window_params | dict | None | default: {'lag_window': 0, 'include_y': False, 'pre_calc': False} |\n",
       "| normalize_features | dict | None | default: {'normalize': True, 'ignore_one_hot': True} |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## XYDataLoader\n",
       "\n",
       ">      XYDataLoader (X:numpy.ndarray, Y:numpy.ndarray,\n",
       ">                    val_index_start:Optional[int]=None,\n",
       ">                    test_index_start:Optional[int]=None,\n",
       ">                    lag_window_params:dict=None, normalize_features:dict=None)\n",
       "\n",
       "*A class for datasets with the typicall X, Y structure. Both X\n",
       "and Y are numpy arrays. X may be of shape (datapoints, features) or (datapoints, sequence_length, features) \n",
       "if lag features are used. The prep_lag_features can be used to create those lag features. Y is of shape\n",
       "(datapoints, units).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | ndarray |  |  |\n",
       "| Y | ndarray |  |  |\n",
       "| val_index_start | Optional | None |  |\n",
       "| test_index_start | Optional | None |  |\n",
       "| lag_window_params | dict | None | default: {'lag_window': 0, 'include_y': False, 'pre_calc': False} |\n",
       "| normalize_features | dict | None | default: {'normalize': True, 'ignore_one_hot': True} |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.prep_lag_features\n",
       "\n",
       ">      XYDataLoader.prep_lag_features (lag_window:int=0, include_y:bool=False,\n",
       ">                                      pre_calc:bool=False)\n",
       "\n",
       "*Create lag feature for the dataset. If \"inlcude_y\" is true, then a lag-1 of of the target variable is added as a feature.\n",
       "If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag\n",
       "window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features\n",
       "including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce\n",
       "computation time later on at the expense of increases memory usage.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| lag_window | int | 0 | length of the lage window |\n",
       "| include_y | bool | False | if lag demand shall be included as feature |\n",
       "| pre_calc | bool | False | if all lags are pre-calculated for the entire dataset |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.prep_lag_features\n",
       "\n",
       ">      XYDataLoader.prep_lag_features (lag_window:int=0, include_y:bool=False,\n",
       ">                                      pre_calc:bool=False)\n",
       "\n",
       "*Create lag feature for the dataset. If \"inlcude_y\" is true, then a lag-1 of of the target variable is added as a feature.\n",
       "If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag\n",
       "window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features\n",
       "including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce\n",
       "computation time later on at the expense of increases memory usage.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| lag_window | int | 0 | length of the lage window |\n",
       "| include_y | bool | False | if lag demand shall be included as feature |\n",
       "| pre_calc | bool | False | if all lags are pre-calculated for the entire dataset |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.prep_lag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L174){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.__getitem__\n",
       "\n",
       ">      XYDataLoader.__getitem__ (idx)\n",
       "\n",
       "*get item by index, depending on the dataset type (train, val, test)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L174){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.__getitem__\n",
       "\n",
       ">      XYDataLoader.__getitem__ (idx)\n",
       "\n",
       "*get item by index, depending on the dataset type (train, val, test)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L227){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_X\n",
       "\n",
       ">      XYDataLoader.get_all_X (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire features dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L227){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_X\n",
       "\n",
       ">      XYDataLoader.get_all_X (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire features dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.get_all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L247){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_Y\n",
       "\n",
       ">      XYDataLoader.get_all_Y (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire target dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L247){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_Y\n",
       "\n",
       ">      XYDataLoader.get_all_Y (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire target dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.get_all_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of ```XYDataLoader``` for simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: [ 0.48934242 -1.5249851 ] [-4.20898833]\n",
      "sample shape Y: (1,)\n",
      "length: 100\n"
     ]
    }
   ],
   "source": [
    "X = np.random.standard_normal((100, 2))\n",
    "Y = np.random.standard_normal((100, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X = X, Y = Y)\n",
    "\n",
    "sample_X, sample_Y = dataloader[0]\n",
    "print(\"sample:\", sample_X, sample_Y)\n",
    "print(\"sample shape Y:\", sample_Y.shape)\n",
    "\n",
    "print(\"length:\", len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of ```XYDataLoader``` on how to handle train, val, and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train: 6 length val: 2 length test: 2\n",
      "\n",
      "### Data from train set ###\n",
      "idx: 0 data: [-0.03919358 -1.81524105] [-5.45285373]\n",
      "idx: 1 data: [-0.33238759 -0.25239502] [-2.28257421]\n",
      "idx: 2 data: [-0.28088596  1.04119928] [2.99245932]\n",
      "idx: 3 data: [-1.41942608 -0.14294466] [-3.4857788]\n",
      "idx: 4 data: [-0.94715851 -0.90926992] [-3.88762596]\n",
      "idx: 5 data: [ 0.83067309 -1.60634229] [-2.1943678]\n",
      "\n",
      "### Data from val set ###\n",
      "idx: 0 data: [0.31728831 0.1915907 ] [0.61714501]\n",
      "idx: 1 data: [0.19605744 0.15569143] [-0.31029128]\n",
      "\n",
      "### Data from test set ###\n",
      "idx: 0 data: [ 0.60736842 -1.62492312] [-1.52729382]\n",
      "idx: 1 data: [-0.33198421  0.91780232] [2.45557787]\n",
      "\n",
      "### Data from train set again ###\n",
      "idx: 0 data: [-0.03919358 -1.81524105] [-5.45285373]\n",
      "idx: 1 data: [-0.33238759 -0.25239502] [-2.28257421]\n",
      "idx: 2 data: [-0.28088596  1.04119928] [2.99245932]\n",
      "idx: 3 data: [-1.41942608 -0.14294466] [-3.4857788]\n",
      "idx: 4 data: [-0.94715851 -0.90926992] [-3.88762596]\n",
      "idx: 5 data: [ 0.83067309 -1.60634229] [-2.1943678]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.standard_normal((10, 2))\n",
    "Y = np.random.standard_normal((10, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X = X, Y = Y, val_index_start=6, test_index_start=8)\n",
    "\n",
    "sample_X, sample_Y = dataloader[0]\n",
    "\n",
    "print(\"length train:\", dataloader.len_train, \"length val:\", dataloader.len_val, \"length test:\", dataloader.len_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.val()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from val set ###\")\n",
    "for i in range(dataloader.len_val):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.test()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from test set ###\")\n",
    "for i in range(dataloader.len_test):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.train()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set again ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.60736842, -1.62492312],\n",
       "       [-0.33198421,  0.91780232]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "dataloader.get_all_X('all')\n",
    "dataloader.get_all_X('train')\n",
    "dataloader.get_all_X('val')\n",
    "dataloader.get_all_X('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.52729382],\n",
       "       [ 2.45557787]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "dataloader.get_all_Y('all')\n",
    "dataloader.get_all_Y('train')\n",
    "dataloader.get_all_Y('val')\n",
    "dataloader.get_all_Y('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of ```XYDataLoader``` on how to include lag features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train: 4 length val: 2 length test: 2\n",
      "\n",
      "### Data from train set ###\n",
      "idx: 0 data: [[ 1.31636985  0.7897913  -0.75019741]\n",
      " [-1.0363052  -0.33563099  5.30310363]] [-1.73725544]\n",
      "idx: 1 data: [[-1.0363052  -0.33563099  5.30310363]\n",
      " [ 0.61748301  0.12942096 -1.73725544]] [0.75991095]\n",
      "idx: 2 data: [[ 0.61748301  0.12942096 -1.73725544]\n",
      " [ 0.97507757  0.60012032  0.75991095]] [3.60080094]\n",
      "idx: 3 data: [[0.97507757 0.60012032 0.75991095]\n",
      " [0.05424665 0.05414227 3.60080094]] [1.06903634]\n",
      "\n",
      "### Data from val set ###\n",
      "idx: 0 data: [[ 0.05424665  0.05414227  3.60080094]\n",
      " [-0.19670518  2.25039121  1.06903634]] [6.53359576]\n",
      "idx: 1 data: [[-0.19670518  2.25039121  1.06903634]\n",
      " [-1.84005742 -0.24281547  6.53359576]] [-3.96123686]\n",
      "\n",
      "### Data from test set ###\n",
      "idx: 0 data: [[-1.84005742 -0.24281547  6.53359576]\n",
      " [ 0.53974671  1.48055778 -3.96123686]] [5.10164607]\n",
      "idx: 1 data: [[ 0.53974671  1.48055778 -3.96123686]\n",
      " [ 0.0885949   1.45853039  5.10164607]] [5.47333133]\n",
      "\n",
      "### Data from train set again ###\n",
      "idx: 0 data: [[ 1.31636985  0.7897913  -0.75019741]\n",
      " [-1.0363052  -0.33563099  5.30310363]] [-1.73725544]\n",
      "idx: 1 data: [[-1.0363052  -0.33563099  5.30310363]\n",
      " [ 0.61748301  0.12942096 -1.73725544]] [0.75991095]\n",
      "idx: 2 data: [[ 0.61748301  0.12942096 -1.73725544]\n",
      " [ 0.97507757  0.60012032  0.75991095]] [3.60080094]\n",
      "idx: 3 data: [[0.97507757 0.60012032 0.75991095]\n",
      " [0.05424665 0.05414227 3.60080094]] [1.06903634]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.standard_normal((10, 2))\n",
    "Y = np.random.standard_normal((10, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "\n",
    "lag_window_params = {'lag_window': 1, 'include_y': True, 'pre_calc': True}\n",
    "\n",
    "dataloader = XYDataLoader(X = X, Y = Y, val_index_start=6, test_index_start=8, lag_window_params=lag_window_params)\n",
    "\n",
    "sample_X, sample_Y = dataloader[0]\n",
    "\n",
    "print(\"length train:\", dataloader.len_train, \"length val:\", dataloader.len_val, \"length test:\", dataloader.len_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.val()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from val set ###\")\n",
    "for i in range(dataloader.len_val):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.test()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from test set ###\")\n",
    "for i in range(dataloader.len_test):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.train()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set again ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiShapeLoader(BaseDataLoader):\n",
    "\n",
    "    \"\"\"\n",
    "    A class designed for comlex datasets with mutlipe feature types. The class is more\n",
    "    memory-efficient than the XYDataLoader, as it separate the storeage of SKU-specific\n",
    "    feature, time-specific features, and time-SKU-specific features. The class works generically\n",
    "    as long as those feature classes are provided during pre-processing. The class is designed \n",
    "    to handle classic learning, but able to work in a meta-learning pipeline where no SKU-dimension\n",
    "    is present and the model needs to make prediction on SKU-time level without knowhing the\n",
    "    specific SKU.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        # mandatory data\n",
    "        demand: pd.DataFrame, # Demand data of shape time x SKU\n",
    "        time_features: pd.DataFrame, # Features constant over SKU of shape time x time_features\n",
    "        time_SKU_features: pd.DataFrame, # Features varying over time and SKU of shape time x (time_SKU_features*SKU) with double index\n",
    "        \n",
    "        # optional data\n",
    "        mask: pd.DataFrame = None, # Mask of shape time x SKU telling which SKUs are available at which time (can be used as mask during trainig or added to features)\n",
    "        SKU_features: pd.DataFrame = None, # Features constant over time of shape SKU x SKU_features - only for algorithms learning across SKUs\n",
    "        \n",
    "        val_index_start: Union[int, None] = None, # Validation index start on the time dimension\n",
    "        test_index_start: Union[int, None] = None, # Test index start on the time dimension\n",
    "        in_sample_val_test_SKUs: List = None, # SKUs in the training set to be used for validation and testing, out-of-sample w.r.t. time dimension\n",
    "        out_of_sample_val_SKUs: List = None, # SKUs to be hold-out for validation (can be same as test if no validation on out-of-sample SKUs required)\n",
    "        out_of_sample_test_SKUs: List = None, # SKUs to be hold-out for testing\n",
    "        lag_window_params: dict | None = None, # default: {'lag_window': 0, 'include_y': False, 'pre_calc': True}\n",
    "        normalize_features: dict | None = None, # default: {'normalize': True, 'ignore_one_hot': True}\n",
    "        engineered_SKU_features: Union[dict] = None, # default: [\"mean_demand\", \"std_demand\", \"kurtosis_demand\", \"skewness_demand\", \"percentile_10_demand\", \"percentile_30_demand\", \"median_demand\", \"percentile_70_demand\", \"percentile_90_demand\", \"inter_quartile_range\"]\n",
    "        use_engineered_SKU_features: bool = False, # if engineered features shall be used\n",
    "        include_non_available: bool = False, # if timestep/SKU combination where the SKU was not available for sale shall be included. If included, it will be used as feature, otherwise as mask.\n",
    "        train_subset: int = None ,# if only a subset of SKUs is used for training. Will always contain in_sample_val_test_SKUs and then fills the rest with random SKUs\n",
    "        train_subset_SKUs: List = None, # if train_subset is set, specific SKUs can be provided\n",
    "        meta_learn_units: bool = False, # if units (SKUs) are trained in the batch dimension to meta-learn across SKUs\n",
    "        lag_demand_normalization: Literal['minmax', 'standard', 'no_normalization'] | None = \"standard\", # minmax, standard, no_normalization or None. If None, same demand_normalization\n",
    "        demand_normalization: Literal['minmax', 'standard', 'no_normalization'] = 'no_normalization', # 'standard' or 'minmax'\n",
    "        demand_unit_size: float | None = None, # use same convention as for other dataloaders and enviornments, but here only full decimal values are allowed\n",
    "        provide_additional_target: bool = False, # follows ICL convention by providing actual demand to token, with the last token receiving 0\n",
    "    ):\n",
    "     \n",
    "        logging.info(\"Setting main env attributes\")\n",
    "        # Set data\n",
    "        self.demand = demand\n",
    "        self.SKU_features = SKU_features\n",
    "        self.time_features = time_features\n",
    "        self.time_SKU_features = time_SKU_features\n",
    "        self.mask = mask\n",
    "\n",
    "        # convert dtypes to float\n",
    "        self.demand = self.demand.astype(float)\n",
    "        self.time_features = self.time_features.astype(float)\n",
    "        self.time_SKU_features = self.time_SKU_features.astype(float)\n",
    "        if self.SKU_features is not None:\n",
    "            self.SKU_features = self.SKU_features.astype(float)\n",
    "        if self.mask is not None:\n",
    "            self.mask = self.mask.astype(float)\n",
    "\n",
    "        # Set default values for dict inputs:\n",
    "        normalize_features = normalize_features or {'normalize': True, 'ignore_one_hot': True}\n",
    "        lag_window_params = lag_window_params or {'lag_window': 0, 'include_y': False, 'pre_calc': False}\n",
    "        self.lag_window_params = lag_window_params # lag window parameters saved as attribute\n",
    "        engineered_SKU_features = engineered_SKU_features or [\"mean_demand\", \"std_demand\", \"kurtosis_demand\", \"skewness_demand\", \"percentile_10_demand\", \"percentile_30_demand\", \"median_demand\", \"percentile_70_demand\", \"percentile_90_demand\", \"inter_quartile_range\"]\n",
    "        if not use_engineered_SKU_features:\n",
    "            engineered_SKU_features = None\n",
    "\n",
    "        # Set further training parameters\n",
    "        self.include_non_available = include_non_available\n",
    "        self.meta_learn_units = meta_learn_units\n",
    "        train_subset, train_subset_SKUs = self.set_train_subset(train_subset, train_subset_SKUs) # set the attributes train_subset and train_subset_SKUs\n",
    "        self.lag_demand_normalization = lag_demand_normalization if lag_demand_normalization is not None else demand_normalization\n",
    "        self.demand_normalization = demand_normalization\n",
    "        if demand_unit_size is not None:\n",
    "            self.demand_unit_size = int(-math.log10(demand_unit_size))\n",
    "            if self.demand_unit_size % 1 != 0:\n",
    "                raise ValueError('demand_unit_size must be a full decimal value')\n",
    "            else:\n",
    "                self.demand_unit_size = int(self.demand_unit_size)\n",
    "        else:\n",
    "            self.demand_unit_size = None\n",
    "        self.provide_additional_target = provide_additional_target\n",
    "        \n",
    "        # Some necessary flags\n",
    "        ## Whether the features are already normalized\n",
    "        self.normalized_in_sample_SKUs = False\n",
    "        self.normalized_out_of_sample_SKUs = False\n",
    "        self.added_engineereed_features_to_in_sample_SKUs = False\n",
    "        self.added_engineereed_features_to_out_of_sample_val_SKUs = False\n",
    "        self.added_engineereed_features_to_out_of_sample_test_SKUs = False\n",
    "        ## What data to return in the __getitem__ method\n",
    "        self.SKU_type = \"in_sample\" # or \"out_of_sample_val\" or \"out_of_sample_test\" # What kind of SKU retruning in __getitem__\n",
    "        self.dataset_type = \"train\" # or \"val\" or \"test\", affecting the time-dimension\n",
    "\n",
    "        logging.info(\"Setting indices for validation and test set\")\n",
    "        # Determine train, val, test indices (time dimension only!)\n",
    "        self.val_index_start = val_index_start\n",
    "        self.test_index_start = test_index_start\n",
    "        self.train_index_start = self.lag_window_params[\"lag_window\"] # start index for training data later to allow for lag features\n",
    "        self.train_index_start += self.lag_window_params[\"include_y\"] # if lag demand is included as feature need one more timestep\n",
    "\n",
    "        # train index ends either at the start of the validation set, the start of the test set or at the end of the dataset\n",
    "        if self.val_index_start is not None:\n",
    "            self.train_index_end = self.val_index_start-1\n",
    "        elif self.test_index_start is not None:\n",
    "            self.train_index_end = self.test_index_start-1\n",
    "        else:\n",
    "            self.train_index_end = len(self.demand)-1\n",
    "\n",
    "        logging.info(\"Setting out-of-sample SKUs\")\n",
    "        # set out-of-sample SKUs - note that this is not done by indecing, but the SKUs data will be\n",
    "        # removed from the in-sample data and stored in separate attributes. Therefore, there are no\n",
    "        # index attributes for out-of-sample SKUs. The feature attributes are stored by the function below.\n",
    "        self.out_of_sample_val_SKUs, self.out_of_sample_test_SKUs = self.set_out_of_sample_SKUs(out_of_sample_val_SKUs, out_of_sample_test_SKUs)\n",
    "\n",
    "        if self.out_of_sample_val_SKUs is not None and self.out_of_sample_test_SKUs is not None:\n",
    "            if len(self.out_of_sample_val_SKUs) != 0 or len(self.out_of_sample_test_SKUs) != 0:\n",
    "                self.out_of_sample = True\n",
    "            else:\n",
    "                self.out_of_sample = False\n",
    "        else:\n",
    "            self.out_of_sample = False\n",
    "\n",
    "        logging.info(\"Identifying training and in-sample validation and test SKUs\")\n",
    "        self.in_sample_val_test_SKUs = self.set_in_sample_val_test_SKUs(in_sample_val_test_SKUs) # need to determine index later after out-of-sample units are removed\n",
    "        self.train_SKUs = self.identify_train_SKUs(train_subset, train_subset_SKUs) # need to determine index later after out-of-sample units are removed \n",
    "        if self.in_sample_val_test_SKUs is None:\n",
    "            self.in_sample_val_test_SKUs = self.train_SKUs # validation and training SKUs are teh same\n",
    "\n",
    "        logging.info(\"Determine number of units and features\")\n",
    "        # Num units is relevant for the output dimension when validating and testing. If the model is not trained as a \n",
    "        # meta learner it is identicall in traing, and validation/testing. If it is trained as a meta learner, the output\n",
    "        # dimension is the number of in_sample_val_test_SKUs, iregardless of the number of SKUs in the training set.\n",
    "        self.num_units = len(self.in_sample_val_test_SKUs) if self.in_sample_val_test_SKUs is not None else len(self.demand.columns)\n",
    "        \n",
    "        # Determine number of features\n",
    "        self.num_time_SKU_features = len(self.time_SKU_features.columns.get_level_values(0).unique())\n",
    "        if lag_window_params[\"include_y\"]:\n",
    "            self.num_time_SKU_features += 1 # lag demand is time and SKU specific\n",
    "        if include_non_available:\n",
    "            self.num_time_SKU_features += 1 # non-availability is time and SKU specific\n",
    "        if self.provide_additional_target:\n",
    "            self.num_time_SKU_features += 1\n",
    "        self.num_time_features = len(self.time_features.columns)\n",
    "        self.num_SKU_features = len(self.SKU_features.columns) if self.SKU_features is not None else 0\n",
    "        if engineered_SKU_features is not None:\n",
    "            self.num_SKU_features += len(engineered_SKU_features) # engineered features are SKU specific\n",
    "\n",
    "        # print(\"xxxxxxx Number features per type:\")\n",
    "        # print(\"num_time_SKU_features:\", self.num_time_SKU_features)\n",
    "        # print(\"num_time_features:\", self.num_time_features)\n",
    "        # print(\"num_SKU_features:\", self.num_SKU_features)\n",
    "\n",
    "        # Determine total number of features per datapoint\n",
    "        self.num_features = self.num_SKU_features + self.num_time_features + self.num_time_SKU_features\n",
    "\n",
    "        if engineered_SKU_features is not None:\n",
    "            logging.info(\"Creating engineered SKU features for training data\")\n",
    "            engineered_SKU_features_data = self.build_engineered_SKU_features(engineered_SKU_features, self.demand.iloc[:self.train_index_end+1]) # only for training data initially\n",
    "            self.SKU_features = pd.concat([self.SKU_features, engineered_SKU_features_data.transpose()], axis=1)\n",
    "            self.added_engineereed_features_to_in_sample_SKUs = True\n",
    "\n",
    "            logging.info(\"Creating engineered SKU features for out-of-sample validation data\")\n",
    "            engineered_SKU_features_data = self.build_engineered_SKU_features(engineered_SKU_features, self.demand_out_of_sample_val) # only for out-of-sample validation data\n",
    "            self.SKU_features_out_of_sample_val = pd.concat([self.SKU_features_out_of_sample_val, engineered_SKU_features_data.transpose()], axis=1)\n",
    "            self.added_engineereed_features_to_out_of_sample_val_SKUs = True\n",
    "\n",
    "            logging.info(\"Creating engineered SKU features for out-of-sample test data\")\n",
    "            engineered_SKU_features_data = self.build_engineered_SKU_features(engineered_SKU_features, self.demand_out_of_sample_test) # only for out-of-sample test data\n",
    "            self.SKU_features_out_of_sample_test = pd.concat([self.SKU_features_out_of_sample_test, engineered_SKU_features_data.transpose()], axis=1)\n",
    "            self.added_engineereed_features_to_out_of_sample_test_SKUs = True\n",
    "\n",
    "        logging.info(\"Normalizing in-sample SKU features (based on training timesteps)\")\n",
    "        self.normalize_demand_and_features_in_sample(normalize=normalize_features[\"normalize\"], ignore_one_hot=normalize_features[\"ignore_one_hot\"],initial_normalization=True)\n",
    "\n",
    "        if self.out_of_sample:\n",
    "            logging.info(\"Normalizing out-of-sample SKU features\")\n",
    "            self.normalize_demand_and_features_out_of_sample(normalize=normalize_features[\"normalize\"], ignore_one_hot=normalize_features[\"ignore_one_hot\"],initial_normalization=True)\n",
    "\n",
    "        logging.info(\"Saving data as numpy and saving indices\")\n",
    "        # store row and column indices of demand, SKU_features time_features mask and then convert to numpy array\n",
    "        logging.info(\"--Saving indices\")\n",
    "\n",
    "        ############ in sample data ############\n",
    "        self.in_sample_val_test_SKUs_indices = self.demand.columns.get_indexer(self.in_sample_val_test_SKUs) if self.in_sample_val_test_SKUs is not None else None\n",
    "\n",
    "        self.train_SKUs_indices = self.demand.columns.get_indexer(self.train_SKUs)\n",
    "\n",
    "        self.demand_indices = self.save_indices(self.demand)\n",
    "        self.SKU_features_indices = self.save_indices(self.SKU_features) if self.SKU_features is not None else None\n",
    "        self.time_features_indices = self.save_indices(self.time_features)\n",
    "        self.time_SKU_features_indices = self.save_indices(self.time_SKU_features)\n",
    "        self.mask_indices = self.save_indices(self.mask)\n",
    "\n",
    "        logging.info(\"--Converting to numpy - in sample\")\n",
    "        self.demand = self.demand.to_numpy()\n",
    "        self.demand_lag = self.demand_lag.to_numpy()\n",
    "        self.SKU_features = self.SKU_features.to_numpy() if self.SKU_features is not None else None\n",
    "        self.time_features = self.time_features.to_numpy()\n",
    "        self.time_SKU_features = self.time_SKU_features.to_numpy()\n",
    "        self.mask = self.mask.to_numpy()\n",
    "\n",
    "        ############ out of sample data ############\n",
    "\n",
    "        if self.out_of_sample:\n",
    "            self.out_of_sample_val_SKUs_indices = self.demand_out_of_sample_val.columns.get_indexer(self.out_of_sample_val_SKUs)\n",
    "            self.out_of_sample_test_SKUs_indices = self.demand_out_of_sample_test.columns.get_indexer(self.out_of_sample_test_SKUs)\n",
    "\n",
    "            self.demand_out_of_sample_val_indices = self.save_indices(self.demand_out_of_sample_val)\n",
    "            self.SKU_features_out_of_sample_val_indices = self.save_indices(self.SKU_features_out_of_sample_val) if self.SKU_features_out_of_sample_val is not None else None\n",
    "            self.time_SKU_features_out_of_sample_val_indices = self.save_indices(self.time_SKU_features_out_of_sample_val)\n",
    "            self.mask_out_of_sample_val_indices = self.save_indices(self.mask_out_of_sample_val)\n",
    "        \n",
    "            logging.info(\"--Converting to numpy - out of sample val\")\n",
    "            self.demand_out_of_sample_val = self.demand_out_of_sample_val.to_numpy()\n",
    "            self.demand_lag_out_of_sample_val = self.demand_lag_out_of_sample_val.to_numpy()\n",
    "            self.SKU_features_out_of_sample_val = self.SKU_features_out_of_sample_val.to_numpy() if self.SKU_features_out_of_sample_val is not None else None\n",
    "            self.time_SKU_features_out_of_sample_val = self.time_SKU_features_out_of_sample_val.to_numpy()\n",
    "            self.mask_out_of_sample_val = self.mask_out_of_sample_val.to_numpy()\n",
    "\n",
    "            self.demand_out_of_sample_test_indices = self.save_indices(self.demand_out_of_sample_test)\n",
    "            self.SKU_features_out_of_sample_test_indices = self.save_indices(self.SKU_features_out_of_sample_test) if self.SKU_features_out_of_sample_test is not None else None\n",
    "            self.time_SKU_features_out_of_sample_test_indices = self.save_indices(self.time_SKU_features_out_of_sample_test)\n",
    "            self.mask_out_of_sample_test_indices = self.save_indices(self.mask_out_of_sample_test)\n",
    "\n",
    "            logging.info(\"--Converting to numpy - out of sample test\")\n",
    "            self.demand_out_of_sample_test = self.demand_out_of_sample_test.to_numpy()\n",
    "            self.demand_lag_out_of_sample_test = self.demand_lag_out_of_sample_test.to_numpy()\n",
    "            self.SKU_features_out_of_sample_test = self.SKU_features_out_of_sample_test.to_numpy() if self.SKU_features_out_of_sample_test is not None else None\n",
    "            self.time_SKU_features_out_of_sample_test = self.time_SKU_features_out_of_sample_test.to_numpy()\n",
    "            self.mask_out_of_sample_test = self.mask_out_of_sample_test.to_numpy()\n",
    "\n",
    "        \n",
    "\n",
    "        ############ final params ############\n",
    "        self.len_train_time = self.train_index_end-self.train_index_start+1\n",
    "\n",
    "        print(\"len_train_time:\", self.len_train_time)\n",
    "        print(\"num_units:\", self.num_units)\n",
    "        print(\"len train_SKU_indices:\", len(self.train_SKUs_indices))\n",
    "\n",
    "        if self.meta_learn_units:\n",
    "            logging.info(\"--Creating time-SKU index for training data\")\n",
    "            self.sku_time_index = [(i, j) for i in self.train_SKUs_indices for j in range(self.len_train_time)]\n",
    "\n",
    "        self.set_return_sku(\"in_sample\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def set_train_subset(self, train_subset, train_subset_SKUs):\n",
    "        \"\"\" Prepare setting the attributes train_subset and train_subset_SKUs \"\"\"\n",
    "\n",
    "        if train_subset is not None and train_subset < 1:\n",
    "            raise ValueError('train_subset must be an integer larger than 0')\n",
    "\n",
    "        if train_subset_SKUs is not None and not isinstance(train_subset_SKUs, list):\n",
    "            raise ValueError(f'train_subset_SKUs must be a list of SKUs, got {type(train_subset_SKUs)}')\n",
    "\n",
    "        if train_subset_SKUs is not None:\n",
    "            if train_subset is None:\n",
    "                logging.info(\"--Infering train_subset from train_subset_SKUs\")\n",
    "                train_subset = len(train_subset_SKUs)\n",
    "            else:\n",
    "                if train_subset != len(train_subset_SKUs):\n",
    "                    raise ValueError('train_subset_SKUs must have the same length as train_subset')\n",
    "        \n",
    "        return train_subset, train_subset_SKUs\n",
    "\n",
    "    def set_in_sample_val_test_SKUs(self, in_sample_val_test_SKUs):\n",
    "\n",
    "        if in_sample_val_test_SKUs is None:\n",
    "            in_sample_val_test_SKUs_indices = None\n",
    "        else:\n",
    "            if not self.meta_learn_units:\n",
    "                raise ValueError('in_sample_val_test_SKUs can only be set if meta_learn_units is True/n \\\n",
    "                otherwise the output dimension needs to be the same for training and validation/testing')\n",
    "            if not set(in_sample_val_test_SKUs).issubset(self.demand.columns):\n",
    "\n",
    "                missing_SKUs = 0\n",
    "                names = []\n",
    "                for i in in_sample_val_test_SKUs:\n",
    "                    if i not in self.demand.columns:\n",
    "                        missing_SKUs += 1\n",
    "                        names.append(i)\n",
    "                \n",
    "                print(names)\n",
    "            \n",
    "\n",
    "                raise ValueError('in_sample_val_test_SKUs must be a subset of all SKUs')\n",
    "\n",
    "        return in_sample_val_test_SKUs\n",
    "            \n",
    "    def identify_train_SKUs(self, train_subset, train_subset_SKUs):\n",
    "        \"\"\" determine which SKUs are used for training, validation and testing \"\"\"\n",
    "        \n",
    "        if train_subset is not None:\n",
    "\n",
    "            if train_subset_SKUs is not None:\n",
    "                # check that all train_SKUs are in demand.collumns\n",
    "                if not set(train_subset_SKUs).issubset(self.demand.columns):\n",
    "                    raise ValueError('train_subset_SKUs must be a subset of all training SKUs')\n",
    "                if self.in_sample_val_test_SKUs is not None:\n",
    "                    if not set(self.in_sample_val_test_SKUs).issubset(train_subset_SKUs):\n",
    "                        raise ValueError('train_subset_SKUs must contain in_sample_val_test_SKUs')\n",
    "            else:\n",
    "                if self.in_sample_val_test_SKUs is not None and train_subset < len(self.in_sample_val_test_SKUs):\n",
    "                    raise ValueError(f'train_subset ({train_subset}) must be equal or larger than the number of in_sample_val_test_SKUs ({len(self.in_sample_val_test_SKUs)})')\n",
    "                train_subset_SKUs = self.in_sample_val_test_SKUs if self.in_sample_val_test_SKUs is not None else []\n",
    "                remaining_SKUs = self.demand.columns.difference(train_subset_SKUs)\n",
    "                additional_SKUs = np.random.choice(remaining_SKUs, train_subset-len(train_subset_SKUs), replace=False)\n",
    "                train_SKUs = np.concatenate((train_subset_SKUs, additional_SKUs))\n",
    "                # order train_SKUs as in demand.columns\n",
    "                train_subset_SKUs = [sku for sku in self.demand.columns if sku in train_SKUs]\n",
    "        else:\n",
    "            train_subset_SKUs = self.demand.columns # val and test SKUs have been removed before, only training SKUs remain \n",
    "\n",
    "        return train_subset_SKUs\n",
    "    \n",
    "    def set_out_of_sample_SKUs(self,\n",
    "            out_of_sample_val_SKUs: List | np.ndarray | None, # SKUs for validation\n",
    "            out_of_sample_test_SKUs: List | np.ndarray | None, # SKUs for testing. If same dataset shall be used, it can be identical\n",
    "            ):\n",
    "\n",
    "        \"\"\"\n",
    "        Sets out-of-sample SKUs for validation and test datasets, and removes them from the in-sample data.\n",
    "        It handles the cases that no out-of-sample SKUs are set (both inputs are None), one of them is set,\n",
    "        both of them are set to different values, and both of them are identical. In-sample data remain,\n",
    "        while out-of-sample data are moved to seperate attributes.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Note: one of the two might be None, both might be None, or both might be identical\n",
    "\n",
    "        self.test_out_of_sample_SKUs(out_of_sample_val_SKUs, out_of_sample_test_SKUs)\n",
    "\n",
    "        if out_of_sample_val_SKUs is None:\n",
    "            out_of_sample_val_SKUs = []\n",
    "        if out_of_sample_test_SKUs is None:\n",
    "            out_of_sample_test_SKUs = []\n",
    "\n",
    "        for sku, attr_suffix in [(out_of_sample_val_SKUs, 'val'), (out_of_sample_test_SKUs, 'test')]:\n",
    "            if sku is not None:\n",
    "                logging.info(f\"--Setting out-of-sample {attr_suffix} SKUs\")\n",
    "                # Set attributes for out-of-sample SKUs\n",
    "                setattr(self, f'demand_out_of_sample_{attr_suffix}', self.demand.loc[:, sku])\n",
    "                if self.SKU_features is not None:\n",
    "                    setattr(self, f'SKU_features_out_of_sample_{attr_suffix}', self.SKU_features.loc[sku])\n",
    "                setattr(self, f'time_SKU_features_out_of_sample_{attr_suffix}', # here SKU are in columns on index level 2\n",
    "                        self.time_SKU_features.loc[:, pd.IndexSlice[:, sku]]) \n",
    "                setattr(self, f'mask_out_of_sample_{attr_suffix}', self.mask.loc[:, sku])\n",
    "                # time_features are independent of SKU, so no need to set them\n",
    "\n",
    "        # unique values of uniion of both lists\n",
    "        skus_to_remove = list(set(out_of_sample_val_SKUs).union(out_of_sample_test_SKUs))\n",
    "        if skus_to_remove:\n",
    "            logging.info(f\"--Removing {len(skus_to_remove)} out-of-sample SKUs from in-sample data\")\n",
    "            # Remove out-of-sample SKUs from in-sample data\n",
    "            self.demand.drop(columns=skus_to_remove, inplace=True)\n",
    "            if self.SKU_features is not None:\n",
    "                self.SKU_features.drop(index=skus_to_remove, inplace=True)\n",
    "            for single_sku in skus_to_remove:\n",
    "                columns_to_drop = self.time_SKU_features.columns.get_loc_level(single_sku, level=1)\n",
    "                self.time_SKU_features.drop(columns=self.time_SKU_features.columns[columns_to_drop[0]], inplace=True)\n",
    "            self.mask.drop(columns=skus_to_remove, inplace=True)\n",
    "\n",
    "        return out_of_sample_val_SKUs, out_of_sample_test_SKUs\n",
    "\n",
    "    def test_out_of_sample_SKUs(self, out_of_sample_val_SKUs, out_of_sample_test_SKUs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Validates the out-of-sample SKU lists for compatibility. Ensures that the inputs are either None, lists, or numpy arrays,\n",
    "        and that they meet specific conditions regarding uniqueness and overlap.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if inputs are either None, list, or numpy array\n",
    "        if not (isinstance(out_of_sample_val_SKUs, (list, np.ndarray)) or out_of_sample_val_SKUs is None):\n",
    "            raise ValueError(\"out_of_sample_val_SKUs must be a list, numpy array, or None.\")\n",
    "        if not (isinstance(out_of_sample_test_SKUs, (list, np.ndarray)) or out_of_sample_test_SKUs is None):\n",
    "            raise ValueError(\"out_of_sample_test_SKUs must be a list, numpy array, or None.\")\n",
    "\n",
    "        # Convert None to empty list for easier processing\n",
    "        if out_of_sample_val_SKUs is None:\n",
    "            out_of_sample_val_SKUs = []\n",
    "        if out_of_sample_test_SKUs is None:\n",
    "            out_of_sample_test_SKUs = []\n",
    "\n",
    "        # Ensure all entries are unique within each list\n",
    "        if len(out_of_sample_val_SKUs) != len(set(out_of_sample_val_SKUs)):\n",
    "            raise ValueError(\"out_of_sample_val_SKUs contains duplicate entries.\")\n",
    "        if len(out_of_sample_test_SKUs) != len(set(out_of_sample_test_SKUs)):\n",
    "            raise ValueError(\"out_of_sample_test_SKUs contains duplicate entries.\")\n",
    "\n",
    "        # Ensure there are no overlapping SKUs between validation and test sets (unless they are meant to be identical)\n",
    "\n",
    "        if out_of_sample_val_SKUs is not None and out_of_sample_test_SKUs is not None:\n",
    "            overlap = set(out_of_sample_val_SKUs).intersection(set(out_of_sample_test_SKUs))\n",
    "\n",
    "            if len(overlap) > 0:\n",
    "                if len(overlap) == len(out_of_sample_val_SKUs):\n",
    "                    logging.warning(\"--Out of sample validation and test SKUs are identical - ensure this is intended.\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Validation and test SKUs must not overlap. Overlapping SKUs: {overlap}\")\n",
    "\n",
    "        logging.debug(\"--Out-of-sample SKUs have passed validation checks.\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def build_engineered_SKU_features(engineered_SKU_features: List, demand: pd.DataFrame):\n",
    "\n",
    "        \"\"\"\n",
    "        Create engineered features for each SKU\n",
    "        \"\"\"\n",
    "\n",
    "        feature_names = []\n",
    "        feature_values = []\n",
    "\n",
    "        for feature in engineered_SKU_features:\n",
    "\n",
    "            if feature == \"mean_demand\":\n",
    "                mean_demand = demand.mean(axis=0)\n",
    "            elif feature == \"std_demand\":\n",
    "                std_demand = demand.std(axis=0)\n",
    "            elif feature == \"kurtosis_demand\":\n",
    "                kurtosis_demand = demand.kurtosis(axis=0)\n",
    "            elif feature == \"skewness_demand\":\n",
    "                skewness_demand = demand.skew(axis=0)\n",
    "            elif feature == \"percentile_10_demand\":\n",
    "                percentile_10_demand = demand.quantile(0.1, axis=0)\n",
    "            elif feature == \"percentile_30_demand\":\n",
    "                percentile_30_demand = demand.quantile(0.3, axis=0)\n",
    "            elif feature == \"median_demand\":\n",
    "                median_demand = demand.median(axis=0)\n",
    "            elif feature == \"percentile_70_demand\":\n",
    "                percentile_70_demand = demand.quantile(0.7, axis=0)\n",
    "            elif feature == \"percentile_90_demand\":\n",
    "                percentile_90_demand = demand.quantile(0.9, axis=0)\n",
    "            elif feature == \"inter_quartile_range\":\n",
    "                inter_quartile_range = demand.quantile(0.75, axis=0) - demand.quantile(0.25, axis=0)\n",
    "            else:  \n",
    "                raise ValueError(f'Feature {feature} not recognized')\n",
    "            \n",
    "            feature_names.append(feature)\n",
    "            feature_values.append(locals()[feature])\n",
    "\n",
    "        return pd.DataFrame(feature_values, columns=demand.columns, index=feature_names)\n",
    "    \n",
    "    def normalize_demand_and_features_in_sample(self,\n",
    "        normalize: bool = True,\n",
    "        ignore_one_hot: bool = True,\n",
    "        initial_normalization = False # Flag if it is set before having added lag features\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Normalize features using a standard scaler. If ignore_one_hot is true, one-hot encoded features are not normalized.\n",
    "        \"\"\"\n",
    "\n",
    "        if normalize:\n",
    "\n",
    "            if self.normalized_in_sample_SKUs:\n",
    "                raise ValueError('Features already normalized')\n",
    "\n",
    "            if self.demand_normalization == 'minmax':\n",
    "                self.scaler_demand = MinMaxScaler()\n",
    "                self.scaler_out_of_sample_val_demand = MinMaxScaler()\n",
    "                self.scaler_out_of_sample_test_demand = MinMaxScaler()\n",
    "            elif self.demand_normalization == 'standard':\n",
    "                self.scaler_demand = StandardScaler()\n",
    "                self.scaler_out_of_sample_val_demand = StandardScaler()\n",
    "                self.scaler_out_of_sample_test_demand = StandardScaler()\n",
    "            elif self.demand_normalization == 'no_normalization':\n",
    "                self.scaler_demand = None\n",
    "                self.scaler_out_of_sample_val_demand = None\n",
    "                self.scaler_out_of_sample_test_demand = None\n",
    "            else:\n",
    "                raise ValueError('demand_normalization must be either \"minmax\", \"standard\", or \"no_normalization\"')\n",
    "            \n",
    "            # If demand data used for lag feautures is normalized differently\n",
    "            if self.lag_demand_normalization != self.demand_normalization:\n",
    "                if self.lag_demand_normalization == 'minmax':\n",
    "                    self.scaler_demand_lag = MinMaxScaler()\n",
    "                    self.scaler_out_of_sample_val_demand_lag = MinMaxScaler()\n",
    "                    self.scaler_out_of_sample_test_demand_lag = MinMaxScaler()\n",
    "                elif self.lag_demand_normalization == 'standard':\n",
    "                    self.scaler_demand_lag = StandardScaler()\n",
    "                    self.scaler_out_of_sample_val_demand_lag = StandardScaler()\n",
    "                    self.scaler_out_of_sample_test_demand_lag = StandardScaler()\n",
    "                elif self.lag_demand_normalization == 'no_normalization':\n",
    "                    self.scaler_demand_lag = None\n",
    "                    self.scaler_out_of_sample_val_demand_lag = None\n",
    "                    self.scaler_out_of_sample_test_demand_lag = None\n",
    "                else:\n",
    "                    raise ValueError('lag_demand_normalization must be either \"minmax\", \"standard\", or \"no_normalization\"')\n",
    "            \n",
    "            self.scaler_SKU_features = StandardScaler() if self.SKU_features is not None else None # only one since out of sample uses the same fit on known  skus\n",
    "            self.scaler_time_features = StandardScaler() # only one since time-features are shared between in-sample and out-of-sample SKUs\n",
    "            self.scaler_time_SKU_features= [StandardScaler() for _ in range(self.num_time_SKU_features)]\n",
    "            self.scaler_out_of_sample_val_SKU_features = [StandardScaler() for _ in range(self.num_time_SKU_features)]\n",
    "            self.scaler_out_of_sample_test_SKU_features = [StandardScaler() for _ in range(self.num_time_SKU_features)]\n",
    "\n",
    "            if initial_normalization:\n",
    "\n",
    "                logging.info(\"--Normalizing demand\")\n",
    "\n",
    "                # Prepare data for lag_demand\n",
    "                self.demand_lag = self.demand.copy() # store original demand values for lag demand\n",
    "\n",
    "                # Normalize demand targets\n",
    "                if self.demand_normalization != 'no_normalization':\n",
    "                    # Normalizing per SKU on time dimension\n",
    "                    self.scaler_demand.fit(self.demand[:self.train_index_end+1])\n",
    "                    transformed_demand = self.scaler_demand.transform(self.demand)\n",
    "                    self.demand.iloc[:,:] = transformed_demand\n",
    "\n",
    "                # Set unit size for demand targets\n",
    "                if self.demand_unit_size != None:\n",
    "                    self.demand = np.round(self.demand, self.demand_unit_size)\n",
    "\n",
    "                # If separate normalization for lag demand, normalize it\n",
    "                if self.lag_demand_normalization != self.demand_normalization:\n",
    "                    if self.lag_demand_normalization != 'no_normalization':\n",
    "                        self.demand_lag = self.demand.copy() # if lag demand shall be normalized, build on the normalized demand (to account for slight variations due to rounding)\n",
    "                        self.scaler_demand_lag.fit(self.demand_lag[:self.train_index_end+1])\n",
    "                        transformed_demand_lag = self.scaler_demand_lag.transform(self.demand_lag)\n",
    "                        self.demand_lag.iloc[:,:] = transformed_demand_lag\n",
    "                \n",
    "                # If lag demand shall be normalized the same way, then copy the normalized demand\n",
    "                else:\n",
    "                    self.demand_lag = self.demand.copy()\n",
    "\n",
    "                if self.SKU_features is not None:\n",
    "                    logging.info(\"--Normalizing SKU features\")\n",
    "                    # Normalizing across SKUs, no time dimension present\n",
    "                    continuous_features = [col for col in self.SKU_features.columns if not self.is_one_hot(self.SKU_features[col])] if ignore_one_hot else self.SKU_features.columns\n",
    "                    self.SKU_features_to_fit = continuous_features\n",
    "                    if len(continuous_features) > 0:\n",
    "                        self.scaler_SKU_features.fit(self.SKU_features[continuous_features]) # SKU features are already calculated based on training index\n",
    "                        transformed_SKU_features = self.scaler_SKU_features.transform(self.SKU_features[continuous_features])\n",
    "                        self.SKU_features[continuous_features] = transformed_SKU_features\n",
    "\n",
    "                logging.info(\"--Normalizing time features\")\n",
    "                # Normalizting time features (no SKU dimension)\n",
    "                continuous_features = [col for col in self.time_features.columns if not self.is_one_hot(self.time_features[col])] if ignore_one_hot else self.time_features.columns\n",
    "                self.time_features_to_fit = continuous_features\n",
    "                if len(continuous_features) > 0:\n",
    "                    self.scaler_time_features.fit(self.time_features.loc[:, continuous_features].iloc[:self.train_index_end+1, :])\n",
    "                    transformed_time_features = self.scaler_time_features.transform(self.time_features.loc[:,continuous_features])\n",
    "                    self.time_features.loc[:,continuous_features] = transformed_time_features\n",
    "\n",
    "                logging.info(\"--Normalizing time-SKU features\")\n",
    "                # Normalize time-SKU features (double-indexed)\n",
    "                self.time_SKU_features_to_fit = dict()\n",
    "                for i, feature in enumerate(self.time_SKU_features.columns.get_level_values(0).unique()):\n",
    "                    # Select all columns corresponding to the current feature in level 0\n",
    "                    feature_df = self.time_SKU_features.xs(key=feature, axis=1, level=0)\n",
    "\n",
    "                    should_scale = not ignore_one_hot or (not self.is_one_hot_across_skus(feature_df) if ignore_one_hot else False)\n",
    "                    self.time_SKU_features_to_fit.update({feature: should_scale})\n",
    "                    if should_scale:\n",
    "                        self.scaler_time_SKU_features[i].fit(feature_df[:self.train_index_end+1])\n",
    "                        transformed_feature_df = self.scaler_time_SKU_features[i].transform(feature_df)\n",
    "                        self.time_SKU_features.loc[:, (feature, slice(None))] = transformed_feature_df\n",
    "\n",
    "            \n",
    "                self.normalized_in_sample_SKUs = True\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError('Training data can only normalized during initialization - later normlization not implemented yet')\n",
    "\n",
    "    def normalize_demand_and_features_out_of_sample(self,\n",
    "        normalize: bool = True,\n",
    "        ignore_one_hot: bool = True,\n",
    "        initial_normalization = False # Flag if it is set before having added lag features\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Normalize features using a standard scaler. If ignore_one_hot is true, one-hot encoded features are not normalized.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.out_of_sample_val_SKUs is None and self.out_of_sample_test_SKUs is None:\n",
    "            return\n",
    "\n",
    "        if normalize:\n",
    "\n",
    "            if self.normalized_out_of_sample_SKUs:\n",
    "                raise ValueError('Features already normalized')\n",
    "\n",
    "            if initial_normalization:\n",
    "\n",
    "                logging.info(\"--Normalizing demand\")\n",
    "\n",
    "                # Prepare data for lag_demand\n",
    "\n",
    "                self.demand_lag_out_of_sample_test = self.demand_out_of_sample_test.copy()\n",
    "                self.demand_lag_out_of_sample_val = self.demand_out_of_sample_val.copy()\n",
    "\n",
    "                # Normalize demand targets\n",
    "                if self.demand_normalization != 'no_normalization':\n",
    "                    # Normalizing per SKU on time dimension\n",
    "                    self.scaler_out_of_sample_test_demand.fit(self.demand_out_of_sample_test[:self.train_index_end+1])\n",
    "                    transformed_demand = self.scaler_out_of_sample_test_demand.transform(self.demand_lag_out_of_sample_test)\n",
    "                    self.demand_out_of_sample_test.iloc[:,:] = transformed_demand\n",
    "\n",
    "                    self.scaler_out_of_sample_val_demand.fit(self.demand_out_of_sample_val[:self.train_index_end+1])\n",
    "                    transformed_demand = self.scaler_out_of_sample_val_demand.transform(self.demand_lag_out_of_sample_val)\n",
    "                    self.demand_out_of_sample_val.iloc[:,:] = transformed_demand\n",
    "                \n",
    "                # Set unit size for demand targets\n",
    "                if self.demand_unit_size != None:\n",
    "                    self.demand_out_of_sample_test = np.round(self.demand_out_of_sample_test, self.demand_unit_size)\n",
    "                    self.demand_out_of_sample_val = np.round(self.demand_out_of_sample_val, self.demand_unit_size)\n",
    "\n",
    "                # If separate normalization for lag demand, normalize it\n",
    "                if self.lag_demand_normalization != self.demand_normalization:\n",
    "                    if self.lag_demand_normalization != 'no_normalization':\n",
    "                        \n",
    "                        self.demand_lag_out_of_sample_test = self.demand_out_of_sample_test.copy()\n",
    "                        self.scaler_out_of_sample_test_demand_lag.fit(self.demand_lag_out_of_sample_test[:self.train_index_end+1])\n",
    "                        transformed_demand_lag = self.scaler_out_of_sample_test_demand_lag.transform(self.demand_lag_out_of_sample_test)\n",
    "                        self.demand_lag_out_of_sample_test.iloc[:,:] = transformed_demand_lag\n",
    "                        \n",
    "                        self.demand_lag_out_of_sample_val = self.demand_out_of_sample_val.copy()\n",
    "                        self.scaler_out_of_sample_val_demand_lag.fit(self.demand_lag_out_of_sample_val[:self.train_index_end+1])\n",
    "                        transformed_demand_lag = self.scaler_out_of_sample_val_demand_lag.transform(self.demand_lag_out_of_sample_val)\n",
    "                        self.demand_lag_out_of_sample_val.iloc[:,:] = transformed_demand_lag\n",
    "                    \n",
    "                # If lag demand shall be normalized the same way, then copy the normalized demand\n",
    "                else:\n",
    "                    self.demand_lag_out_of_sample_test = self.demand_out_of_sample_test.copy()\n",
    "                    self.demand_lag_out_of_sample_val = self.demand_out_of_sample_val.copy()\n",
    "                    \n",
    "                if self.SKU_features is not None:\n",
    "                    logging.info(\"--Normalizing SKU features\")\n",
    "                    # Normalizing across SKUs, no time dimension present\n",
    "                    continuous_features = self.SKU_features_to_fit\n",
    "                    if len(continuous_features) > 0:\n",
    "                        transformed_SKU_features = self.scaler_SKU_features.transform(self.SKU_features_out_of_sample_test[continuous_features])\n",
    "                        self.SKU_features_out_of_sample_test[continuous_features] = transformed_SKU_features\n",
    "                    \n",
    "                    if len(continuous_features) > 0:\n",
    "                        transformed_SKU_features = self.scaler_SKU_features.transform(self.SKU_features_out_of_sample_val[continuous_features])\n",
    "                        self.SKU_features_out_of_sample_val[continuous_features] = transformed_SKU_features\n",
    "\n",
    "                logging.info(\"--Normalizing time-SKU features\")\n",
    "                \n",
    "                for i, feature in enumerate(self.time_SKU_features_out_of_sample_test.columns.get_level_values(0).unique()):\n",
    "                    # Select all columns corresponding to the current feature in level 0\n",
    "                    feature_df = self.time_SKU_features_out_of_sample_test.xs(key=feature, axis=1, level=0)\n",
    "\n",
    "                    should_scale = self.time_SKU_features_to_fit[feature]\n",
    "                    if should_scale:\n",
    "                        self.scaler_out_of_sample_test_SKU_features[i].fit(feature_df[:self.train_index_end+1])\n",
    "                        transformed_feature_df = self.scaler_out_of_sample_test_SKU_features[i].transform(feature_df)\n",
    "                        self.time_SKU_features_out_of_sample_test.loc[:, (feature, slice(None))] = transformed_feature_df\n",
    "                \n",
    "                for i, feature in enumerate(self.time_SKU_features_out_of_sample_val.columns.get_level_values(0).unique()):\n",
    "                    # Select all columns corresponding to the current feature in level 0\n",
    "                    feature_df = self.time_SKU_features_out_of_sample_val.xs(key=feature, axis=1, level=0)\n",
    "\n",
    "                    should_scale = self.time_SKU_features_to_fit[feature]\n",
    "                    if should_scale:\n",
    "                        self.scaler_out_of_sample_val_SKU_features[i].fit(feature_df[:self.train_index_end+1])\n",
    "                        transformed_feature_df = self.scaler_out_of_sample_val_SKU_features[i].transform(feature_df)\n",
    "                        self.time_SKU_features_out_of_sample_val.loc[:, (feature, slice(None))] = transformed_feature_df\n",
    "            \n",
    "                self.normalized_out_of_sample_SKUs = True\n",
    "\n",
    "                print(self.demand_out_of_sample_test)\n",
    "                print(self.demand_out_of_sample_val)\n",
    "\n",
    "                print(self.demand_lag_out_of_sample_test)\n",
    "                print(self.demand_lag_out_of_sample_val)\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError('Training data can only normalized during initialization - later normlization not implemented yet')\n",
    "\n",
    "    def update_lag_features(self,\n",
    "        lag_window: int,\n",
    "        ):\n",
    "\n",
    "        \"\"\" Update lag window parameters for dataloader object that is already initialized \"\"\"\n",
    "\n",
    "        raise NotImplementedError('Not implemented yet')\n",
    "\n",
    "        # Problem: updating lag_features naively would shorten the dataset each time it is called\n",
    "\n",
    "    def get_time_SKU_idx(self, idx):\n",
    "\n",
    "        \"\"\" get time and SKU index by index, depending on the dataset type (train, val, test) \"\"\"\n",
    "\n",
    "        # print(\"mode of dataloader when getting item:\", self.dataset_type)\n",
    "\n",
    "        if self.dataset_type == \"train\":\n",
    "\n",
    "            if self.meta_learn_units:\n",
    "\n",
    "                if idx >= len(self.sku_time_index):\n",
    "                    raise IndexError(f'index {idx} out of range{len(self.sku_time_index)}')\n",
    "                idx_sku, idx_time, = self.sku_time_index[idx]\n",
    "                idx_skus = [idx_sku]\n",
    "\n",
    "            else:\n",
    "                if idx+self.train_index_start > self.train_index_end:\n",
    "                    raise IndexError(f'index {idx} out of range{self.train_index_end-self.train_index_start}')\n",
    "                idx_skus = self.train_SKUs_indices\n",
    "                idx_time = idx\n",
    "            idx_time += self.train_index_start\n",
    "\n",
    "        elif self.dataset_type == \"val\":\n",
    "            idx_time = idx + self.val_index_start\n",
    "            if self.return_SKU_type == \"in_sample\":\n",
    "                if self.in_sample_val_test_SKUs is not None:\n",
    "                    idx_skus = self.in_sample_val_test_SKUs_indices\n",
    "                else:\n",
    "                    idx_skus = self.train_SKUs_indices\n",
    "            elif self.return_SKU_type == \"out_of_sample_val\":\n",
    "                idx_skus = self.out_of_sample_val_SKUs_indices\n",
    "            elif self.return_SKU_type == \"out_of_sample_test\":\n",
    "                idx_skus = self.out_of_sample_test_SKUs_indices\n",
    "            else:\n",
    "                raise ValueError('return_SKU_type not set')\n",
    "            \n",
    "            if idx >= self.test_index_start:\n",
    "                raise IndexError(f'index{idx} out of range{self.test_index_start}')\n",
    "        \n",
    "        elif self.dataset_type == \"test\":\n",
    "            idx_time = idx + self.test_index_start\n",
    "            if self.return_SKU_type == \"in_sample\":\n",
    "                if self.in_sample_val_test_SKUs is not None:\n",
    "                    idx_skus = self.in_sample_val_test_SKUs_indices\n",
    "                else:\n",
    "                    idx_skus = self.train_SKUs_indices\n",
    "            elif self.return_SKU_type == \"out_of_sample_val\":\n",
    "                idx_skus = self.out_of_sample_val_SKUs_indices\n",
    "            elif self.return_SKU_type == \"out_of_sample_test\":\n",
    "                idx_skus = self.out_of_sample_test_SKUs_indices\n",
    "            else:\n",
    "                raise ValueError('return_SKU_type not set')\n",
    "            \n",
    "            if idx >= len(self.demand):\n",
    "                raise IndexError(f'index{idx} out of range{len(self.demand)}')\n",
    "        else:\n",
    "            raise ValueError('dataset_type not set')\n",
    "\n",
    "        return idx_time, idx_skus\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        \"\"\" get item by index, depending on the dataset type (train, val, test)\"\"\"\n",
    "\n",
    "        if self.dataset_type != \"train\":\n",
    "\n",
    "            time_features = self.time_features # time features independent of SKU\n",
    "\n",
    "            if self.return_SKU_type == \"in_sample\":\n",
    "                demand = self.demand\n",
    "                demand_lag = self.demand_lag\n",
    "                SKU_features = self.SKU_features\n",
    "                time_SKU_features = self.time_SKU_features\n",
    "                mask = self.mask\n",
    "                len_SKUs = len(self.train_SKUs)\n",
    "            elif self.return_SKU_type == \"out_of_sample_val\":\n",
    "                demand = self.demand_out_of_sample_val\n",
    "                demand_lag = self.demand_lag_out_of_sample_val\n",
    "                SKU_features = self.SKU_features_out_of_sample_val\n",
    "                time_SKU_features = self.time_SKU_features_out_of_sample_val\n",
    "                mask = self.mask_out_of_sample_val\n",
    "                len_SKUs = len(self.out_of_sample_val_SKUs)\n",
    "            elif self.return_SKU_type == \"out_of_sample_test\":\n",
    "                demand = self.demand_out_of_sample_test\n",
    "                demand_lag = self.demand_lag_out_of_sample_test\n",
    "                SKU_features = self.SKU_features_out_of_sample_test\n",
    "                time_SKU_features = self.time_SKU_features_out_of_sample_test\n",
    "                mask = self.mask_out_of_sample_test\n",
    "                len_SKUs = len(self.out_of_sample_test_SKUs)\n",
    "        \n",
    "        else:\n",
    "            demand = self.demand\n",
    "            demand_lag = self.demand_lag\n",
    "            SKU_features = self.SKU_features\n",
    "            time_SKU_features = self.time_SKU_features\n",
    "            mask = self.mask\n",
    "            len_SKUs = len(self.train_SKUs)\n",
    "        \n",
    "        time_features = self.time_features # time features independent of SKU\n",
    "\n",
    "\n",
    "        lag_window = self.lag_window_params[\"lag_window\"]\n",
    "        include_y = self.lag_window_params[\"include_y\"]\n",
    "\n",
    "        idx_time, idx_skus = self.get_time_SKU_idx(idx)\n",
    "        num_skus = len(idx_skus)\n",
    "\n",
    "        demand = demand[idx_time, idx_skus]\n",
    "        \n",
    "        item = np.empty((1,lag_window+1, self.num_features, num_skus))\n",
    "\n",
    "        if include_y:\n",
    "            assert idx_time-1-lag_window >= 0\n",
    "            lag_demand = demand_lag[idx_time-1-lag_window:idx_time, idx_skus] # need to use t-1 to get the lag  \n",
    "\n",
    "        if self.provide_additional_target:\n",
    "            additional_target = demand_lag[idx_time-lag_window:idx_time+1, idx_skus] # provide target without lag\n",
    "            additional_target[-1, :] = 0 # The transformer cannot see the last target --> this is to be predicted\n",
    "\n",
    "        if self.SKU_features is not None:\n",
    "            SKU_features = SKU_features[idx_skus].transpose()\n",
    "            len_SKU_features = len(SKU_features)\n",
    "            SKU_features = np.expand_dims(SKU_features, axis=0)\n",
    "            SKU_features = np.repeat(SKU_features, repeats=lag_window+1, axis=0) \n",
    "        else:\n",
    "            len_SKU_features = 0\n",
    "\n",
    "        time_features = time_features[idx_time-lag_window:idx_time+1]\n",
    "        len_time_features = time_features.shape[1]\n",
    "        time_features = np.expand_dims(time_features, axis=-1)\n",
    "        time_features = np.repeat(time_features, num_skus, axis=-1)\n",
    "\n",
    "        num_time_SKU_features_without_lag_demand = self.num_time_SKU_features-include_y-self.include_non_available-self.provide_additional_target\n",
    "\n",
    "        for i, idx_sku in enumerate(idx_skus):\n",
    "\n",
    "            SKU_indices = [len_SKUs*i+idx_sku for i in range(num_time_SKU_features_without_lag_demand)]\n",
    "            time_SKU_features_sku = time_SKU_features[idx_time-lag_window:idx_time+1, SKU_indices]\n",
    "            item[:,:,(len_SKU_features+len_time_features):(len_SKU_features+len_time_features+num_time_SKU_features_without_lag_demand),i] = np.expand_dims(time_SKU_features_sku, axis=0)\n",
    "        \n",
    "        item[:,:,:len_SKU_features,:] =  np.expand_dims(SKU_features, axis=0)\n",
    "        \n",
    "        item[:,:,len_SKU_features:(len_SKU_features+len_time_features),:] = np.expand_dims(time_features, axis=0)\n",
    "\n",
    "        extra_info = sum([self.include_non_available, include_y, self.provide_additional_target])\n",
    "        additional_info = np.empty((1,lag_window+1, extra_info, num_skus))\n",
    "\n",
    "        current_index = 0\n",
    "\n",
    "        if self.include_non_available:\n",
    "\n",
    "            additional_info[:, :, current_index, :] = np.expand_dims(mask[idx_time-lag_window:idx_time+1, idx_skus], axis=0)\n",
    "            current_index += 1\n",
    "\n",
    "        if include_y:\n",
    "        \n",
    "            additional_info[:, :, current_index, :] = np.expand_dims(lag_demand, axis=0)\n",
    "            current_index += 1\n",
    "    \n",
    "        if self.provide_additional_target:\n",
    "            \n",
    "            additional_info[:, :, current_index, :] = np.expand_dims(additional_target, axis=0)\n",
    "\n",
    "        item[:,:,-extra_info:,:] = additional_info\n",
    "\n",
    "        if self.meta_learn_units:\n",
    "            if self.dataset_type == \"train\":\n",
    "                if item.shape[-1] == 1:\n",
    "                    item = item.squeeze(-1)\n",
    "                else:\n",
    "                    raise ValueError('SKU as batch, but item has more than one SKU dimension')\n",
    "        else:\n",
    "            if item.shape[-1] != 1:\n",
    "                raise ValueError('Num_units dimension must be 1 if not meta-learning')\n",
    "            else:\n",
    "                item = item.squeeze(-1)\n",
    "\n",
    "        if item.shape[0] != 1:\n",
    "            raise ValueError('Batch dimension must be 1')\n",
    "        item = item.squeeze(0) # remove batch dimension\n",
    " \n",
    "        return item, demand\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.demand)\n",
    "    \n",
    "    @property\n",
    "    def X_shape(self):\n",
    "\n",
    "        if self.meta_learn_units:\n",
    "            return (len(self.time_features), self.lag_window_params[\"lag_window\"]+1, self.num_features)\n",
    "        else:\n",
    "            return (len(self.time_features), self.lag_window_params[\"lag_window\"]+1, self.num_features) # check if there will be a difference.\n",
    "    \n",
    "    @property\n",
    "    def Y_shape(self):\n",
    "        return (len(self.time_features), self.num_units) # using num of val_test SKUs\n",
    "\n",
    "    @property\n",
    "    def len_train(self):\n",
    "        if self.meta_learn_units:\n",
    "            return len(self.sku_time_index) # sku_time_index contains only timesteps that are in the training set and skus in the training set.\n",
    "        else:\n",
    "            return self.len_train_time\n",
    "\n",
    "    @property\n",
    "    def len_val(self):\n",
    "        if self.val_index_start is None:\n",
    "            raise ValueError('no validation set defined')\n",
    "        return self.test_index_start-self.val_index_start # validating and testing is always along the time demension (units are a separate dimension)\n",
    "\n",
    "    @property\n",
    "    def len_test(self):\n",
    "        if self.test_index_start is None:\n",
    "            raise ValueError('no test set defined')\n",
    "        return len(self.demand)-self.test_index_start # validating and testing is always along the time demension (units are a separate dimension)\n",
    "\n",
    "    def get_all_X(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        logging.info(\"Retrieving all X data\")\n",
    "\n",
    "        \"\"\"\n",
    "        Returns the entire features dataset.\n",
    "        Return either the train, val, test, or all data.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"mode of dataloader when getting item:\", self.dataset_type)\n",
    "\n",
    "        if dataset_type == 'train':\n",
    "\n",
    "            X = np.empty((self.len_train, self.lag_window_params[\"lag_window\"]+1, self.num_features))\n",
    "\n",
    "            for i in range(self.len_train):\n",
    "                X[i], _ = self[i]\n",
    "\n",
    "        elif dataset_type == 'val':\n",
    "            raise NotImplementedError('Not implemented yet')\n",
    "        elif dataset_type == 'test':\n",
    "            raise NotImplementedError('Not implemented yet')\n",
    "        elif dataset_type == 'all':\n",
    "            raise NotImplementedError('Not implemented yet')\n",
    "        else:\n",
    "            raise ValueError('dataset_type not recognized')\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_all_Y(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        \"\"\"\n",
    "        Returns the entire target dataset.\n",
    "        Return either the train, val, test, or all data.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"mode of dataloader when getting item:\", self.dataset_type)\n",
    "        if dataset_type == 'train':\n",
    "            return self.demand[self.train_index_start:self.val_index_start, self.train_SKUs_indices]\n",
    "        elif dataset_type == 'val':\n",
    "            raise NotImplementedError('Not implemented yet')\n",
    "        elif dataset_type == 'test':\n",
    "            raise NotImplementedError('Not implemented yet')\n",
    "        elif dataset_type == 'all':\n",
    "            raise NotImplementedError('Not implemented yet')\n",
    "        else:\n",
    "            raise ValueError('dataset_type not recognized')\n",
    "\n",
    "    @staticmethod\n",
    "    def is_one_hot(column):\n",
    "        return set(column.unique()) <= {0, 1}\n",
    "\n",
    "    @staticmethod\n",
    "    def is_one_hot_across_skus(feature_df):\n",
    "        \"\"\"\n",
    "        Check if the set of unique values in a feature across all SKU_ids is {0, 1}.\n",
    "        feature_df: DataFrame slice for a specific feature with SKU_ids as columns.\n",
    "        \"\"\"\n",
    "        flattened_values = feature_df.values.flatten()\n",
    "        \n",
    "        # Check if the unique values in this flattened array are exactly {0, 1}\n",
    "        unique_values = set(flattened_values)\n",
    "        \n",
    "        return unique_values <= {0, 1}\n",
    "\n",
    "    @staticmethod\n",
    "    def save_indices(df):\n",
    "        \"\"\"\n",
    "        Saves the row and column indices of a DataFrame.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'rows': df.index,\n",
    "            'columns': df.columns\n",
    "        }\n",
    "    \n",
    "    def set_return_sku(\n",
    "        self,\n",
    "        sku_type: str = \"in_sample\" # if train, then the dataloader returns in-sample SKUs, other option \"out_of_sample_val\" or \"out_of_sample_test\"\n",
    "        ):\n",
    "       \n",
    "        \"\"\"\n",
    "        Set the SKU type to be returned in the __getitem__ method.\n",
    "        \"\"\"\n",
    "\n",
    "        self.return_SKU_type = sku_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_example = False\n",
    "\n",
    "if run_example:\n",
    "    from ddopnew.datasets.kaggle_m5 import KaggleM5DatasetLoader\n",
    "\n",
    "    data_path = \"/Users/magnus/Documents/02_PhD/Reinforcement_Learning/general_purpose_drl/Newsvendor/kaggle_data\" # For testing purposes, please specify the path to the data on your machine\n",
    "    if data_path is not None:\n",
    "        loader = KaggleM5DatasetLoader(data_path, overwrite=False, product_as_feature=False)\n",
    "        demand, SKU_features, time_features, time_SKU_features, mask = loader.load_dataset()\n",
    "    \n",
    "    val_index_start = len(demand)-300\n",
    "    test_index_start = len(demand)-100\n",
    "\n",
    "    out_of_sample_val_SKUs = [\"HOBBIESit_1_002_CA_1\", \"HOBBIES_1_003_CA_1\"]\n",
    "    out_of_sample_test_SKUs = [\"HOBBIES_1_005_CA_1\", \"FOODS_3_819_WI_3\"]\n",
    "\n",
    "    dataloader = MultiShapeLoader(\n",
    "        demand.copy(),\n",
    "        SKU_features.copy(),\n",
    "        time_features.copy(),\n",
    "        time_SKU_features.copy(),\n",
    "        mask.copy(),\n",
    "        val_index_start=val_index_start,\n",
    "        test_index_start=test_index_start,\n",
    "        # in_sample_val_test_SKUs=[\"FOODS_3_825_WI_3\"],\n",
    "        out_of_sample_val_SKUs=out_of_sample_val_SKUs,\n",
    "        out_of_sample_test_SKUs=out_of_sample_test_SKUs,\n",
    "        lag_window_params = {'lag_window': 5, 'include_y': True, 'pre_calc': False},\n",
    "        # train_subset=300,\n",
    "        # train_subset_SKUs=[\"HOBBIES_1_001_CA_1\", \"HOBBIES_1_012_CA_1\"],\n",
    "        SKU_as_batch = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.__getitem__(49844609) #986 with non-zero lag demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
