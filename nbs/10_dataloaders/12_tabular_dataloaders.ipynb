{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular dataloaders\n",
    "\n",
    "> Dataloaders for tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataloaders.tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union\n",
    "\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XYDataLoader(BaseDataLoader):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    A class for datasets with the typicall X, Y structure. Both X\n",
    "    and Y are numpy arrays. X may be of shape (datapoints, features) or (datapoints, sequence_length, features) \n",
    "    if lag features are used. The prep_lag_features can be used to create those lag features. Y is of shape\n",
    "    (datapoints, units).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        val_index_start: Union[int, None] = None, \n",
    "        test_index_start: Union[int, None] = None, \n",
    "        lag_window_params: Union[dict] = None, # default: {'lag_window': 0, 'include_y': False, 'pre_calc': False}\n",
    "        normalize_features: Union[dict] = None, # default: {'normalize': True, 'ignore_one_hot': True}\n",
    "    ):\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.val_index_start = val_index_start\n",
    "        self.test_index_start = test_index_start\n",
    "\n",
    "        # train index ends either at the start of the validation set, the start of the test set or at the end of the dataset\n",
    "        if self.val_index_start is not None:\n",
    "            self.train_index_end = self.val_index_start-1\n",
    "        elif self.test_index_start is not None:\n",
    "            self.train_index_end = self.test_index_start-1\n",
    "        else:\n",
    "            self.train_index_end = len(Y)-1\n",
    "\n",
    "        self.dataset_type = \"train\"\n",
    "\n",
    "        normalize_features = normalize_features or {'normalize': True, 'ignore_one_hot': True}\n",
    "        lag_window_params = lag_window_params or {'lag_window': 0, 'include_y': False, 'pre_calc': False}\n",
    "\n",
    "        self.normalize_features(**normalize_features, initial_normalization=True)\n",
    "        self.prep_lag_features(**lag_window_params)\n",
    "\n",
    "        # X must at least have datapoint and feature dimension\n",
    "        if len(X.shape) == 1:\n",
    "            self.X = X.reshape(-1, 1)\n",
    "        \n",
    "        # Y must have at least datapoint and unit dimension (even if only one unit is present)\n",
    "        if len(Y.shape) == 1:\n",
    "            self.Y = Y.reshape(-1, 1)\n",
    "\n",
    "        assert len(X) == len(Y), 'X and Y must have the same length'\n",
    "\n",
    "        self.num_units = Y.shape[1] # shape 0 is alsways time, shape 1 is the number of units (e.g., SKUs)\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def normalize_features(self,\n",
    "        normalize: bool = True,\n",
    "        ignore_one_hot: bool = True,\n",
    "        initial_normalization=False # Flag if it is set before having added lag features\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Normalize features using a standard scaler. If ignore_one_hot is true, one-hot encoded features are not normalized.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if normalize:\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            if initial_normalization:\n",
    "\n",
    "                if len(self.X.shape) == 3:\n",
    "                    raise ValueError('Normalization not possible with lag features. Please set initial_normalization=False')\n",
    "            \n",
    "                scaler.fit(self.X[:self.train_index_end+1]) # +1 to include the last training point\n",
    "                scaler.transform(self.X)\n",
    "\n",
    "                if initial_normalization:\n",
    "                    return\n",
    "                else:\n",
    "                    raise NotImplementedError('Normalization after lag features have been set not implemented yet')\n",
    "\n",
    "                    # Idea:\n",
    "                        # remove time dimension\n",
    "                        # normalize features\n",
    "                        # add time_dimension back\n",
    "                    # Problem:\n",
    "                        # usage of prep_lag_features needs to ensure y is not added a second time\n",
    "\n",
    "    def prep_lag_features(self,\n",
    "        lag_window: int = 0, # length of the lage window\n",
    "        include_y: bool = False, # if lag demand shall be included as feature\n",
    "        pre_calc: bool = False # if all lags are pre-calculated for the entire dataset\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Create lag feature for the dataset. If \"inlcude_y\" is true, then a lag-1 of of the target variable is added as a feature.\n",
    "        If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag\n",
    "        window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features\n",
    "        including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce\n",
    "        computation time later on at the expense of increases memory usage. \n",
    "\n",
    "        \"\"\"\n",
    "        # to be discussed: Do we need option to only provide lag demand wihtout lag features?\n",
    "        self.lag_window = lag_window\n",
    "        self.pre_calc = pre_calc\n",
    "        self.include_y = include_y\n",
    "        \n",
    "        if self.pre_calc:\n",
    "            if self.include_y:\n",
    "                # add additional column to X with demand shifted by 1\n",
    "                self.X = np.concatenate((self.X, np.roll(self.Y, 1, axis=0)), axis=1)\n",
    "                self.X = self.X[1:] # remove first row\n",
    "                self.Y = self.Y[1:] # remove first row\n",
    "                \n",
    "                self.val_index_start = self.val_index_start-1\n",
    "                self.test_index_start = self.test_index_start-1\n",
    "                self.train_index_end  = self.train_index_end-1\n",
    "        \n",
    "            if self.lag_window is not None and self.lag_window > 0:\n",
    "\n",
    "                # add lag features as dimention 2 to X (making it dimension (datapoints, sequence_length, features))\n",
    "                X_lag = np.zeros((self.X.shape[0], self.lag_window+1, self.X.shape[1]))\n",
    "                for i in range(self.lag_window+1):\n",
    "                    if i == 0:\n",
    "                        features = self.X\n",
    "                    else:    \n",
    "                        features = self.X[:-i, :]\n",
    "                    X_lag[i:, self.lag_window-i, :] = features\n",
    "                self.X = X_lag[self.lag_window:]\n",
    "                self.Y = self.Y[self.lag_window:]\n",
    "\n",
    "                self.val_index_start = self.val_index_start-self.lag_window\n",
    "                self.test_index_start = self.test_index_start-self.lag_window\n",
    "                self.train_index_end  = self.train_index_end-self.lag_window\n",
    "\n",
    "        else:\n",
    "            self.lag_window = None\n",
    "            self.include_y = False\n",
    "            # add time dimension to X\n",
    "\n",
    "    def update_lag_features(self,\n",
    "        lag_window: int,\n",
    "        ):\n",
    "\n",
    "        \"\"\" Update lag window parameters for dataloader object that is already initialized \"\"\"\n",
    "\n",
    "        raise NotImplementedError('Not implemented yet')\n",
    "\n",
    "        # Problem: updating lag_features naively would shorten the dataset each time it is called\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "\n",
    "        \"\"\" get item by index, depending on the dataset type (train, val, test)\"\"\"\n",
    "\n",
    "        if self.dataset_type == \"train\":\n",
    "            if idx > self.train_index_end:\n",
    "                raise IndexError(f'index {idx} out of range{self.train_index_end}')\n",
    "            idx = idx\n",
    "\n",
    "        elif self.dataset_type == \"val\":\n",
    "            idx = idx + self.val_index_start\n",
    "            \n",
    "            if idx >= self.test_index_start:\n",
    "                raise IndexError(f'index{idx} out of range{self.test_index_start}')\n",
    "            \n",
    "        elif self.dataset_type == \"test\":\n",
    "            idx = idx + self.test_index_start\n",
    "            \n",
    "            if idx >= len(self.X):\n",
    "                raise IndexError(f'index{idx} out of range{len(self.X)}')\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('dataset_type not set')\n",
    "\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    @property\n",
    "    def X_shape(self):\n",
    "        return self.X.shape\n",
    "    \n",
    "    @property\n",
    "    def Y_shape(self):\n",
    "        return self.Y.shape\n",
    "\n",
    "    @property\n",
    "    def len_train(self):\n",
    "        return self.train_index_end+1\n",
    "\n",
    "    @property\n",
    "    def len_val(self):\n",
    "        if self.val_index_start is None:\n",
    "            raise ValueError('no validation set defined')\n",
    "        return self.test_index_start-self.val_index_start\n",
    "\n",
    "    @property\n",
    "    def len_test(self):\n",
    "        if self.test_index_start is None:\n",
    "            raise ValueError('no test set defined')\n",
    "        return len(self.Y)-self.test_index_start\n",
    "\n",
    "    def get_all_X(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        \"\"\"\n",
    "        Returns the entire features dataset.\n",
    "        Return either the train, val, test, or all data.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset_type == 'train':\n",
    "            return self.X[:self.val_index_start].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'val':\n",
    "            return self.X[self.val_index_start:self.test_index_start].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'test':\n",
    "            return self.X[self.test_index_start:].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'all':\n",
    "            return self.X.copy() if self.X is not None else None\n",
    "        else:\n",
    "            raise ValueError('dataset_type not recognized')\n",
    "\n",
    "    def get_all_Y(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        \"\"\"\n",
    "        Returns the entire target dataset.\n",
    "        Return either the train, val, test, or all data.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset_type == 'train':\n",
    "            return self.Y[:self.val_index_start].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'val':\n",
    "            return self.Y[self.val_index_start:self.test_index_start].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'test':\n",
    "            return self.Y[self.test_index_start:].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'all':\n",
    "            return self.Y.copy() if self.Y is not None else None\n",
    "        else:\n",
    "            raise ValueError('dataset_type not recognized')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L16){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## XYDataLoader\n",
       "\n",
       ">      XYDataLoader (X:numpy.ndarray, Y:numpy.ndarray,\n",
       ">                    val_index_start:Optional[int]=None,\n",
       ">                    test_index_start:Optional[int]=None,\n",
       ">                    lag_window_params:dict=None, normalize_features:dict=None)\n",
       "\n",
       "*A class for datasets with the typicall X, Y structure. Both X\n",
       "and Y are numpy arrays. X may be of shape (datapoints, features) or (datapoints, sequence_length, features) \n",
       "if lag features are used. The prep_lag_features can be used to create those lag features. Y is of shape\n",
       "(datapoints, units).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | ndarray |  |  |\n",
       "| Y | ndarray |  |  |\n",
       "| val_index_start | Optional | None |  |\n",
       "| test_index_start | Optional | None |  |\n",
       "| lag_window_params | dict | None | default: {'lag_window': 0, 'include_y': False, 'pre_calc': False} |\n",
       "| normalize_features | dict | None | default: {'normalize': True, 'ignore_one_hot': True} |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L16){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## XYDataLoader\n",
       "\n",
       ">      XYDataLoader (X:numpy.ndarray, Y:numpy.ndarray,\n",
       ">                    val_index_start:Optional[int]=None,\n",
       ">                    test_index_start:Optional[int]=None,\n",
       ">                    lag_window_params:dict=None, normalize_features:dict=None)\n",
       "\n",
       "*A class for datasets with the typicall X, Y structure. Both X\n",
       "and Y are numpy arrays. X may be of shape (datapoints, features) or (datapoints, sequence_length, features) \n",
       "if lag features are used. The prep_lag_features can be used to create those lag features. Y is of shape\n",
       "(datapoints, units).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | ndarray |  |  |\n",
       "| Y | ndarray |  |  |\n",
       "| val_index_start | Optional | None |  |\n",
       "| test_index_start | Optional | None |  |\n",
       "| lag_window_params | dict | None | default: {'lag_window': 0, 'include_y': False, 'pre_calc': False} |\n",
       "| normalize_features | dict | None | default: {'normalize': True, 'ignore_one_hot': True} |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L107){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.prep_lag_features\n",
       "\n",
       ">      XYDataLoader.prep_lag_features (lag_window:int=0, include_y:bool=False,\n",
       ">                                      pre_calc:bool=False)\n",
       "\n",
       "*Create lag feature for the dataset. If \"inlcude_y\" is true, then a lag-1 of of the target variable is added as a feature.\n",
       "If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag\n",
       "window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features\n",
       "including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce\n",
       "computation time later on at the expense of increases memory usage.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| lag_window | int | 0 | length of the lage window |\n",
       "| include_y | bool | False | if lag demand shall be included as feature |\n",
       "| pre_calc | bool | False | if all lags are pre-calculated for the entire dataset |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L107){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.prep_lag_features\n",
       "\n",
       ">      XYDataLoader.prep_lag_features (lag_window:int=0, include_y:bool=False,\n",
       ">                                      pre_calc:bool=False)\n",
       "\n",
       "*Create lag feature for the dataset. If \"inlcude_y\" is true, then a lag-1 of of the target variable is added as a feature.\n",
       "If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag\n",
       "window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features\n",
       "including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce\n",
       "computation time later on at the expense of increases memory usage.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| lag_window | int | 0 | length of the lage window |\n",
       "| include_y | bool | False | if lag demand shall be included as feature |\n",
       "| pre_calc | bool | False | if all lags are pre-calculated for the entire dataset |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.prep_lag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L169){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.__getitem__\n",
       "\n",
       ">      XYDataLoader.__getitem__ (idx)\n",
       "\n",
       "*get item by index, depending on the dataset type (train, val, test)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L169){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.__getitem__\n",
       "\n",
       ">      XYDataLoader.__getitem__ (idx)\n",
       "\n",
       "*get item by index, depending on the dataset type (train, val, test)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L222){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_X\n",
       "\n",
       ">      XYDataLoader.get_all_X (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire features dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L222){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_X\n",
       "\n",
       ">      XYDataLoader.get_all_X (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire features dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.get_all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L242){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_Y\n",
       "\n",
       ">      XYDataLoader.get_all_Y (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire target dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/dataloaders/tabular.py#L242){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### XYDataLoader.get_all_Y\n",
       "\n",
       ">      XYDataLoader.get_all_Y (dataset_type:str='train')\n",
       "\n",
       "*Returns the entire target dataset.\n",
       "Return either the train, val, test, or all data.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str | train | can be 'train', 'val', 'test', 'all' |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(XYDataLoader.get_all_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of ```XYDataLoader``` for simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: [-0.42385982  0.7019158 ] [1.13521106]\n",
      "sample shape Y: (1,)\n",
      "length: 100\n"
     ]
    }
   ],
   "source": [
    "X = np.random.standard_normal((100, 2))\n",
    "Y = np.random.standard_normal((100, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X = X, Y = Y)\n",
    "\n",
    "sample_X, sample_Y = dataloader[0]\n",
    "print(\"sample:\", sample_X, sample_Y)\n",
    "print(\"sample shape Y:\", sample_Y.shape)\n",
    "\n",
    "print(\"length:\", len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of ```XYDataLoader``` on how to handle train, val, and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train: 6 length val: 2 length test: 2\n",
      "\n",
      "### Data from train set ###\n",
      "idx: 0 data: [-0.37166809  0.71075037] [1.65376876]\n",
      "idx: 1 data: [-0.21804472  0.91068277] [2.33257268]\n",
      "idx: 2 data: [-1.04216162 -0.0807868 ] [-1.98027247]\n",
      "idx: 3 data: [-1.62845905  0.313468  ] [-1.9377411]\n",
      "idx: 4 data: [-0.74723207 -1.00907747] [-5.34868116]\n",
      "idx: 5 data: [ 0.67368307 -2.17422889] [-6.41373988]\n",
      "\n",
      "### Data from val set ###\n",
      "idx: 0 data: [-0.51178383  1.03651835] [2.87778826]\n",
      "idx: 1 data: [ 0.03225077 -0.03668587] [1.2431435]\n",
      "\n",
      "### Data from test set ###\n",
      "idx: 0 data: [-0.25727476 -1.37935847] [-4.14031444]\n",
      "idx: 1 data: [-0.12235761  2.29179115] [6.23461214]\n",
      "\n",
      "### Data from train set again ###\n",
      "idx: 0 data: [-0.37166809  0.71075037] [1.65376876]\n",
      "idx: 1 data: [-0.21804472  0.91068277] [2.33257268]\n",
      "idx: 2 data: [-1.04216162 -0.0807868 ] [-1.98027247]\n",
      "idx: 3 data: [-1.62845905  0.313468  ] [-1.9377411]\n",
      "idx: 4 data: [-0.74723207 -1.00907747] [-5.34868116]\n",
      "idx: 5 data: [ 0.67368307 -2.17422889] [-6.41373988]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.standard_normal((10, 2))\n",
    "Y = np.random.standard_normal((10, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X = X, Y = Y, val_index_start=6, test_index_start=8)\n",
    "\n",
    "sample_X, sample_Y = dataloader[0]\n",
    "\n",
    "print(\"length train:\", dataloader.len_train, \"length val:\", dataloader.len_val, \"length test:\", dataloader.len_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.val()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from val set ###\")\n",
    "for i in range(dataloader.len_val):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.test()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from test set ###\")\n",
    "for i in range(dataloader.len_test):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.train()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set again ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25727476, -1.37935847],\n",
       "       [-0.12235761,  2.29179115]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "dataloader.get_all_X('all')\n",
    "dataloader.get_all_X('train')\n",
    "dataloader.get_all_X('val')\n",
    "dataloader.get_all_X('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.14031444],\n",
       "       [ 6.23461214]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "dataloader.get_all_Y('all')\n",
    "dataloader.get_all_Y('train')\n",
    "dataloader.get_all_Y('val')\n",
    "dataloader.get_all_Y('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of ```XYDataLoader``` on how to include lag features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train: 4 length val: 2 length test: 2\n",
      "\n",
      "### Data from train set ###\n",
      "idx: 0 data: [[-2.05550983  1.00697184 -2.09458771]\n",
      " [-0.99485469 -0.51723711 -0.95039455]] [-5.04795275]\n",
      "idx: 1 data: [[-0.99485469 -0.51723711 -0.95039455]\n",
      " [-2.03886764 -0.0509625  -5.04795275]] [-4.11651972]\n",
      "idx: 2 data: [[-2.03886764 -0.0509625  -5.04795275]\n",
      " [-0.03441691  0.93914279 -4.11651972]] [2.01628296]\n",
      "idx: 3 data: [[-0.03441691  0.93914279 -4.11651972]\n",
      " [ 0.93748207 -0.61476079  2.01628296]] [1.91951511]\n",
      "\n",
      "### Data from val set ###\n",
      "idx: 0 data: [[ 0.93748207 -0.61476079  2.01628296]\n",
      " [ 0.97973217  0.08565794  1.91951511]] [2.44694631]\n",
      "idx: 1 data: [[ 0.97973217  0.08565794  1.91951511]\n",
      " [-0.24381243 -0.6729163   2.44694631]] [-1.03887324]\n",
      "\n",
      "### Data from test set ###\n",
      "idx: 0 data: [[-0.24381243 -0.6729163   2.44694631]\n",
      " [-2.75827219 -0.29493011 -1.03887324]] [-6.96114421]\n",
      "idx: 1 data: [[-2.75827219 -0.29493011 -1.03887324]\n",
      " [-0.20028243  0.73914439 -6.96114421]] [1.84460677]\n",
      "\n",
      "### Data from train set again ###\n",
      "idx: 0 data: [[-2.05550983  1.00697184 -2.09458771]\n",
      " [-0.99485469 -0.51723711 -0.95039455]] [-5.04795275]\n",
      "idx: 1 data: [[-0.99485469 -0.51723711 -0.95039455]\n",
      " [-2.03886764 -0.0509625  -5.04795275]] [-4.11651972]\n",
      "idx: 2 data: [[-2.03886764 -0.0509625  -5.04795275]\n",
      " [-0.03441691  0.93914279 -4.11651972]] [2.01628296]\n",
      "idx: 3 data: [[-0.03441691  0.93914279 -4.11651972]\n",
      " [ 0.93748207 -0.61476079  2.01628296]] [1.91951511]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.standard_normal((10, 2))\n",
    "Y = np.random.standard_normal((10, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "\n",
    "lag_window_params = {'lag_window': 1, 'include_y': True, 'pre_calc': True}\n",
    "\n",
    "dataloader = XYDataLoader(X = X, Y = Y, val_index_start=6, test_index_start=8, lag_window_params=lag_window_params)\n",
    "\n",
    "sample_X, sample_Y = dataloader[0]\n",
    "\n",
    "print(\"length train:\", dataloader.len_train, \"length val:\", dataloader.len_val, \"length test:\", dataloader.len_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.val()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from val set ###\")\n",
    "for i in range(dataloader.len_val):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.test()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from test set ###\")\n",
    "for i in range(dataloader.len_test):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)\n",
    "\n",
    "dataloader.train()\n",
    "\n",
    "print(\"\")\n",
    "print(\"### Data from train set again ###\")\n",
    "for i in range(dataloader.len_train):\n",
    "    sample_X, sample_Y = dataloader[i]\n",
    "    print(\"idx:\", i, \"data:\", sample_X, sample_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiSourceDataLoader(BaseDataLoader):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    A class for datasets that combine multiple sources of data. The data is provided as a list of numpy arrays.\n",
    "    It converts the data into X,Y pairs at runtime. It also has the capability to handle the multi-product case\n",
    "    with a meta-learning approach across products.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        val_index_start: Union[int, None] = None, \n",
    "        test_index_start: Union[int, None] = None, \n",
    "        lag_window_params: Union[dict] = None, # default: {'lag_window': 0, 'include_y': False, 'pre_calc': False}\n",
    "        normalize_features: Union[dict] = None, # default: {'normalize': True, 'ignore_one_hot': True}\n",
    "    ):\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.val_index_start = val_index_start\n",
    "        self.test_index_start = test_index_start\n",
    "\n",
    "        # train index ends either at the start of the validation set, the start of the test set or at the end of the dataset\n",
    "        if self.val_index_start is not None:\n",
    "            self.train_index_end = self.val_index_start-1\n",
    "        elif self.test_index_start is not None:\n",
    "            self.train_index_end = self.test_index_start-1\n",
    "        else:\n",
    "            self.train_index_end = len(Y)-1\n",
    "\n",
    "        self.dataset_type = \"train\"\n",
    "\n",
    "        normalize_features = normalize_features or {'normalize': True, 'ignore_one_hot': True}\n",
    "        lag_window_params = lag_window_params or {'lag_window': 0, 'include_y': False, 'pre_calc': False}\n",
    "\n",
    "        self.normalize_features(**normalize_features, initial_normalization=True)\n",
    "        self.prep_lag_features(**lag_window_params)\n",
    "\n",
    "        # X must at least have datapoint and feature dimension\n",
    "        if len(X.shape) == 1:\n",
    "            self.X = X.reshape(-1, 1)\n",
    "        \n",
    "        # Y must have at least datapoint and unit dimension (even if only one unit is present)\n",
    "        if len(Y.shape) == 1:\n",
    "            self.Y = Y.reshape(-1, 1)\n",
    "\n",
    "        assert len(X) == len(Y), 'X and Y must have the same length'\n",
    "\n",
    "        self.num_units = Y.shape[1] # shape 0 is alsways time, shape 1 is the number of units (e.g., SKUs)\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def normalize_features(self,\n",
    "        normalize: bool = True,\n",
    "        ignore_one_hot: bool = True,\n",
    "        initial_normalization=False # Flag if it is set before having added lag features\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Normalize features using a standard scaler. If ignore_one_hot is true, one-hot encoded features are not normalized.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if normalize:\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            if initial_normalization:\n",
    "\n",
    "                if len(self.X.shape) == 3:\n",
    "                    raise ValueError('Normalization not possible with lag features. Please set initial_normalization=False')\n",
    "            \n",
    "                scaler.fit(self.X[:self.train_index_end+1]) # +1 to include the last training point\n",
    "                scaler.transform(self.X)\n",
    "\n",
    "                if initial_normalization:\n",
    "                    return\n",
    "                else:\n",
    "                    raise NotImplementedError('Normalization after lag features have been set not implemented yet')\n",
    "\n",
    "                    # Idea:\n",
    "                        # remove time dimension\n",
    "                        # normalize features\n",
    "                        # add time_dimension back\n",
    "                    # Problem:\n",
    "                        # usage of prep_lag_features needs to ensure y is not added a second time\n",
    "\n",
    "    def prep_lag_features(self,\n",
    "        lag_window: int = 0, # length of the lage window\n",
    "        include_y: bool = False, # if lag demand shall be included as feature\n",
    "        pre_calc: bool = False # if all lags are pre-calculated for the entire dataset\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Create lag feature for the dataset. If \"inlcude_y\" is true, then a lag-1 of of the target variable is added as a feature.\n",
    "        If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag\n",
    "        window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features\n",
    "        including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce\n",
    "        computation time later on at the expense of increases memory usage. \n",
    "\n",
    "        \"\"\"\n",
    "        # to be discussed: Do we need option to only provide lag demand wihtout lag features?\n",
    "        self.lag_window = lag_window\n",
    "        self.pre_calc = pre_calc\n",
    "        self.include_y = include_y\n",
    "        \n",
    "        if self.pre_calc:\n",
    "            if self.include_y:\n",
    "                # add additional column to X with demand shifted by 1\n",
    "                self.X = np.concatenate((self.X, np.roll(self.Y, 1, axis=0)), axis=1)\n",
    "                self.X = self.X[1:] # remove first row\n",
    "                self.Y = self.Y[1:] # remove first row\n",
    "                \n",
    "                self.val_index_start = self.val_index_start-1\n",
    "                self.test_index_start = self.test_index_start-1\n",
    "                self.train_index_end  = self.train_index_end-1\n",
    "        \n",
    "            if self.lag_window is not None and self.lag_window > 0:\n",
    "\n",
    "                # add lag features as dimention 2 to X (making it dimension (datapoints, sequence_length, features))\n",
    "                X_lag = np.zeros((self.X.shape[0], self.lag_window+1, self.X.shape[1]))\n",
    "                for i in range(self.lag_window+1):\n",
    "                    if i == 0:\n",
    "                        features = self.X\n",
    "                    else:    \n",
    "                        features = self.X[:-i, :]\n",
    "                    X_lag[i:, self.lag_window-i, :] = features\n",
    "                self.X = X_lag[self.lag_window:]\n",
    "                self.Y = self.Y[self.lag_window:]\n",
    "\n",
    "                self.val_index_start = self.val_index_start-self.lag_window\n",
    "                self.test_index_start = self.test_index_start-self.lag_window\n",
    "                self.train_index_end  = self.train_index_end-self.lag_window\n",
    "\n",
    "        else:\n",
    "            self.lag_window = None\n",
    "            self.include_y = False\n",
    "            # add time dimension to X\n",
    "\n",
    "    def update_lag_features(self,\n",
    "        lag_window: int,\n",
    "        ):\n",
    "\n",
    "        \"\"\" Update lag window parameters for dataloader object that is already initialized \"\"\"\n",
    "\n",
    "        raise NotImplementedError('Not implemented yet')\n",
    "\n",
    "        # Problem: updating lag_features naively would shorten the dataset each time it is called\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "\n",
    "        \"\"\" get item by index, depending on the dataset type (train, val, test)\"\"\"\n",
    "\n",
    "        if self.dataset_type == \"train\":\n",
    "            if idx > self.train_index_end:\n",
    "                raise IndexError(f'index {idx} out of range{self.train_index_end}')\n",
    "            idx = idx\n",
    "\n",
    "        elif self.dataset_type == \"val\":\n",
    "            idx = idx + self.val_index_start\n",
    "            \n",
    "            if idx >= self.test_index_start:\n",
    "                raise IndexError(f'index{idx} out of range{self.test_index_start}')\n",
    "            \n",
    "        elif self.dataset_type == \"test\":\n",
    "            idx = idx + self.test_index_start\n",
    "            \n",
    "            if idx >= len(self.X):\n",
    "                raise IndexError(f'index{idx} out of range{len(self.X)}')\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('dataset_type not set')\n",
    "\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    @property\n",
    "    def X_shape(self):\n",
    "        return self.X.shape\n",
    "    \n",
    "    @property\n",
    "    def Y_shape(self):\n",
    "        return self.Y.shape\n",
    "\n",
    "    @property\n",
    "    def len_train(self):\n",
    "        return self.train_index_end+1\n",
    "\n",
    "    @property\n",
    "    def len_val(self):\n",
    "        if self.val_index_start is None:\n",
    "            raise ValueError('no validation set defined')\n",
    "        return self.test_index_start-self.val_index_start\n",
    "\n",
    "    @property\n",
    "    def len_test(self):\n",
    "        if self.test_index_start is None:\n",
    "            raise ValueError('no test set defined')\n",
    "        return len(self.Y)-self.test_index_start\n",
    "\n",
    "    def get_all_X(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        \"\"\"\n",
    "        Returns the entire features dataset.\n",
    "        Return either the train, val, test, or all data.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset_type == 'train':\n",
    "            return self.X[:self.val_index_start].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'val':\n",
    "            return self.X[self.val_index_start:self.test_index_start].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'test':\n",
    "            return self.X[self.test_index_start:].copy() if self.X is not None else None\n",
    "        elif dataset_type == 'all':\n",
    "            return self.X.copy() if self.X is not None else None\n",
    "        else:\n",
    "            raise ValueError('dataset_type not recognized')\n",
    "\n",
    "    def get_all_Y(self,\n",
    "                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'\n",
    "                ): \n",
    "\n",
    "        \"\"\"\n",
    "        Returns the entire target dataset.\n",
    "        Return either the train, val, test, or all data.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset_type == 'train':\n",
    "            return self.Y[:self.val_index_start].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'val':\n",
    "            return self.Y[self.val_index_start:self.test_index_start].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'test':\n",
    "            return self.Y[self.test_index_start:].copy() if self.Y is not None else None\n",
    "        elif dataset_type == 'all':\n",
    "            return self.Y.copy() if self.Y is not None else None\n",
    "        else:\n",
    "            raise ValueError('dataset_type not recognized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
