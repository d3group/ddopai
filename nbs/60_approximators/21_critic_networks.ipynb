{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximators\n",
    "\n",
    "> Models that approximate a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp RL_approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging_level = logging.DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Tuple, List\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RNNWrapper(nn.Module):\n",
    "    def __init__(self, rnn_cell_class, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the RNNWrapper with the specified RNN cell.\n",
    "        \n",
    "        Parameters:\n",
    "        - rnn_cell_class: The RNN cell class (e.g., nn.GRU, nn.LSTM, nn.RNN).\n",
    "        - *args, **kwargs: The arguments and keyword arguments to be passed to the RNN cell.\n",
    "        \"\"\"\n",
    "        super(RNNWrapper, self).__init__()\n",
    "        self.rnn = rnn_cell_class(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)  # Extract and return only the output\n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, rnn_cell_class):\n",
    "        \"\"\"\n",
    "        A factory method to create a new RNNWrapper subclass with a specific RNN cell.\n",
    "        \n",
    "        Parameters:\n",
    "        - rnn_cell_class: The RNN cell class to be wrapped (e.g., nn.GRU, nn.LSTM).\n",
    "        \n",
    "        Returns:\n",
    "        - A new subclass of RNNWrapper.\n",
    "        \"\"\"\n",
    "        class SpecificRNNWrapper(cls):\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super(SpecificRNNWrapper, self).__init__(rnn_cell_class, *args, **kwargs)\n",
    "\n",
    "        return SpecificRNNWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseApproximator():\n",
    "\n",
    "    \"\"\" Some basic functions for approximators \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def init_weights(self, layer, init_method, activation):\n",
    "        \"\"\" Initialize the weights of a layer \"\"\"\n",
    "        init_method_function = self.select_init_method(init_method)\n",
    "        \n",
    "        # Check if initialization method requires gain\n",
    "        if init_method in [\"xavier_uniform\", \"xavier_normal\"]:\n",
    "            if activation == \"identity\":\n",
    "                activation = \"linear\"\n",
    "            gain = nn.init.calculate_gain(activation)\n",
    "            init_method_function(layer.weight, gain=gain)\n",
    "        else:\n",
    "            init_method_function(layer.weight)\n",
    "            \n",
    "    def init_rnn_weights(self, rnn_layer, init_method):\n",
    "        \"\"\"Initialize the weights for the RNN layer.\"\"\"\n",
    "        init_method_function = self.select_init_method(init_method)\n",
    "\n",
    "        for name, param in rnn_layer.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                # Initialize weights using the selected method\n",
    "                init_method_function(param)\n",
    "            elif 'bias' in name:\n",
    "                # Initialize biases to zero\n",
    "                nn.init.constant_(param, 0)\n",
    "                \n",
    "    @staticmethod\n",
    "    def select_rnn_cell(RNN_cell):\n",
    "        \"\"\" Select the RNN cell based on input string \"\"\"\n",
    "        RNN_cell = RNN_cell.lower()  # Convert input to lowercase for consistency\n",
    "        if RNN_cell == \"gru\":\n",
    "            rnn_cell_class = nn.GRU\n",
    "        elif RNN_cell == \"lstm\":\n",
    "            rnn_cell_class = nn.LSTM\n",
    "        elif RNN_cell == \"rnn\":\n",
    "            rnn_cell_class = nn.RNN\n",
    "        else:\n",
    "            raise ValueError(f\"RNN cell '{RNN_cell}' not recognized\")\n",
    "\n",
    "        return RNNWrapper.create(rnn_cell_class)\n",
    "\n",
    "    @staticmethod\n",
    "    def select_init_method(init_method):\n",
    "        \"\"\" Select the initialization method based on input string \"\"\"\n",
    "        init_method = init_method.lower()\n",
    "        if init_method in [\"xavier_uniform\", \"xavier\"]:\n",
    "            return nn.init.xavier_uniform_\n",
    "        elif init_method in [\"xavier_normal\", \"xaviernorm\"]:\n",
    "            return nn.init.xavier_normal_\n",
    "        elif init_method in [\"he_uniform\", \"kaiming_uniform\"]:\n",
    "            return nn.init.kaiming_uniform_\n",
    "        elif init_method in [\"he_normal\", \"kaiming_normal\"]:\n",
    "            return nn.init.kaiming_normal_\n",
    "        elif init_method in [\"normal\", \"gaussian\"]:\n",
    "            return nn.init.normal_\n",
    "        elif init_method == \"uniform\":\n",
    "            return nn.init.uniform_\n",
    "        else:\n",
    "            raise ValueError(\"Initialization method not recognized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def select_activation(activation):\n",
    "        \"\"\" Select the activation function based on input string \"\"\"\n",
    "        activation = activation.lower()  # Convert input to lowercase for consistency\n",
    "        if activation == \"relu\":\n",
    "            return nn.ReLU\n",
    "        elif activation == \"sigmoid\":\n",
    "            return nn.Sigmoid\n",
    "        elif activation == \"tanh\":\n",
    "            return nn.Tanh\n",
    "        elif activation == \"elu\":\n",
    "            return nn.ELU\n",
    "        elif activation == \"leakyrelu\":\n",
    "            return nn.LeakyReLU\n",
    "        elif activation == \"identity\":\n",
    "            return nn.Identity\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function {activation} not recognized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through the network - overwrite this if necessary \"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseApproximatorMLP(BaseApproximator, nn.Module):\n",
    "\n",
    "    \"\"\" Some basic functions for approximators \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def build_MLP(  self,\n",
    "                    input_size: int,\n",
    "                    output_size: int,\n",
    "                    hidden_layers: list,\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0,\n",
    "                    batch_norm: bool = False,\n",
    "                    final_activation: str = \"identity\",\n",
    "                    init_method: str = \"xavier_uniform\" # Parameter for initialization\n",
    "                  ):\n",
    "\n",
    "        \"\"\" Builds a multi-layer perceptron (MLP) \"\"\"\n",
    "\n",
    "        HiddenActivation = self.select_activation(activation)\n",
    "        FinalActivation = self.select_activation(final_activation)\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Hidden layers\n",
    "        last_size = input_size\n",
    "        for num_neurons in hidden_layers:\n",
    "            \n",
    "            linear_layer = nn.Linear(last_size, num_neurons)\n",
    "            self.init_weights(linear_layer, init_method, activation)\n",
    "            layers.append(linear_layer)\n",
    "\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(num_neurons))\n",
    "        \n",
    "            layers.append(HiddenActivation())\n",
    "            layers.append(nn.Dropout(p=drop_prob))\n",
    "\n",
    "            last_size = num_neurons\n",
    "        \n",
    "        # Output layer\n",
    "        output_layer = nn.Linear(last_size, output_size)\n",
    "        self.init_weights(output_layer, init_method, final_activation)\n",
    "        layers.append(output_layer)\n",
    "        layers.append(FinalActivation())\n",
    "\n",
    "        # Combine layers\n",
    "        model = nn.Sequential(*layers)\n",
    "\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RNNMLPHybrid(nn.Module, BaseApproximator):\n",
    "\n",
    "    \"\"\" A hybrid model combining an RNN and an MLP \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 RNN_input_size: int,\n",
    "                 MLP_input_size: int | None,\n",
    "                 output_size: int,\n",
    "                 num_hidden_units_RNN: int,\n",
    "                 hidden_layers_RNN: int,\n",
    "                 hidden_layers_MLP: List[int],\n",
    "                 hidden_layers_input_MLP: List[int] | None,\n",
    "                 RNN_cell: nn.Module,\n",
    "                 activation: nn.Module,\n",
    "                 final_activation: nn.Module,\n",
    "                 drop_prob: float,\n",
    "                 batch_norm: bool,\n",
    "                 init_method: str):\n",
    "        super(RNNMLPHybrid, self).__init__()\n",
    "\n",
    "        HiddenActivation = self.select_activation(activation)\n",
    "        FinalActivation = self.select_activation(final_activation)\n",
    "        RNNCell = self.select_rnn_cell(RNN_cell)\n",
    "\n",
    "        # RNN\n",
    "        # RNN layers\n",
    "\n",
    "        rnn_layers = []\n",
    "        rnn = RNNCell(RNN_input_size, num_hidden_units_RNN, hidden_layers_RNN, batch_first=True, dropout=drop_prob)\n",
    "        self.init_rnn_weights(rnn, init_method)\n",
    "        hidden_activation_rnn = HiddenActivation()  # Activation used after RNN layers\n",
    "        rnn_layers.append(rnn)\n",
    "        rnn_layers.append(hidden_activation_rnn)\n",
    "\n",
    "        self.rnn = nn.Sequential(*rnn_layers)\n",
    "\n",
    "        # Input MLP, if required:\n",
    "        last_size = 0 if MLP_input_size is None else MLP_input_size\n",
    "        if hidden_layers_input_MLP is not None:\n",
    "\n",
    "            if last_size == 0:\n",
    "                raise ValueError(\"MLP input size must be specified if input MLP is used\")\n",
    "            \n",
    "            layers_input_MLP = []\n",
    "            for num_neurons in hidden_layers_input_MLP:\n",
    "                linear_layer = nn.Linear(last_size, num_neurons)\n",
    "                self.init_weights(linear_layer, init_method, activation)\n",
    "                layers_input_MLP.append(linear_layer)\n",
    "                if batch_norm:\n",
    "                    layers_input_MLP.append(nn.BatchNorm1d(num_neurons))\n",
    "                layers_input_MLP.append(HiddenActivation())\n",
    "                layers_input_MLP.append(nn.Dropout(p=drop_prob))\n",
    "                last_size = num_neurons\n",
    "            \n",
    "            self.input_mlp = nn.Sequential(*layers_input_MLP)\n",
    "        else:\n",
    "            self.input_mlp = None\n",
    "        \n",
    "        # Main MLP layers\n",
    "        layers_MLP = []\n",
    "        last_size = num_hidden_units_RNN + last_size\n",
    "        for num_neurons in hidden_layers_MLP:\n",
    "            linear_layer = nn.Linear(last_size, num_neurons)\n",
    "            self.init_weights(linear_layer, init_method, activation)\n",
    "            layers_MLP.append(linear_layer)\n",
    "            if batch_norm:\n",
    "                layers_MLP.append(nn.BatchNorm1d(num_neurons))\n",
    "            layers_MLP.append(HiddenActivation())\n",
    "            layers_MLP.append(nn.Dropout(p=drop_prob))\n",
    "            last_size = num_neurons\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = nn.Linear(last_size, output_size)\n",
    "        self.init_weights(output_layer, init_method, final_activation)\n",
    "        layers_MLP.append(output_layer)\n",
    "\n",
    "        self.main_mlp = nn.Sequential(*layers_MLP)\n",
    "\n",
    "    \n",
    "    def forward(self, x_rnn, x_mlp=None):\n",
    "        # RNN\n",
    "\n",
    "        rnn_out = self.rnn(x_rnn) # Only one output due to the wrapper\n",
    "        rnn_out = rnn_out[:, -1, :]  # Take the last output of the RNN\n",
    "        \n",
    "        # Input MLP\n",
    "        if x_mlp is not None:\n",
    "            if self.input_mlp is not  None:\n",
    "                x_mlp = self.input_mlp(x_mlp)\n",
    "            x = torch.cat((rnn_out, x_mlp), dim=1)\n",
    "        else:\n",
    "            x = rnn_out\n",
    "\n",
    "        # Main MLP\n",
    "        x = self.main_mlp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseApproximatorRNN(BaseApproximator, nn.Module):\n",
    "\n",
    "    \"\"\" Some basic functions for approximators \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def build_RNN(  self,\n",
    "                    input_size: int | List[int], # is List, it means that multiple inputs are used. The first element is alwas for the RNN, the rest for the MLP\n",
    "                    output_size: int,\n",
    "                    hidden_layers_RNN:int,\n",
    "                    num_hidden_units_RNN: int,\n",
    "                    hidden_layers_MLP:List,\n",
    "                    hidden_layers_input_MLP: List | None = None, # If a separate MLP is used for (potential) MLP input\n",
    "                    RNN_cell: str = \"GRU\",\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0,\n",
    "                    batch_norm: bool = False,\n",
    "                    final_activation: str = \"identity\",\n",
    "                    init_method: str = \"xavier_uniform\" # Parameter for initialization\n",
    "                  ):\n",
    "\n",
    "        \"\"\" Builds a recurrent neural network (RNN) \"\"\"\n",
    "\n",
    "        if isinstance(input_size, int):\n",
    "            RNN_input_size = input_size\n",
    "            MLP_input_size = None\n",
    "        elif isinstance(input_size, list):\n",
    "\n",
    "            if len(input_size) != 2:\n",
    "                raise ValueError(f\"Input size must be a list of length 2 (got {len(input_size)}) with elementes (RNN_input_size, MLP_input_size)\")\n",
    "\n",
    "            RNN_input_size = input_size[0]\n",
    "            MLP_input_size = input_size[1]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Input size must be an integer or a list of integers\")\n",
    "    \n",
    "        self.model = RNNMLPHybrid(  RNN_input_size,\n",
    "                                    MLP_input_size,\n",
    "                                    output_size,\n",
    "                                    num_hidden_units_RNN,\n",
    "                                    hidden_layers_RNN,\n",
    "                                    hidden_layers_MLP,\n",
    "                                    hidden_layers_input_MLP,\n",
    "                                    RNN_cell,\n",
    "                                    activation,\n",
    "                                    final_activation,\n",
    "                                    drop_prob,\n",
    "                                    batch_norm,\n",
    "                                    init_method,\n",
    "                                    )\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLPStateAction(BaseApproximatorMLP):\n",
    "\n",
    "    \"\"\"Multilayer perceptron model for critic networks that take\n",
    "    both states and actions as inputs to output the q-value\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                    input_shape: Tuple | List[Tuple], # number of features\n",
    "                    output_shape: Tuple, # number of outputs/actions\n",
    "                    hidden_layers: list, # list of number of neurons in each hidden layer\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0, # dropout probability\n",
    "                    batch_norm: bool = False, # whether to apply batch normalization\n",
    "                    final_activation: str = \"identity\", # whether to apply ReLU activation to the output\n",
    "                    init_method: str = \"xavier_uniform\",  # Parameter for initialization\n",
    "                    use_cuda: bool = False, # handled by mushroomRL, not used here\n",
    "                    dropout: bool = False # legacy parameter to ensure compatibility, use drop_prob instead\n",
    "                    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # if input shape is list, then concatenate the elements\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = (sum([shape[0] for shape in input_shape]),)\n",
    "        \n",
    "        self.build_MLP(    input_shape[0],\n",
    "                            output_shape[0],\n",
    "                            hidden_layers,\n",
    "                            activation, \n",
    "                            drop_prob,\n",
    "                            batch_norm,\n",
    "                            final_activation,\n",
    "                            init_method)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "\n",
    "\n",
    "        state_action = torch.cat([state.float(), action.float()], dim=1)\n",
    "\n",
    "        q = self.model(state_action)\n",
    "\n",
    "        # TODO: check if squeeze is necessary\n",
    "        # return q\n",
    "        return torch.squeeze(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLPState(BaseApproximatorMLP):\n",
    "\n",
    "    \"\"\"Multilayer perceptron model for critic networks that take\n",
    "    both states and actions as inputs to output the q-value\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                    input_shape: Tuple, # number of features\n",
    "                    output_shape: Tuple, # number of outputs/actions\n",
    "                    hidden_layers: list, # list of number of neurons in each hidden layer\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0, # dropout probability\n",
    "                    batch_norm: bool = False, # whether to apply batch normalization\n",
    "                    final_activation: str = \"identity\", # whether to apply ReLU activation to the output\n",
    "                    init_method: str = \"xavier_uniform\",  # Parameter for initialization\n",
    "                    use_cuda: bool = False, # handled by mushroomRL, not used here\n",
    "                    dropout: bool = False # legacy parameter to ensure compatibility, use drop_prob instead\n",
    "                    ):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.build_MLP(    input_shape[0],\n",
    "                            output_shape[0],\n",
    "                            hidden_layers,\n",
    "                            activation, \n",
    "                            drop_prob,\n",
    "                            batch_norm,\n",
    "                            final_activation,\n",
    "                            init_method)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        state = state.float()\n",
    "\n",
    "        q = self.model(state)\n",
    "\n",
    "        # TODO: check if squeeze is necessary\n",
    "        # return q.squeeze()\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLPActor(BaseApproximatorMLP):\n",
    "\n",
    "    \"\"\"Multilayer perceptron model for critic networks that take\n",
    "    both states and actions as inputs to output the q-value\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                    input_shape: Tuple, # number of features\n",
    "                    output_shape: Tuple, # number of outputs/actions\n",
    "                    hidden_layers: list, # list of number of neurons in each hidden layer\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0, # dropout probability\n",
    "                    batch_norm: bool = False, # whether to apply batch normalization\n",
    "                    final_activation: str = \"identity\", # whether to apply ReLU activation to the output\n",
    "                    init_method: str = \"xavier_uniform\",  # Parameter for initialization\n",
    "                    use_cuda: bool = False,\n",
    "                    dropout: bool = False, # legacy parameter to ensure compatibility, use drop_prob instead\n",
    "                    **kwargs\n",
    "                    ): \n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.build_MLP(    input_shape[0],\n",
    "                            output_shape[0],\n",
    "                            hidden_layers,\n",
    "                            activation, \n",
    "                            drop_prob,\n",
    "                            batch_norm,\n",
    "                            final_activation,\n",
    "                            init_method)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        state = state.float()\n",
    "\n",
    "        a = self.model(state)\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RNNActor(BaseApproximatorRNN):\n",
    "\n",
    "    \"\"\"Multilayer perceptron model for critic networks that take\n",
    "    both states and actions as inputs to output the q-value\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                    input_shape: Tuple | List[Tuple], # number of features\n",
    "                    output_shape: Tuple, # number of outputs/actions\n",
    "                    hidden_layers_RNN: int, # number of initial hidden RNN layers\n",
    "                    num_hidden_units_RNN: int, # number of neurons in the RNN layers\n",
    "                    hidden_layers_MLP: List, # list of number of neurons in each hidden MLP layer, following the RNN layers\n",
    "                    hidden_layers_input_MLP: List | None = None, # If a separate MLP is used for (potential) MLP input\n",
    "                    RNN_cell: str = \"GRU\", # RNN cell type\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0, # dropout probability\n",
    "                    batch_norm: bool = False, # whether to apply batch normalization\n",
    "                    final_activation: str = \"identity\", # whether to apply ReLU activation to the output\n",
    "                    init_method: str = \"xavier_uniform\",  # Parameter for initialization\n",
    "                    use_cuda: bool = False,\n",
    "                    dropout: bool = False, # legacy parameter to ensure compatibility, use drop_prob instead\n",
    "                    **kwargs\n",
    "                    ): \n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(input_shape, tuple):\n",
    "            if len(input_shape) != 2:\n",
    "                raise ValueError(f\"Input shape must be a tuple with dimensions (time_steps, features), got {input_shape}\")\n",
    "            input_size = input_shape[1]\n",
    "        else:\n",
    "            if len(input_shape) > 2:\n",
    "                raise ValueError(f\"Input shape must be a tuple or a list of tuples with length 1 or 2, got length {len(input_shape)}\")\n",
    "            input_size = [input_shape[0][1], input_shape[1][0]]\n",
    "\n",
    "        self.build_RNN(     input_size,\n",
    "                            output_shape[0],\n",
    "                            hidden_layers_RNN,\n",
    "                            num_hidden_units_RNN,\n",
    "                            hidden_layers_MLP,\n",
    "                            hidden_layers_input_MLP,\n",
    "                            RNN_cell,\n",
    "                            activation, \n",
    "                            drop_prob,\n",
    "                            batch_norm,\n",
    "                            final_activation,\n",
    "                            init_method)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        state = state.float()\n",
    "\n",
    "        a = self.model(state)\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RNNStateAction(BaseApproximatorRNN):\n",
    "\n",
    "    \"\"\"Multilayer perceptron model for critic networks that take\n",
    "    both states and actions as inputs to output the q-value\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                    input_shape: List[Tuple], # input shape of features for rnn (optional for mlp) and action\n",
    "                    output_shape: Tuple, # Output shape\n",
    "                    hidden_layers_RNN: int, # number of initial hidden RNN layers\n",
    "                    num_hidden_units_RNN: int, # number of neurons in the RNN layers\n",
    "                    hidden_layers_MLP: List, # list of number of neurons in each hidden MLP layer, following the RNN layers\n",
    "                    hidden_layers_input_MLP: List | None = None, # structure of MLP to speratly process non-RNN input\n",
    "                    RNN_cell: str = \"GRU\", # RNN cell type\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0, # dropout probability\n",
    "                    batch_norm: bool = False, # whether to apply batch normalization\n",
    "                    final_activation: str = \"identity\", # whether to apply ReLU activation to the output\n",
    "                    init_method: str = \"xavier_uniform\",  # Parameter for initialization\n",
    "                    use_cuda: bool = False,\n",
    "                    dropout: bool = False, # legacy parameter to ensure compatibility, use drop_prob instead\n",
    "                    **kwargs\n",
    "                    ): \n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # check that input lenght of list is 2 or 3\n",
    "        if len(input_shape) < 2 or len(input_shape) > 3:\n",
    "            raise ValueError(f\"Input shape must be a list of length 2 or 3, got {len(input_shape)}\")\n",
    "        \n",
    "        rnn_input = input_shape[0][1] # RNN input is always the first element, only the feature dimension is used\n",
    "        mlp_input = input_shape[1][0] if len(input_shape) == 2 else input_shape[1][0] + input_shape[2][0] # if there is a separate MLP input, it is the second element\n",
    "        \n",
    "        self.build_RNN(     [rnn_input, mlp_input],\n",
    "                            output_shape[0],\n",
    "                            hidden_layers_RNN,\n",
    "                            num_hidden_units_RNN,\n",
    "                            hidden_layers_MLP,\n",
    "                            hidden_layers_input_MLP,\n",
    "                            RNN_cell,\n",
    "                            activation, \n",
    "                            drop_prob,\n",
    "                            batch_norm,\n",
    "                            final_activation,\n",
    "                            init_method)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "\n",
    "        if isinstance(state, list):\n",
    "            rnn_input = state[0]\n",
    "            non_time_features = state[1]\n",
    "            mlp_input = torch.cat([non_time_features, action], dim=-1)\n",
    "        else:\n",
    "            rnn_input = state\n",
    "            mlp_input = action\n",
    "\n",
    "        a = self.model(rnn_input.float(), mlp_input.float())\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
