{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximators\n",
    "\n",
    "> Models that approximate a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp RL_approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging_level = logging.DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Tuple\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseApproximator(nn.Module):\n",
    "\n",
    "    \"\"\" Some basic functions for approximators \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def build_MLP(  self,\n",
    "                    input_size: int,\n",
    "                    output_size: int,\n",
    "                    hidden_layers: list,\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0,\n",
    "                    batch_norm: bool = False,\n",
    "                    final_activation: str = \"identity\",\n",
    "                    init_method: str = \"xavier_uniform\" # Parameter for initialization\n",
    "                  ):\n",
    "\n",
    "        \"\"\" Builds a multi-layer perceptron (MLP) \"\"\"\n",
    "\n",
    "        HiddenActivation = self.select_activation(activation)\n",
    "        FinalActivation = self.select_activation(final_activation)\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Hidden layers\n",
    "        last_size = input_size\n",
    "        for num_neurons in hidden_layers:\n",
    "            \n",
    "            linear_layer = nn.Linear(last_size, num_neurons)\n",
    "            self.init_weights(linear_layer, init_method, activation)\n",
    "            layers.append(linear_layer)\n",
    "        \n",
    "            layers.append(HiddenActivation())\n",
    "            layers.append(nn.Dropout(p=drop_prob))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(num_neurons))\n",
    "            last_size = num_neurons\n",
    "        \n",
    "        # Output layer\n",
    "        output_layer = nn.Linear(last_size, output_size)\n",
    "        self.init_weights(output_layer, init_method, final_activation)\n",
    "        layers.append(output_layer)\n",
    "        layers.append(FinalActivation())\n",
    "\n",
    "        # Combine layers\n",
    "        model = nn.Sequential(*layers)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def init_weights(self, layer, init_method, activation):\n",
    "        \"\"\" Initialize the weights of a layer \"\"\"\n",
    "        init_method_function = self.select_init_method(init_method)\n",
    "        \n",
    "        # Check if initialization method requires gain\n",
    "        if init_method in [\"xavier_uniform\", \"xavier_normal\"]:\n",
    "            if activation == \"identity\":\n",
    "                activation = \"linear\"\n",
    "            gain = nn.init.calculate_gain(activation)\n",
    "            init_method_function(layer.weight, gain=gain)\n",
    "        else:\n",
    "            init_method_function(layer.weight)\n",
    "\n",
    "        # print layer weights\n",
    "\n",
    "    @staticmethod\n",
    "    def select_init_method(init_method):\n",
    "        \"\"\" Select the initialization method based on input string \"\"\"\n",
    "        init_method = init_method.lower()\n",
    "        if init_method in [\"xavier_uniform\", \"xavier\"]:\n",
    "            return nn.init.xavier_uniform_\n",
    "        elif init_method in [\"xavier_normal\", \"xaviernorm\"]:\n",
    "            return nn.init.xavier_normal_\n",
    "        elif init_method in [\"he_uniform\", \"kaiming_uniform\"]:\n",
    "            return nn.init.kaiming_uniform_\n",
    "        elif init_method in [\"he_normal\", \"kaiming_normal\"]:\n",
    "            return nn.init.kaiming_normal_\n",
    "        elif init_method in [\"normal\", \"gaussian\"]:\n",
    "            return nn.init.normal_\n",
    "        elif init_method == \"uniform\":\n",
    "            return nn.init.uniform_\n",
    "        else:\n",
    "            raise ValueError(\"Initialization method not recognized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def select_activation(activation):\n",
    "        \"\"\" Select the activation function based on input string \"\"\"\n",
    "        activation = activation.lower()  # Convert input to lowercase for consistency\n",
    "        if activation == \"relu\":\n",
    "            return nn.ReLU\n",
    "        elif activation == \"sigmoid\":\n",
    "            return nn.Sigmoid\n",
    "        elif activation == \"tanh\":\n",
    "            return nn.Tanh\n",
    "        elif activation == \"elu\":\n",
    "            return nn.ELU\n",
    "        elif activation == \"leakyrelu\":\n",
    "            return nn.LeakyReLU\n",
    "        elif activation == \"identity\":\n",
    "            return nn.Identity\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function {activation} not recognized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through the network - overwrite this if necessary \"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLPStateAction(BaseApproximator):\n",
    "\n",
    "    \"\"\"Multilayer perceptron model for critic networks that take\n",
    "    both states and actions as inputs to output the q-value\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                    input_shape: Tuple, # number of features\n",
    "                    output_shape: Tuple, # number of outputs/actions\n",
    "                    hidden_layers: list, # list of number of neurons in each hidden layer\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0, # dropout probability\n",
    "                    batch_norm: bool = False, # whether to apply batch normalization\n",
    "                    final_activation: str = \"identity\", # whether to apply ReLU activation to the output\n",
    "                    init_method: str = \"xavier_uniform\",  # Parameter for initialization\n",
    "                    use_cuda: bool = False, # handled by mushroomRL, not used here\n",
    "                    dropout: bool = False # legacy parameter to ensure compatibility, use drop_prob instead\n",
    "                    ):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = self.build_MLP(    input_shape[0],\n",
    "                                        output_shape[0],\n",
    "                                        hidden_layers,\n",
    "                                        activation, \n",
    "                                        drop_prob,\n",
    "                                        batch_norm,\n",
    "                                        final_activation,\n",
    "                                        init_method)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "\n",
    "\n",
    "        state_action = torch.cat([state.float(), action.float()], dim=1)\n",
    "\n",
    "        q = self.model(state_action)\n",
    "\n",
    "        # TODO: check if squeeze is necessary\n",
    "        # return q\n",
    "        return torch.squeeze(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLPState(BaseApproximator):\n",
    "\n",
    "    \"\"\"Multilayer perceptron model for critic networks that take\n",
    "    both states and actions as inputs to output the q-value\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                    input_shape: Tuple, # number of features\n",
    "                    output_shape: Tuple, # number of outputs/actions\n",
    "                    hidden_layers: list, # list of number of neurons in each hidden layer\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0, # dropout probability\n",
    "                    batch_norm: bool = False, # whether to apply batch normalization\n",
    "                    final_activation: str = \"identity\", # whether to apply ReLU activation to the output\n",
    "                    init_method: str = \"xavier_uniform\",  # Parameter for initialization\n",
    "                    use_cuda: bool = False, # handled by mushroomRL, not used here\n",
    "                    dropout: bool = False # legacy parameter to ensure compatibility, use drop_prob instead\n",
    "                    ):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = self.build_MLP(    input_shape[0],\n",
    "                                        output_shape[0],\n",
    "                                        hidden_layers,\n",
    "                                        activation, \n",
    "                                        drop_prob,\n",
    "                                        batch_norm,\n",
    "                                        final_activation,\n",
    "                                        init_method)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        state = state.float()\n",
    "\n",
    "        q = self.model(state)\n",
    "\n",
    "        # TODO: check if squeeze is necessary\n",
    "        # return q.squeeze()\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLPActor(BaseApproximator):\n",
    "\n",
    "    \"\"\"Multilayer perceptron model for critic networks that take\n",
    "    both states and actions as inputs to output the q-value\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                    input_shape: Tuple, # number of features\n",
    "                    output_shape: Tuple, # number of outputs/actions\n",
    "                    hidden_layers: list, # list of number of neurons in each hidden layer\n",
    "                    activation: str = \"relu\",\n",
    "                    drop_prob: float = 0.0, # dropout probability\n",
    "                    batch_norm: bool = False, # whether to apply batch normalization\n",
    "                    final_activation: str = \"identity\", # whether to apply ReLU activation to the output\n",
    "                    init_method: str = \"xavier_uniform\",  # Parameter for initialization\n",
    "                    use_cuda: bool = False,\n",
    "                    dropout: bool = False, # legacy parameter to ensure compatibility, use drop_prob instead\n",
    "                    **kwargs\n",
    "                    ): \n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = self.build_MLP(    input_shape[0],\n",
    "                                        output_shape[0],\n",
    "                                        hidden_layers,\n",
    "                                        activation, \n",
    "                                        drop_prob,\n",
    "                                        batch_norm,\n",
    "                                        final_activation,\n",
    "                                        init_method)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        state = state.float()\n",
    "\n",
    "        a = self.model(state)\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
