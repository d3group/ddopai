{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation for Bakery dataset for Meta-Learning\n",
    "\n",
    "> Some pre-processings steps implemented to prepare the bakery dataset for meta-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets.bakery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BakeryDatasetLoader():\n",
    "\n",
    "    \"\"\" Class to download the Kaggle M5 dataset and apply some preprocessing steps\n",
    "    to prepare it for application in inventory management. \"\"\"\n",
    "\n",
    "    def __init__(self, data_path, overwrite=False, product_as_feature=False, store_as_features=False):\n",
    "        self.create_paths(data_path)\n",
    "        self.store_as_features = store_as_features\n",
    "        self.product_as_feature = product_as_feature\n",
    "    \n",
    "    def load_dataset(self):\n",
    "\n",
    "        \"\"\" Main function to load the dataset. \"\"\"\n",
    "\n",
    "        logging.info(\"Importing data\")\n",
    "        self.import_from_folder()\n",
    "        logging.info(\"Preprocessing data\")\n",
    "        self.preprocess_pipeline()\n",
    "\n",
    "        return self.demand, self.SKU_features, self.time_features, self.time_SKU_features, self.mask\n",
    "\n",
    "    def create_paths(self, data_path):\n",
    "\n",
    "        \"\"\" Create the paths for the data files. \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.features_path = os.path.join(data_path, \"bakery_data.csv\")\n",
    "        self.demand_path = os.path.join(data_path, \"bakery_target.csv\")\n",
    "\n",
    "    def preprocess_pipeline(self):\n",
    "    \n",
    "        \"\"\" Apply simple preprocessing steps to the data. \"\"\"\n",
    "        \n",
    "        logging.info(\"--Creating catogory mapping and features\")  \n",
    "        # self.sale[\"id\"] = self.sale[\"id\"].str.replace(\"_evaluation\", \"\")\n",
    "        unique_mapping = self.features[['store', 'product']].drop_duplicates()\n",
    "        unique_mapping[\"store\"] = unique_mapping[\"store\"].astype(str)\n",
    "        unique_mapping[\"product\"] = unique_mapping[\"product\"].astype(str)\n",
    "        unique_mapping[\"SKU_id\"] = unique_mapping[\"product\"] + \"_\" + unique_mapping[\"store\"]\n",
    "        unique_mapping.set_index(\"SKU_id\", inplace=True)\n",
    "\n",
    "        dummy_columns = []\n",
    "        if self.product_as_feature:\n",
    "            dummy_columns.append(\"product\")\n",
    "        if self.store_as_features:\n",
    "            dummy_columns.append(\"store\")\n",
    "        if dummy_columns:\n",
    "            categories = pd.get_dummies(unique_mapping[dummy_columns], drop_first=True)\n",
    "\n",
    "        # logging.info(\"--Preparing sales time series data\")\n",
    "        # id = self.sale[\"id\"]\n",
    "        # self.sale = self.sale.iloc[:,6:]\n",
    "        # self.sale[\"SKU_id\"] = id\n",
    "        # self.sale.set_index(\"SKU_id\", inplace=True)\n",
    "        # self.sale = self.sale.transpose()\n",
    "        # self.sale.reset_index(inplace=True, drop=True)\n",
    "        # self.sale.rename_axis(None, axis=1, inplace=True)\n",
    "        # self.sale = self.sale[unique_mapping.index]\n",
    "        \n",
    "        logging.info(\"--Preparing calendric information\")\n",
    "\n",
    "        \n",
    "        calendar_data = self.features[[\"date\", \"weekday\", \"month\", \"year\"]]\n",
    "        date_as_time = pd.to_datetime(calendar_data['date']).copy().unique()\n",
    "\n",
    "        # get only rows with unique dates\n",
    "        calendar_data = calendar_data.drop_duplicates()\n",
    "\n",
    "        calendar_data['date'] = date_as_time\n",
    "\n",
    "        # Adding the 'trend' column which counts from 0 and increments by 1\n",
    "        calendar_data['trend'] = range(len(calendar_data))\n",
    "\n",
    "        # Adding the 'quarter' column\n",
    "        calendar_data['quarter'] = calendar_data['date'].dt.quarter\n",
    "\n",
    "        # Defining the seasonal cut-off dates\n",
    "        def get_season(date):\n",
    "            year = date.year\n",
    "            # Defining specific dates for season changes\n",
    "            winter_start = pd.Timestamp(year=year, month=12, day=21)\n",
    "            spring_start = pd.Timestamp(year=year, month=3, day=20)\n",
    "            summer_start = pd.Timestamp(year=year, month=6, day=21)\n",
    "            autumn_start = pd.Timestamp(year=year, month=9, day=22)\n",
    "            \n",
    "            if date >= winter_start or date < spring_start:\n",
    "                return \"Winter\"\n",
    "            elif spring_start <= date < summer_start:\n",
    "                return \"Spring\"\n",
    "            elif summer_start <= date < autumn_start:\n",
    "                return \"Summer\"\n",
    "            else:\n",
    "                return \"Autumn\"\n",
    "\n",
    "        # Adding the 'season' column with updated season logic\n",
    "        calendar_data['season'] = calendar_data['date'].apply(get_season)\n",
    "\n",
    "        dummy_columns = [\"weekday\", \"month\", \"quarter\", \"season\"]\n",
    "        self.calendar = pd.get_dummies(calendar_data, columns=dummy_columns, drop_first=True)\n",
    "\n",
    "        logging.info(\"--Preparing state-specific features\")\n",
    "        # snap_features = self.calendar[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].copy()\n",
    "        # self.calendar.drop([\"snap_CA\", \"snap_TX\", \"snap_WI\"], axis=1, inplace=True)\n",
    "\n",
    "        # new_snap_features = pd.DataFrame(index=range(snap_features.shape[0]), columns=unique_mapping.index)\n",
    "        # for sku in unique_mapping.index:\n",
    "        #     state_code = sku.split('_')[-2]\n",
    "        #     snap_column = f\"snap_{state_code}\"\n",
    "        #     new_snap_features[sku] = snap_features[snap_column].values\n",
    "        # snap_features = new_snap_features\n",
    "\n",
    "        # logging.info(\"--Preparing price information\")\n",
    "        # self.price[\"SKU_id\"] = self.price[\"item_id\"] + \"_\" + self.price[\"store_id\"]\n",
    "        # self.price.drop([\"store_id\", \"item_id\"], axis=1, inplace=True)\n",
    "        # self.price = self.price.pivot_table(index='wm_yr_wk', columns=['SKU_id'], values='sell_price')\n",
    "        # self.price = self.price[unique_mapping.index]\n",
    "\n",
    "        # logging.info(\"--Creating indicator table if products are available for purchase\")\n",
    "        # self.available = self.price.copy()\n",
    "        # self.available = self.available.notnull().astype(int)\n",
    "        # # fill missing values for price (indicated in the available table)\n",
    "        # self.price.fillna(0, inplace=True)\n",
    "\n",
    "        # logging.info(\"--Preparing final outputs and ensure consistency of time and feature dimensions\")\n",
    "        # wm_yr_wk_per_day = self.calendar[[\"wm_yr_wk\"]]\n",
    "        # self.price = self.price.reset_index()\n",
    "        # missing_wm_yr_wk_in_price = wm_yr_wk_per_day[~wm_yr_wk_per_day[\"wm_yr_wk\"].isin(self.price[\"wm_yr_wk\"])]\n",
    "        # if not missing_wm_yr_wk_in_price.empty:\n",
    "        #     raise ValueError(\"The following wm_yr_wk values are in calendar but not in price: \", missing_wm_yr_wk_in_price.tolist())\n",
    "        # self.price = self.price.merge(wm_yr_wk_per_day, on=\"wm_yr_wk\", how=\"right\")\n",
    "        # self.price.drop([\"wm_yr_wk\"], axis=1, inplace=True)\n",
    "        # self.available = self.available.reset_index()\n",
    "        # self.available = self.available.merge(wm_yr_wk_per_day, on=\"wm_yr_wk\", how=\"right\")\n",
    "        # self.available.drop([\"wm_yr_wk\"], axis=1, inplace=True)\n",
    "        # self.calendar.drop([\"wm_yr_wk\"], axis=1, inplace=True)\n",
    "\n",
    "        # price_multi_index = pd.MultiIndex.from_product([['Price'], self.price.columns], names=['Type', 'SKU'])\n",
    "        # self.price.columns = price_multi_index\n",
    "        # snap_multi_index = pd.MultiIndex.from_product([['Snap'], snap_features.columns], names=['DataType', 'SKU'])\n",
    "        # snap_features.columns = snap_multi_index\n",
    "        # time_SKU_features = pd.concat([self.price, snap_features], axis=1)\n",
    "       \n",
    "        # self.demand = self.sale\n",
    "        # self.SKU_features = categories # features that are not time-dependent\n",
    "        # self.time_features = self.calendar # features that are time-dependent\n",
    "        # self.time_SKU_features = time_SKU_features # features taht are time- and SKU-dependent\n",
    "        # self.mask = self.available # A mask that can either mask datapoints during training or be used as a feature\n",
    "\n",
    "\n",
    "               \n",
    "        self.demand = None\n",
    "        self.SKU_features = None # features that are not time-dependent\n",
    "        self.time_features = None # features that are time-dependent\n",
    "        self.time_SKU_features = None # features taht are time- and SKU-dependent\n",
    "        self.mask = None # A mask that can either mask datapoints during training or be used as a feature\n",
    "\n",
    "    def import_from_folder(self):\n",
    "        \n",
    "        \"\"\" Import data from a folder. \"\"\"\n",
    "\n",
    "        self.features = pd.read_csv(self.features_path)\n",
    "        self.demand = pd.read_csv(self.demand_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Importing data\n",
      "INFO:root:Preprocessing data\n",
      "INFO:root:--Creating catogory mapping and features\n",
      "INFO:root:--Preparing calendric information\n",
      "INFO:root:--Preparing state-specific features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date weekday month  year\n",
      "0    2016-01-02     FRI   JAN  2016\n",
      "1    2016-01-03     SAT   JAN  2016\n",
      "2    2016-01-04     SUN   JAN  2016\n",
      "3    2016-01-05     MON   JAN  2016\n",
      "4    2016-01-06     TUE   JAN  2016\n",
      "...         ...     ...   ...   ...\n",
      "1210 2019-04-26     THU   APR  2019\n",
      "1211 2019-04-27     FRI   APR  2019\n",
      "1212 2019-04-28     SAT   APR  2019\n",
      "1213 2019-04-29     SUN   APR  2019\n",
      "1214 2019-04-30     MON   APR  2019\n",
      "\n",
      "[1215 rows x 4 columns]\n",
      "           date weekday month  year  trend\n",
      "0    2016-01-02     FRI   JAN  2016      0\n",
      "1    2016-01-03     SAT   JAN  2016      1\n",
      "2    2016-01-04     SUN   JAN  2016      2\n",
      "3    2016-01-05     MON   JAN  2016      3\n",
      "4    2016-01-06     TUE   JAN  2016      4\n",
      "...         ...     ...   ...   ...    ...\n",
      "1210 2019-04-26     THU   APR  2019   1210\n",
      "1211 2019-04-27     FRI   APR  2019   1211\n",
      "1212 2019-04-28     SAT   APR  2019   1212\n",
      "1213 2019-04-29     SUN   APR  2019   1213\n",
      "1214 2019-04-30     MON   APR  2019   1214\n",
      "\n",
      "[1215 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "run_test = True\n",
    "if run_test:\n",
    "    data_path = \"/Users/magnus/Documents/02_PhD/03_Newsvendor_foundation_model/experiments/datasets/raw/bakery\" # For testing purposes, please specify the path to the data on your machine\n",
    "    if data_path is not None:\n",
    "        loader = BakeryDatasetLoader(data_path, overwrite=False, product_as_feature=False)\n",
    "        demand, SKU_features, time_features, time_SKU_features, mask = loader.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
