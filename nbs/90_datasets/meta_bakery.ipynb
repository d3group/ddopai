{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation for Bakery dataset for Meta-Learning\n",
    "\n",
    "> Some pre-processings steps implemented to prepare the bakery dataset for meta-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets.bakery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BakeryDatasetLoader():\n",
    "\n",
    "    \"\"\" Class to download the Kaggle M5 dataset and apply some preprocessing steps\n",
    "    to prepare it for application in inventory management. \"\"\"\n",
    "\n",
    "    def __init__(self, data_path, overwrite=False, product_as_feature=False, store_as_features=False):\n",
    "        self.create_paths(data_path)\n",
    "        self.store_as_features = store_as_features\n",
    "        self.product_as_feature = product_as_feature\n",
    "    \n",
    "    def load_dataset(self):\n",
    "\n",
    "        \"\"\" Main function to load the dataset. \"\"\"\n",
    "\n",
    "        logging.info(\"Importing data\")\n",
    "        self.import_from_folder()\n",
    "        logging.info(\"Preprocessing data\")\n",
    "        self.preprocess_pipeline()\n",
    "\n",
    "        return self.demand, self.SKU_features, self.time_features, self.time_SKU_features, self.mask\n",
    "\n",
    "    def create_paths(self, data_path):\n",
    "\n",
    "        \"\"\" Create the paths for the data files. \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.features_path = os.path.join(data_path, \"bakery_data.csv\")\n",
    "        self.demand_path = os.path.join(data_path, \"bakery_target.csv\")\n",
    "\n",
    "    def preprocess_pipeline(self):\n",
    "    \n",
    "        \"\"\" Apply simple preprocessing steps to the data. \"\"\"\n",
    "        \n",
    "        logging.info(\"--Creating catogory mapping and features\")  \n",
    "        # self.sale[\"id\"] = self.sale[\"id\"].str.replace(\"_evaluation\", \"\")\n",
    "        unique_mapping = self.features[['store', 'product']].drop_duplicates()\n",
    "        unique_mapping[\"store\"] = unique_mapping[\"store\"].astype(str)\n",
    "        unique_mapping[\"product\"] = unique_mapping[\"product\"].astype(str)\n",
    "        unique_mapping[\"SKU_id\"] = unique_mapping[\"product\"] + \"_\" + unique_mapping[\"store\"]\n",
    "        unique_mapping.set_index(\"SKU_id\", inplace=True)\n",
    "\n",
    "        dummy_columns = []\n",
    "        if self.product_as_feature:\n",
    "            dummy_columns.append(\"product\")\n",
    "        if self.store_as_features:\n",
    "            dummy_columns.append(\"store\")\n",
    "        if dummy_columns:\n",
    "            categories = pd.get_dummies(unique_mapping[dummy_columns], drop_first=True)\n",
    "        else:\n",
    "            categories = None\n",
    "        \n",
    "        logging.info(\"--Preparing calendric information\")\n",
    "\n",
    "        \n",
    "        calendar_data = self.features[[\"date\", \"weekday\", \"month\", \"year\"]]\n",
    "        date_as_time = pd.to_datetime(calendar_data['date']).copy().unique()\n",
    "\n",
    "        # get only rows with unique dates\n",
    "        calendar_data = calendar_data.drop_duplicates()\n",
    "\n",
    "        calendar_data['date'] = date_as_time\n",
    "\n",
    "        # Adding the 'trend' column which counts from 0 and increments by 1\n",
    "        calendar_data['trend'] = range(len(calendar_data))\n",
    "\n",
    "        # Adding the 'quarter' column\n",
    "        calendar_data['quarter'] = calendar_data['date'].dt.quarter\n",
    "\n",
    "        # Defining the seasonal cut-off dates\n",
    "        def get_season(date):\n",
    "            year = date.year\n",
    "            # Defining specific dates for season changes\n",
    "            winter_start = pd.Timestamp(year=year, month=12, day=21)\n",
    "            spring_start = pd.Timestamp(year=year, month=3, day=20)\n",
    "            summer_start = pd.Timestamp(year=year, month=6, day=21)\n",
    "            autumn_start = pd.Timestamp(year=year, month=9, day=22)\n",
    "            \n",
    "            if date >= winter_start or date < spring_start:\n",
    "                return \"Winter\"\n",
    "            elif spring_start <= date < summer_start:\n",
    "                return \"Spring\"\n",
    "            elif summer_start <= date < autumn_start:\n",
    "                return \"Summer\"\n",
    "            else:\n",
    "                return \"Autumn\"\n",
    "\n",
    "        # Adding the 'season' column with updated season logic\n",
    "        calendar_data['season'] = calendar_data['date'].apply(get_season)\n",
    "\n",
    "        dummy_columns = [\"weekday\", \"month\", \"quarter\", \"season\"]\n",
    "        self.calendar = pd.get_dummies(calendar_data, columns=dummy_columns, drop_first=True)\n",
    "\n",
    "        # drop date column\n",
    "        self.calendar.drop([\"date\"], axis=1, inplace=True)\n",
    "\n",
    "        # print(self.calendar)\n",
    "        # for column in self.calendar.columns:\n",
    "        #     print(column)\n",
    "\n",
    "        logging.info(\"--Preparing demand\")\n",
    "\n",
    "        self.demand[\"store\"] = self.features[\"store\"].astype(str)   \n",
    "        self.demand[\"product\"] = self.features[\"product\"].astype(str)   \n",
    "        self.demand[\"date\"] = self.features[\"date\"]\n",
    "\n",
    "        self.demand[\"SKU_index\"] = self.demand[\"product\"] + \"_\" + self.demand[\"store\"]\n",
    "        self.demand = self.demand.drop([\"store\", \"product\"], axis=1)\n",
    "\n",
    "        self.demand = self.demand.pivot(index=\"date\", columns=\"SKU_index\", values=\"demand\")\n",
    "\n",
    "        # sort the columns\n",
    "        self.demand = self.demand.reindex(sorted(self.demand.columns), axis=1)\n",
    "\n",
    "        logging.info(\"--Preparing SKU-specific features\")\n",
    "\n",
    "        logging.info(\"--Preparing SKU-time-specific features\")\n",
    "\n",
    "        self.time_SKU_features = self.features.drop([\"weekday\", \"month\", \"year\", \"is_holiday_next2days\"], axis=1).copy()\n",
    "\n",
    "        self.time_SKU_features[\"SKU_index\"] = self.time_SKU_features[\"product\"].astype(str) + \"_\" + self.time_SKU_features[\"store\"].astype(str)\n",
    "        self.time_SKU_features = self.time_SKU_features.drop([\"store\", \"product\"], axis=1)\n",
    "\n",
    "        self.time_SKU_features.set_index(['date', 'SKU_index'], inplace=True)\n",
    "\n",
    "        df_unstacked = self.time_SKU_features.unstack('SKU_index')\n",
    "\n",
    "        df_unstacked = df_unstacked.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "        self.time_SKU_features = df_unstacked\n",
    "\n",
    "        self.time_SKU_features.sort_index(axis=1, level='SKU_index')\n",
    "      \n",
    "        self.SKU_features = categories # features that are not time-dependent\n",
    "        self.time_features = self.calendar # features that are time-dependent\n",
    "        self.time_SKU_features = self.time_SKU_features # features that are time- and SKU-dependent\n",
    "        self.mask = None # A mask that can either mask datapoints during training or be used as a feature\n",
    "         \n",
    "    def import_from_folder(self):\n",
    "        \n",
    "        \"\"\" Import data from a folder. \"\"\"\n",
    "\n",
    "        self.features = pd.read_csv(self.features_path)\n",
    "        self.demand = pd.read_csv(self.demand_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Importing data\n",
      "INFO:root:Preprocessing data\n",
      "INFO:root:--Creating catogory mapping and features\n",
      "INFO:root:--Preparing calendric information\n",
      "INFO:root:--Preparing demand\n",
      "INFO:root:--Preparing SKU-specific features\n",
      "INFO:root:--Preparing SKU-time-specific features\n"
     ]
    }
   ],
   "source": [
    "run_test = False\n",
    "if run_test:\n",
    "    data_path = \"/Users/magnus/Documents/02_PhD/03_Newsvendor_foundation_model/experiments/datasets/raw/bakery\" # For testing purposes, please specify the path to the data on your machine\n",
    "    if data_path is not None:\n",
    "        loader = BakeryDatasetLoader(data_path, overwrite=False, product_as_feature=False, store_as_features=False)\n",
    "        demand, SKU_features, time_features, time_SKU_features, mask = loader.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
