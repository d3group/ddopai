{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loader\n",
    "\n",
    "> Class to load datasets available in GitHub releases of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets.default_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info\n",
    "\n",
    "We provide a range of synthetic and real-world datasets to enable reproducible research. Typically we have multiple datasets of the same dataset type (e.g., 16 multivariate datasets following an arma(10,10) process). The datasets are available in the releases of this repository. Below are automated functions that help to easily download those datasets. Three steps to load datasets:\n",
    "\n",
    "* **Step 1**: Create a DatasetLoader object: ```datasetloader = DatasetLoader()```\n",
    "\n",
    "* **Step 2**: Check available dataset types: ```datasetloader.show_dataset_types(show_num_datasets_per_type=True)```\n",
    "\n",
    "* **Step 3**: Load a dataset: ```data = datasetloader.load_dataset(\"arma_10_10\", 1))``` where the first string argument is the name of the dataset type and the second integer argument is the dataset number.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_all_release_tags(token=None):\n",
    "    url = \"https://api.github.com/repos/d3group/ddopai/releases\"\n",
    "    headers = {'Authorization': f'Bearer {token}'} if token else {}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        releases = response.json()\n",
    "        tags = [release['tag_name'] for release in releases]\n",
    "        return tags\n",
    "    else:\n",
    "        raise ValueError(f\"Failed to fetch releases: {response.status_code} with message: {response.text}\")\n",
    "\n",
    "def get_release_tag(dataset_type, version, token=None):\n",
    "    release_tags = get_all_release_tags(token)\n",
    "    release_tags_filtered = [tag for tag in release_tags if dataset_type in tag]\n",
    "\n",
    "    if version == \"latest\":\n",
    "        release_tags_filtered.sort(key=lambda x: [int(num) if num.isdigit() else num for num in re.findall(r'\\d+|\\D+', x.split('_v')[-1])])\n",
    "        release_tag = release_tags_filtered[-1]\n",
    "    else:\n",
    "        release_tag = f\"{dataset_type}_{version}\"\n",
    "    \n",
    "    logging.debug(f\"Filtered release tags: {release_tags_filtered}\")\n",
    "    return release_tag\n",
    "\n",
    "def get_dataset_url(dataset_type, dataset_number, release_tag, token=None):\n",
    "    api_url = f\"https://api.github.com/repos/d3group/ddopai/releases/tags/{release_tag}\"\n",
    "    headers = {'Authorization': f'Bearer {token}'} if token else {}\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        release_info = response.json()\n",
    "        assets = release_info.get(\"assets\", [])\n",
    "\n",
    "        assets = [asset for asset in assets if f\"{dataset_type}_dataset_{dataset_number}_\" in asset['name']]\n",
    "\n",
    "        for asset in assets:\n",
    "            logging.debug(f\"Found dataset: {asset['name']}\")\n",
    "\n",
    "        if len(assets) == 0:\n",
    "            raise ValueError(f\"Dataset {dataset_type}_dataset_{dataset_number} not found in release {release_tag}\")\n",
    "        elif len(assets) > 1:\n",
    "            raise ValueError(f\"Multiple datasets found for {dataset_type}_dataset_{dataset_number} in release {release_tag}\")\n",
    "        else:\n",
    "            asset = assets[0]\n",
    "            return asset['browser_download_url']\n",
    "    else:\n",
    "        raise ValueError(f\"Failed to fetch release information: {response.status_code} with message: {response.text}\")\n",
    "\n",
    "def get_asset_url(dataset_type, dataset_number, version=\"latest\", token=None):\n",
    "    release_tag = get_release_tag(dataset_type, version, token)\n",
    "    asset_url = get_dataset_url(dataset_type, dataset_number, release_tag, token)\n",
    "    return asset_url\n",
    "\n",
    "def download_file_from_github(url, output_path, token=None):\n",
    "    headers = {'Authorization': f'Bearer {token}'} if token else {}\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        logging.debug(f\"File downloaded successfully: {output_path}\")\n",
    "    else:\n",
    "        logging.error(f\"Failed to download file: {response.status_code}\")\n",
    "\n",
    "def unzip_file(zip_file_path, output_dir, delete_zip_file=True):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "    if delete_zip_file:\n",
    "        os.remove(zip_file_path)\n",
    "\n",
    "def load_data_from_directory(dir):\n",
    "    data = dict()\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            key = os.path.splitext(file)[0]\n",
    "            data[key] = pd.read_csv(os.path.join(dir, file))\n",
    "        elif file.endswith(\".pkl\"):\n",
    "            key = os.path.splitext(file)[0]\n",
    "            data[key] = pd.read_pickle(os.path.join(dir, file))\n",
    "        elif file.endswith(\".npy\"):\n",
    "            key = os.path.splitext(file)[0]\n",
    "            data[key] = np.load(os.path.join(dir, file))\n",
    "        else:\n",
    "            raise ValueError(f\"File {file} is not a valid file type (csv, pkl, or npy)\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DatasetLoader():\n",
    "\n",
    "    \"\"\"\n",
    "    Class to load datasets from the GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_types_univariate = [\n",
    "    ]\n",
    "\n",
    "    dataset_types_multivariate = [\n",
    "        \"arma_10_10\",\n",
    "        \"arma_2_2\",\n",
    "        \"ar_1\",\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def show_dataset_types(self,\n",
    "            show_num_datasets_per_type=False # Whether to show the number of datasets per type\n",
    "            ):\n",
    "\n",
    "        \"\"\" Show an overview of all dataset types available in the repository.\"\"\"\n",
    "\n",
    "        if show_num_datasets_per_type:\n",
    "            raise NotImplementedError(\"show_num_datasets_per_type is not implemented yet.\")\n",
    "        else:\n",
    "            print(\"Univariate datasets:\")\n",
    "            for dataset in DatasetLoader.dataset_types_univariate:\n",
    "                print(dataset)\n",
    "            \n",
    "            print(\"\\nMultivariate datasets:\")\n",
    "            for dataset in DatasetLoader.dataset_types_multivariate:\n",
    "                print(dataset)\n",
    "        \n",
    "    def load_dataset(self,\n",
    "        dataset_type: str,\n",
    "        dataset_number: int,\n",
    "        overwrite: bool = False, # Whether to overwrite the dataset if it already exists\n",
    "        version: str = \"latest\", # Which version of the dataset to load, \"latest\" or a specific version,\n",
    "        token: str = None # GitHub token to enable more requests (otherwise limited to 60 requests per hour)\n",
    "    ):\n",
    "\n",
    "        \"\"\" Load a dataset from the GitHub repository.\"\"\"\n",
    "\n",
    "        if dataset_type not in self.dataset_types_univariate and dataset_type not in self.dataset_types_multivariate:\n",
    "            raise ValueError(f\"Dataset type {dataset_type} is not valid. Use the function show_dataset_types() to see valid dataset types.\")\n",
    "\n",
    "        asset_url = get_asset_url(dataset_type, dataset_number, version=version, token=token)\n",
    "\n",
    "\n",
    "        # check if folder \"data\" exists, if not create it\n",
    "        if not os.path.exists(\"data\"):\n",
    "            os.makedirs(\"data\")\n",
    "\n",
    "        output_file_path = f\"data/{dataset_type}_dataset_{dataset_number}\"\n",
    "\n",
    "        download = False\n",
    "        # check if the dataset has already been downloaded\n",
    "        if os.path.exists(output_file_path):\n",
    "            logging.warning(f\"Dataset {dataset_type}_dataset_{dataset_number} has already been downloaded.\")\n",
    "            if overwrite:\n",
    "                logging.warning(\"Overwriting dataset.\")\n",
    "                download = True\n",
    "            else:\n",
    "                logging.warning(\"Keeping existing dataset.\")\n",
    "        else:\n",
    "            download = True\n",
    "\n",
    "        if download:\n",
    "            download_file_from_github(asset_url, output_file_path+\".zip\", token=token)\n",
    "            unzip_file(output_file_path+\".zip\", output_file_path)\n",
    "\n",
    "        data = load_data_from_directory(output_file_path)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopai/blob/main/ddopai/datasets/default_datasets.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## DatasetLoader\n",
       "\n",
       ">      DatasetLoader ()\n",
       "\n",
       "*Class to load datasets from the GitHub repository.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopai/blob/main/ddopai/datasets/default_datasets.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## DatasetLoader\n",
       "\n",
       ">      DatasetLoader ()\n",
       "\n",
       "*Class to load datasets from the GitHub repository.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DatasetLoader, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopai/blob/main/ddopai/datasets/default_datasets.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DatasetLoader.show_dataset_types\n",
       "\n",
       ">      DatasetLoader.show_dataset_types (show_num_datasets_per_type=False)\n",
       "\n",
       "*Show an overview of all dataset types available in the repository.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| show_num_datasets_per_type | bool | False | Whether to show the number of datasets per type |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopai/blob/main/ddopai/datasets/default_datasets.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DatasetLoader.show_dataset_types\n",
       "\n",
       ">      DatasetLoader.show_dataset_types (show_num_datasets_per_type=False)\n",
       "\n",
       "*Show an overview of all dataset types available in the repository.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| show_num_datasets_per_type | bool | False | Whether to show the number of datasets per type |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DatasetLoader.show_dataset_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopai/blob/main/ddopai/datasets/default_datasets.py#L147){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DatasetLoader.load_dataset\n",
       "\n",
       ">      DatasetLoader.load_dataset (dataset_type:str, dataset_number:int,\n",
       ">                                  overwrite:bool=False, version:str='latest',\n",
       ">                                  token:str=None)\n",
       "\n",
       "*Load a dataset from the GitHub repository.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str |  |  |\n",
       "| dataset_number | int |  |  |\n",
       "| overwrite | bool | False | Whether to overwrite the dataset if it already exists |\n",
       "| version | str | latest | Which version of the dataset to load, \"latest\" or a specific version, |\n",
       "| token | str | None | GitHub token to enable more requests (otherwise limited to 60 requests per hour) |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopai/blob/main/ddopai/datasets/default_datasets.py#L147){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DatasetLoader.load_dataset\n",
       "\n",
       ">      DatasetLoader.load_dataset (dataset_type:str, dataset_number:int,\n",
       ">                                  overwrite:bool=False, version:str='latest',\n",
       ">                                  token:str=None)\n",
       "\n",
       "*Load a dataset from the GitHub repository.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset_type | str |  |  |\n",
       "| dataset_number | int |  |  |\n",
       "| overwrite | bool | False | Whether to overwrite the dataset if it already exists |\n",
       "| version | str | latest | Which version of the dataset to load, \"latest\" or a specific version, |\n",
       "| token | str | None | GitHub token to enable more requests (otherwise limited to 60 requests per hour) |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DatasetLoader.load_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univariate datasets:\n",
      "\n",
      "Multivariate datasets:\n",
      "arma_10_10\n",
      "arma_2_2\n",
      "ar_1\n"
     ]
    }
   ],
   "source": [
    "datasetloader = DatasetLoader()\n",
    "datasetloader.show_dataset_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_test = False\n",
    "\n",
    "if download_test:\n",
    "    data = datasetloader.load_dataset(\"arma_10_10\", 1)\n",
    "    X = data[\"data_raw_features\"]\n",
    "    y = data[\"data_raw_target\"]\n",
    "    X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
