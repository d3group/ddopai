{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation for Kaggle M5 dataset for Meta-Learning\n",
    "\n",
    "> Some pre-processings steps implemented to prepare the Kaggle M5 dataset for meta-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets.kaggle_m5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class KaggleM5DatasetLoader():\n",
    "\n",
    "    \"\"\" Class to download the Kaggle M5 dataset and apply some preprocessing steps\n",
    "    to prepare it for application in inventory management. \"\"\"\n",
    "\n",
    "    def __init__(self, data_path, overwrite=False, product_as_feature=False):\n",
    "        self.create_paths(data_path)\n",
    "        self.check_data_path(data_path, overwrite)\n",
    "        self.product_as_feature = product_as_feature\n",
    "    \n",
    "    def load_dataset(self):\n",
    "\n",
    "        \"\"\" Main function to load the dataset. \"\"\"\n",
    "\n",
    "        if self.download_data_flag:\n",
    "            logging.info(\"Downloading dataset from Kaggle\")\n",
    "            self.download_data()\n",
    "        else:\n",
    "            logging.info(\"Using existing data from disk\")\n",
    "        logging.info(\"Importing data\")\n",
    "        self.import_from_folder()\n",
    "        logging.info(\"Preprocessing data\")\n",
    "        self.preprocess_pipeline()\n",
    "\n",
    "        return self.demand, self.SKU_features, self.time_features, self.time_SKU_features, self.mask\n",
    "\n",
    "    def check_data_path(self, data_path, overwrite):\n",
    "\n",
    "        \"\"\" Check if the data path exists and if the data should be downloaded. \"\"\"\n",
    "\n",
    "        if not os.path.exists(data_path):\n",
    "            os.makedirs(data_path)\n",
    "            self.download_data_flag = True\n",
    "        else:\n",
    "            if os.path.exists(self.calendar_path) and os.path.exists(self.sale_path) and os.path.exists(self.price_path):\n",
    "                self.download_data_flag = True if overwrite else False\n",
    "            else:\n",
    "                self.download_data_flag = True\n",
    "\n",
    "    def create_paths(self, data_path):\n",
    "\n",
    "        \"\"\" Create the paths for the data files. \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.calendar_path = os.path.join(data_path, \"calendar.csv\")\n",
    "        self.sale_path = os.path.join(data_path, \"sales_train_evaluation.csv\")\n",
    "        self.price_path = os.path.join(data_path, \"sell_prices.csv\")\n",
    "\n",
    "    def preprocess_pipeline(self):\n",
    "    \n",
    "        \"\"\" Apply simple preprocessing steps to the data. \"\"\"\n",
    "        \n",
    "        logging.info(\"--Creating catogory mapping and features\")  \n",
    "        self.sale[\"id\"] = self.sale[\"id\"].str.replace(\"_evaluation\", \"\")\n",
    "        unique_mapping = self.sale[['item_id', 'dept_id', 'cat_id', 'store_id']].drop_duplicates()\n",
    "        unique_mapping[\"SKU_id\"] = unique_mapping[\"item_id\"] + \"_\" + unique_mapping[\"store_id\"]\n",
    "        unique_mapping[\"state\"] = unique_mapping[\"store_id\"].str.split(\"_\", expand=True)[0]\n",
    "        unique_mapping.set_index(\"SKU_id\", inplace=True)\n",
    "\n",
    "        dummy_columns = [\"dept_id\", \"cat_id\", \"store_id\", \"state\"]\n",
    "        if self.product_as_feature:\n",
    "            dummy_columns.append(\"item_id\")\n",
    "        categories = pd.get_dummies(unique_mapping[dummy_columns], drop_first=True) \n",
    "\n",
    "        logging.info(\"--Preparing sales time series data\")\n",
    "        id = self.sale[\"id\"]\n",
    "        self.sale = self.sale.iloc[:,6:]\n",
    "        self.sale[\"SKU_id\"] = id\n",
    "        self.sale.set_index(\"SKU_id\", inplace=True)\n",
    "        self.sale = self.sale.transpose()\n",
    "        self.sale.reset_index(inplace=True, drop=True)\n",
    "        self.sale.rename_axis(None, axis=1, inplace=True)\n",
    "        self.sale = self.sale[unique_mapping.index]\n",
    "        \n",
    "        logging.info(\"--Preparing calendric information\")\n",
    "        self.calendar = self.calendar[:1941] # we are only interested in the data we also have sales data for\n",
    "        self.calendar.drop([\"date\", \"d\", \"weekday\"], axis=1, inplace=True)\n",
    "        self.calendar[\"trend\"] = np.arange(1, len(self.calendar)+1)\n",
    "        # For even larger datasets this coule be done at runtime when creating samples, but for this one it should be fine memory-wise\n",
    "        dummy_columns = [\"wday\", \"month\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "        self.calendar = pd.get_dummies(self.calendar, columns=dummy_columns, drop_first=True)\n",
    "        cols = self.calendar.columns.tolist()\n",
    "        cols.insert(4, cols.pop(1))\n",
    "        self.calendar = self.calendar[cols]\n",
    "\n",
    "        logging.info(\"--Preparing snap features\")\n",
    "        snap_features = self.calendar[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].copy()\n",
    "        self.calendar.drop([\"snap_CA\", \"snap_TX\", \"snap_WI\"], axis=1, inplace=True)\n",
    "\n",
    "        new_snap_features = pd.DataFrame(index=range(snap_features.shape[0]), columns=unique_mapping.index)\n",
    "        for sku in unique_mapping.index:\n",
    "            state_code = sku.split('_')[-2]\n",
    "            snap_column = f\"snap_{state_code}\"\n",
    "            new_snap_features[sku] = snap_features[snap_column].values\n",
    "        snap_features = new_snap_features\n",
    "\n",
    "        logging.info(\"--Preparing price information\")\n",
    "        self.price[\"SKU_id\"] = self.price[\"item_id\"] + \"_\" + self.price[\"store_id\"]\n",
    "        self.price.drop([\"store_id\", \"item_id\"], axis=1, inplace=True)\n",
    "        self.price = self.price.pivot_table(index='wm_yr_wk', columns=['SKU_id'], values='sell_price')\n",
    "        self.price = self.price[unique_mapping.index]\n",
    "\n",
    "        logging.info(\"--Creating indicator table if products are available for purchase\")\n",
    "        self.available = self.price.copy()\n",
    "        self.available = self.available.notnull().astype(int)\n",
    "        # fill missing values for price (indicated in the available table)\n",
    "        self.price.fillna(0, inplace=True)\n",
    "\n",
    "        logging.info(\"--Preparing final outputs and ensure consistency of time and feature dimensions\")\n",
    "        wm_yr_wk_per_day = self.calendar[[\"wm_yr_wk\"]]\n",
    "        self.price = self.price.reset_index()\n",
    "        missing_wm_yr_wk_in_price = wm_yr_wk_per_day[~wm_yr_wk_per_day[\"wm_yr_wk\"].isin(self.price[\"wm_yr_wk\"])]\n",
    "        if not missing_wm_yr_wk_in_price.empty:\n",
    "            raise ValueError(\"The following wm_yr_wk values are in calendar but not in price: \", missing_wm_yr_wk_in_price.tolist())\n",
    "        self.price = self.price.merge(wm_yr_wk_per_day, on=\"wm_yr_wk\", how=\"right\")\n",
    "        self.price.drop([\"wm_yr_wk\"], axis=1, inplace=True)\n",
    "        self.available = self.available.reset_index()\n",
    "        self.available = self.available.merge(wm_yr_wk_per_day, on=\"wm_yr_wk\", how=\"right\")\n",
    "        self.available.drop([\"wm_yr_wk\"], axis=1, inplace=True)\n",
    "        self.calendar.drop([\"wm_yr_wk\"], axis=1, inplace=True)\n",
    "\n",
    "        price_multi_index = pd.MultiIndex.from_product([['Price'], self.price.columns], names=['Type', 'SKU'])\n",
    "        self.price.columns = price_multi_index\n",
    "        snap_multi_index = pd.MultiIndex.from_product([['Snap'], snap_features.columns], names=['DataType', 'SKU'])\n",
    "        snap_features.columns = snap_multi_index\n",
    "        time_SKU_features = pd.concat([self.price, snap_features], axis=1)\n",
    "       \n",
    "        self.demand = self.sale\n",
    "        self.SKU_features = categories # features that are not time-dependent\n",
    "        self.time_features = self.calendar # features that are time-dependent\n",
    "        self.time_SKU_features = time_SKU_features # features taht are time- and SKU-dependent\n",
    "        self.mask = self.available # A mask that can either mask datapoints during training or be used as a feature\n",
    "\n",
    "        print(time_features)\n",
    "\n",
    "    def import_from_folder(self):\n",
    "        \n",
    "        \"\"\" Import data from a folder. \"\"\"\n",
    "\n",
    "        self.calendar = pd.read_csv(self.calendar_path)\n",
    "        self.sale = pd.read_csv(self.sale_path)\n",
    "        self.price = pd.read_csv(self.price_path)\n",
    "    \n",
    "    def download_data(self):\n",
    "\n",
    "        \"\"\" Download the data directly from Kaggle. \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\n",
    "            \"Download function not implemented yet - please download the data manually\\n\"\n",
    "            \"from Kaggle and place it in the directory specified in the 'data_path' variable during initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test = False\n",
    "if run_test:\n",
    "    data_path = \"/Users/magnus/Documents/02_PhD/03_Newsvendor_foundation_model/experiments/datasets/raw/kaggle_m5\"\n",
    "    if data_path is not None:\n",
    "        loader = KaggleM5DatasetLoader(data_path, overwrite=False, product_as_feature=False)\n",
    "        demand, SKU_features, time_features, time_SKU_features, mask = loader.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
