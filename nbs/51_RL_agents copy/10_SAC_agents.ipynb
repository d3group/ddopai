{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC agents\n",
    "\n",
    "> Soft Actor Critic based agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.rl.sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "\n",
    "# set logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional, List, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.base import BaseAgent\n",
    "from ddopnew.utils import MDPInfo, Parameter, DatasetWrapper\n",
    "from ddopnew.obsprocessors import FlattenTimeDimNumpy\n",
    "from ddopnew.RL_approximators import MLPStateAction, MLPActor\n",
    "from ddopnew.postprocessors import ClipAction\n",
    "\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "\n",
    "from mushroom_rl.algorithms.actor_critic.deep_actor_critic import SAC\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class RLBaseAgent(BaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Base class for Agents that integrate MushroomRL agents.\n",
    "    \"\"\"\n",
    "\n",
    "    train_mode = \"env_interaction\"\n",
    "    \n",
    "    def __init__(self, \n",
    "            environment_info: MDPInfo,\n",
    "            obsprocessors: Optional[List] = None,     # default: []\n",
    "            device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "            agent_name: str | None = None\n",
    "            ):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.network_list, self.actor, self.critic = self.get_network_list(set_actor_critic_attributes=True)\n",
    "\n",
    "        super().__init__(environment_info = environment_info, obsprocessors = obsprocessors, agent_name = agent_name)\n",
    "\n",
    "        self.transfer_obs_processors_to_mushroom_agent()\n",
    "\n",
    "    def transfer_obs_processors_to_mushroom_agent(self):\n",
    "    \n",
    "        \"\"\" Transfer the obs-processors to the MushroomRL agent preprocessors\"\"\"\n",
    "\n",
    "        for obsprocessor in self.obsprocessors:\n",
    "            self.add_obsprocessor(obsprocessor)\n",
    "        self.obsprocessors = []\n",
    "\n",
    "    def add_obsprocessor(self, obsprocessor: object): \n",
    "        \"\"\"Add an obsprocessor to the agent - overwrites the base\n",
    "        class method to add the obsprocessor to the MushroomRL agent\n",
    "        as preprocessor. Postprocessors stay with the base class\"\"\"\n",
    "        self.agent.add_preprocessor(obsprocessor)\n",
    "\n",
    "    @property\n",
    "    def preprocessors(self):\n",
    "\n",
    "        \"\"\" Return the obsprocessors of the agent,\n",
    "        which are the preprocessors of the MushroomRL agent \"\"\"\n",
    "\n",
    "        return self.agent.preprocessors\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_model(self, input_shape: Tuple, output_shape: Tuple):\n",
    "        \"\"\" Set the model for the agent \"\"\"\n",
    "        pass\n",
    "\n",
    "    def set_optimizer(self, optimizer_params: dict): # dict with keys: optimizer, lr, weight_decay\n",
    "        \n",
    "        \"\"\" Set the optimizer for the model \"\"\"\n",
    "        optimizer = optimizer_params[\"optimizer\"]\n",
    "        optimizer_params_copy = optimizer_params.copy()\n",
    "        del optimizer_params_copy[\"optimizer\"]\n",
    "\n",
    "        if optimizer == \"Adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), **optimizer_params_copy)\n",
    "        elif optimizer == \"SGD\":\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), **optimizer_params_copy)\n",
    "        elif optimizer == \"RMSprop\":\n",
    "            self.optimizer = torch.optim.RMSprop(self.model.parameters(), **optimizer_params_copy)\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer {optimizer} not supported\")\n",
    "\n",
    "    def draw_action_(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \n",
    "        \"\"\" \n",
    "        Draw an action based on the fitted model (see predict method)\n",
    "        \"\"\"\n",
    "\n",
    "        # Remove batch dimension if it is one\n",
    "        if observation.shape[0] == 1:\n",
    "            observation = observation[0]\n",
    "\n",
    "        if self.mode==\"train\":\n",
    "            action = self.agent.draw_action(observation)\n",
    "        else:\n",
    "            action = self.predict(observation) # bypass the agent's draw_action method and directly get prediction from policy network\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def predict(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \"\"\" Do one forward pass of the model directly and return the prediction\"\"\"\n",
    "\n",
    "        if self.mode==\"eval\":\n",
    "\n",
    "            # Apply pre-processors of the mushroom agent\n",
    "            for preprocessor in self.agent.preprocessors:\n",
    "                observation = preprocessor(observation)\n",
    "            \n",
    "            # add batch dimension back to mimic mushroom_rl library\n",
    "            observation = np.expand_dims(observation, axis=0)\n",
    "            action = self.predict_(observation)\n",
    "\n",
    "            return action\n",
    "        else:\n",
    "            raise ValueError(\"Model is in train mode. Use draw_action method instead.\")\n",
    "\n",
    "    def predict_(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \"\"\" Do one forward pass of the model directly and return the prediction\n",
    "        Overwrite for agents that have additional steps such as SAC\"\"\"\n",
    "\n",
    "        observation = torch.tensor(observation, dtype=torch.float32).to(self.device)\n",
    "        action = self.actor.forward(observation)\n",
    "        action = action.cpu().detach().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"set the internal state of the agent and its model to train\"\"\"\n",
    "        self.mode = \"train\"\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"set the internal state of the agent and its model to eval\"\"\"\n",
    "        self.mode = \"eval\"\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def to(self, device: str): #\n",
    "        \"\"\"Move the model to the specified device\"\"\"\n",
    "\n",
    "        # check if self.model or something else\n",
    "        self.model.to(device)\n",
    "\n",
    "    def save(self,\n",
    "                path: str, # The directory where the file will be saved.\n",
    "                overwrite: bool=True): # Allow overwriting; if False, a FileExistsError will be raised if the file exists.\n",
    "        \n",
    "        \"\"\"\n",
    "        Save the PyTorch model to a file in the specified directory.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'network_list') or self.network_list is None:\n",
    "            raise AttributeError(\"Cannot find networks.\")\n",
    "\n",
    "        # Create the directory path if it does not exist\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Construct the file path using os.path.join for better cross-platform compatibility\n",
    "\n",
    "        for network_number, network in enumerate(self.network_list):\n",
    "            full_path = os.path.join(path, f\"network_{network_number}.pth\")\n",
    "\n",
    "            if os.path.exists(full_path):\n",
    "                if not overwrite:\n",
    "                    raise FileExistsError(f\"The file {full_path} already exists and will not be overwritten.\")\n",
    "                else:\n",
    "                    logging.debug(f\"Overwriting file {full_path}\") # Only log with info as during training we will continuously overwrite the model\n",
    "            \n",
    "            # Save the model's state_dict using torch.save\n",
    "            torch.save(network.state_dict(), full_path)\n",
    "        logging.debug(f\"Model saved successfully to {full_path}\")\n",
    "\n",
    "\n",
    "    def load(self, path: str): # Only the path to the folder is needed, not the file itself\n",
    " \n",
    "        \"\"\"\n",
    "        Load the PyTorch model from a file.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            raise AttributeError(\"Model is not defined in the class.\")\n",
    "\n",
    "        # Construct the file path\n",
    "        full_path = os.path.join(path, \"model.pth\")\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            raise FileNotFoundError(f\"The file {full_path} does not exist.\")\n",
    "\n",
    "        try:\n",
    "            # Load the model's state_dict using torch.load\n",
    "            self.model.load_state_dict(torch.load(full_path))\n",
    "            logging.info(f\"Model loaded successfully from {full_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred while loading the model: {e}\")\n",
    "\n",
    "    def set_device(self, device: str):\n",
    "\n",
    "        \"\"\" Set the device for the model \"\"\"\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            if torch.cuda.is_available():\n",
    "                use_cuda = True\n",
    "            else:\n",
    "                logging.warning(\"CUDA is not available. Using CPU instead.\")\n",
    "                use_cuda = False\n",
    "        elif device == \"cpu\":\n",
    "            use_cuda = False\n",
    "        else:\n",
    "            raise ValueError(f\"Device {device} not currently not supported, use 'cuda' or 'cpu'\")\n",
    "\n",
    "        return use_cuda\n",
    "\n",
    "    @staticmethod\n",
    "    def get_optimizer_class(optimizer_name: str): #\n",
    "\n",
    "        \"\"\" Get optimizer class based on the optimizer name \"\"\"\n",
    "\n",
    "        if optimizer_name == \"Adam\":\n",
    "            return torch.optim.Adam\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            return torch.optim.SGD\n",
    "        elif optimizer_name == \"RMSprop\":\n",
    "            return torch.optim.RMSprop\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer {optimizer_name} not supported\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss_function(loss: str): #\n",
    "\n",
    "        \"\"\" Get optimizer class based on the optimizer name \"\"\"\n",
    "\n",
    "        if loss == \"MSE\":\n",
    "            return F.mse_loss\n",
    "        else:\n",
    "            raise ValueError(f\"Loss {loss} not supported\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_input_shape(observation_space: object, flatten_time_dim: bool = True): #\n",
    "\n",
    "        \"\"\" Get the input shape of the model based on the environment info \"\"\"\n",
    "\n",
    "        # TODO: Account for more complex spaces like dicts\n",
    "\n",
    "        observation_space_shape = observation_space.shape\n",
    "\n",
    "        if flatten_time_dim:\n",
    "            input_shape = (np.prod(observation_space_shape),)\n",
    "        else:\n",
    "            input_shape = input_shape\n",
    "\n",
    "        return input_shape\n",
    "\n",
    "    def episode_start(self):\n",
    "\n",
    "        \"\"\" What to do if a new episode starts, e.g., reset policy of the agent\n",
    "        Often this does not need to do anything (default), otherwise this funciton \n",
    "        needs to be overwritten in the subclass. \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def fit(self, dataset, **dataset_info):\n",
    "\n",
    "        \"\"\" Hand the fit mehtod to the mushroom agent \"\"\"\n",
    "\n",
    "        self.agent.fit(dataset, **dataset_info)\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\" Stop the agent \"\"\"\n",
    "\n",
    "        self.agent.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/rl/sac.py#L35){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## RLBaseAgent\n",
       "\n",
       ">      RLBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                   obsprocessors:Optional[List]=None, device:str='cpu',\n",
       ">                   agent_name:str|None=None)\n",
       "\n",
       "*Base class for Agents that integrate MushroomRL agents.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| obsprocessors | Optional | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/rl/sac.py#L35){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## RLBaseAgent\n",
       "\n",
       ">      RLBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                   obsprocessors:Optional[List]=None, device:str='cpu',\n",
       ">                   agent_name:str|None=None)\n",
       "\n",
       "*Base class for Agents that integrate MushroomRL agents.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| obsprocessors | Optional | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RLBaseAgent, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XXX\n",
    "\n",
    "XXX\n",
    "\n",
    "**XXXs**:\n",
    "\n",
    "* XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SACAgent(RLBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    XXX\n",
    "    \"\"\"\n",
    "\n",
    "    dropout = True # always keep in True for mushroom_RL, dropout is not desired set drop_prob=0.0\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "\n",
    "                learning_rate_actor: float = 3e-4,\n",
    "                learning_rate_critic: float | None = None, # If none, then it is set to learning_rate_actor\n",
    "                initial_replay_size: int = 64,\n",
    "                max_replay_size: int = 50000,\n",
    "                batch_size: int = 64,\n",
    "                hidden_layers: List = None, # if None, then default is [64, 64]\n",
    "                activation: str = \"relu\", # \"relu\", \"sigmoid\", \"tanh\", \"leakyrelu\", \"elu\"\n",
    "                warmup_transitions: int = 100,\n",
    "                lr_alpha: float = 3e-4,\n",
    "                tau: float = 0.005,\n",
    "                log_std_min: float = -20.0,\n",
    "                log_std_max: float = 2.0,\n",
    "                use_log_alpha_loss=False,\n",
    "                target_entropy: float | None = None,\n",
    "                optimizer: str = \"Adam\", # \"Adam\" or \"SGD\" or \"RMSprop\"  \n",
    "                loss: str = \"MSE\", # currently only MSE is supported     \n",
    "                obsprocessors: list | None = None,      # default: []\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = \"SAC\",\n",
    "                ):\n",
    "\n",
    "        # The standard SAC agent needs a 2D input, so we need to flatten the time dimension\n",
    "        flatten_time_dim_processor = FlattenTimeDimNumpy(allow_2d=True, batch_dim_included=False)\n",
    "        obsprocessors = (obsprocessors or []) + [flatten_time_dim_processor]\n",
    "\n",
    "        use_cuda = self.set_device(device)\n",
    "\n",
    "        hidden_layers = hidden_layers or [64, 64]\n",
    "        self.warmup_training_steps = initial_replay_size\n",
    "\n",
    "\n",
    "        OptimizerClass=self.get_optimizer_class(optimizer)\n",
    "        learning_rate_critic = learning_rate_critic or learning_rate_actor\n",
    "        lossfunction = self.get_loss_function(loss)\n",
    "\n",
    "        actor_input_shape = self.get_input_shape(environment_info.observation_space)\n",
    "        actor_output_shape = environment_info.action_space.shape\n",
    "        critic_input_shape = (actor_input_shape[0] + actor_output_shape[0],) # check how this works when RNN and mixed agents are used\n",
    "\n",
    "        actor_mu_params = dict(network=MLPActor,\n",
    "                                hidden_layers=hidden_layers,\n",
    "                                input_shape=actor_input_shape,\n",
    "                                output_shape=actor_output_shape,\n",
    "                                activation=activation,\n",
    "                                use_cuda=use_cuda,\n",
    "                                dropout=self.dropout\n",
    "                                )\n",
    "\n",
    "        actor_sigma_params = dict(network=MLPActor,\n",
    "                                    hidden_layers=hidden_layers,\n",
    "                                    input_shape= actor_input_shape,\n",
    "                                    output_shape=actor_output_shape,\n",
    "                                    activation=activation,\n",
    "                                    use_cuda=use_cuda,\n",
    "                                    dropout=self.dropout \n",
    "                                    )\n",
    "        \n",
    "        actor_optimizer = {'class': OptimizerClass,\n",
    "            'params': {'lr': learning_rate_actor}} \n",
    "\n",
    "        critic_params = dict(network=MLPStateAction,\n",
    "                optimizer={'class': OptimizerClass,\n",
    "                        'params': {'lr': learning_rate_critic}}, \n",
    "                loss=lossfunction,\n",
    "                # n_features=n_features,\n",
    "                input_shape=critic_input_shape,\n",
    "                hidden_layers=hidden_layers,\n",
    "                output_shape=(1,),\n",
    "                # squeeze_output=squeeze_output,\n",
    "                use_cuda=use_cuda,\n",
    "                dropout=self.dropout,)\n",
    "\n",
    "        self.agent = SAC(\n",
    "            mdp_info=environment_info,\n",
    "            actor_mu_params=actor_mu_params,\n",
    "            actor_sigma_params=actor_sigma_params,\n",
    "            actor_optimizer=actor_optimizer,\n",
    "            critic_params=critic_params,\n",
    "            batch_size=batch_size,\n",
    "            initial_replay_size=initial_replay_size,\n",
    "            max_replay_size=max_replay_size,\n",
    "            warmup_transitions=warmup_transitions,\n",
    "            tau=tau,\n",
    "            lr_alpha=lr_alpha,\n",
    "            use_log_alpha_loss=use_log_alpha_loss,\n",
    "            log_std_min=log_std_min,\n",
    "            log_std_max=log_std_max,\n",
    "            target_entropy=target_entropy,\n",
    "            critic_fit_params=None\n",
    "        )\n",
    "\n",
    "        all_attributes_methods = dir(self.agent)\n",
    "\n",
    "        # Filter out special attributes/methods (those starting and ending with double underscores)\n",
    "        filtered_attributes_methods = [item for item in all_attributes_methods if not item.startswith('__')]\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name\n",
    "        )\n",
    "\n",
    "        logging.info(\"Actor network (mu network):\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "            summary(self.actor, input_size=actor_input_shape)\n",
    "            time.sleep(.2)\n",
    "\n",
    "        logging.info(\"Critic network:\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "            summary(self.critic, input_size=[actor_input_shape, actor_output_shape])\n",
    "\n",
    "    def get_network_list(self, set_actor_critic_attributes: bool = True):\n",
    "        \"\"\" Get the list of networks in the agent for the save and load functions\n",
    "        Get the actor for the predict function in eval mode \"\"\"\n",
    "\n",
    "        networks = []\n",
    "        ensemble_critic = self.agent._critic_approximator._impl.model\n",
    "        for i, model in enumerate(ensemble_critic):\n",
    "            networks.append(model.network)\n",
    "        networks.append(self.agent.policy._mu_approximator._impl.model.network)\n",
    "        networks.append(self.agent.policy._sigma_approximator._impl.model.network)\n",
    "\n",
    "        actor = self.agent.policy._mu_approximator._impl.model.network\n",
    "        critic = ensemble_critic[0].network\n",
    "\n",
    "        if set_actor_critic_attributes:\n",
    "            return networks, actor, critic\n",
    "        else:\n",
    "            return networks\n",
    "    \n",
    "    def load(self, path: str):\n",
    "            pass\n",
    "\n",
    "    def predict_(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \"\"\" Do one forward pass of the model directly and return the prediction.\n",
    "        Apply tanh as implemented for the SAC actor in mushroom_rl\"\"\"\n",
    "\n",
    "        # make observation torch tensor\n",
    "\n",
    "        observation = torch.tensor(observation, dtype=torch.float32).to(self.device)\n",
    "        action = self.actor.forward(observation)\n",
    "        # print(\"a before tanh: \", action)\n",
    "        action = torch.tanh(action)\n",
    "        # print(\"a after tanh: \", action)\n",
    "        action = action * self.agent.policy._delta_a + self.agent.policy._central_a\n",
    "        # print(\"a after scaling: \", action)\n",
    "        action = action.cpu().detach().numpy()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(10000, 2) (10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnus/miniforge3/envs/inventory_gym_2/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "INFO:root:Actor network (mu network):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             192\n",
      "              ReLU-2                   [-1, 64]               0\n",
      "           Dropout-3                   [-1, 64]               0\n",
      "            Linear-4                   [-1, 64]           4,160\n",
      "              ReLU-5                   [-1, 64]               0\n",
      "           Dropout-6                   [-1, 64]               0\n",
      "            Linear-7                    [-1, 1]              65\n",
      "          Identity-8                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 4,417\n",
      "Trainable params: 4,417\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Critic network:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             256\n",
      "              ReLU-2                   [-1, 64]               0\n",
      "           Dropout-3                   [-1, 64]               0\n",
      "            Linear-4                   [-1, 64]           4,160\n",
      "              ReLU-5                   [-1, 64]               0\n",
      "           Dropout-6                   [-1, 64]               0\n",
      "            Linear-7                    [-1, 1]              65\n",
      "          Identity-8                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n",
      "-410.2044837003058 -258.7497311678286\n",
      "-410.2044837003058 -258.7497311678286\n"
     ]
    }
   ],
   "source": [
    "from ddopnew.envs.inventory import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.experiment_functions import run_experiment, test_agent\n",
    "\n",
    "val_index_start = 8000 #90_000\n",
    "test_index_start = 9000 #100_000\n",
    "\n",
    "X = np.random.standard_normal((10000, 2))\n",
    "Y = np.random.standard_normal((10000, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "Y = X[:,0].reshape(-1, 1)\n",
    "# truncate Y at 0:\n",
    "Y = np.maximum(Y, 0)\n",
    "# normalize Y max to 1\n",
    "Y = Y/np.max(Y)\n",
    "\n",
    "print(np.max(Y))\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "clip_action = ClipAction(0., 1.)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start, lag_window_params =  {'lag_window': 0, 'include_y': False, 'pre_calc': True})\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    "    q_bound_high = 1.0,\n",
    "    q_bound_low = -0.1,\n",
    "    postprocessors = [clip_action],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "agent = SACAgent(environment.mdp_info,\n",
    "                obsprocessors = None,      # default: []\n",
    "                device=\"cpu\", # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "environment.train()\n",
    "agent.train()\n",
    "environment.print=False\n",
    "\n",
    "# run_experiment(agent, environment, n_epochs=50, n_steps=1000, run_id = \"test\", save_best=True, print_freq=1) # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
