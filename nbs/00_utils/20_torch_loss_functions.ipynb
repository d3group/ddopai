{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch loss functions\n",
    "\n",
    "> Loss functions that are implemented in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp torch_utils.loss_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "from ddopnew.utils import Parameter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def quantile_loss(\n",
    "    input: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    quantile: torch.Tensor,\n",
    "    reduction: str = 'mean',\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    if not (target.size() == input.size()):\n",
    "        warnings.warn(\n",
    "            f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n",
    "            \"This will likely lead to incorrect results due to broadcasting. \"\n",
    "            \"Please ensure they have the same size.\",\n",
    "            stacklevel=2,\n",
    "        )\n",
    "\n",
    "    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
    "\n",
    "    # print(expanded_input.size(), expanded_target.size(), quantile.size())\n",
    "    # print(quantile)\n",
    "\n",
    "    loss = torch.max((expanded_target - expanded_input) * quantile, (expanded_input - expanded_target) * (1 - quantile))\n",
    "\n",
    "    # print(losks.size())\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        raise ValueError(f\"reduction={reduction} is not valid\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TorchQuantileLoss(_Loss):\n",
    "\n",
    "    \"\"\"\n",
    "    Implmentation of the quantile loss in Pytorch.\n",
    "    Unlike the Numpy-based implementation ```quantile_loss``` in the loss_functions module, this implementation\n",
    "    this implementation reduces the results to a scalar value using the specified reduction method. This class is \n",
    "    used to train Pytorch models using the quantile loss.\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = ['reduction']\n",
    "    def __init__(self,reduction: str = 'mean') -> None: #\n",
    "        super().__init__(reduction=reduction)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor, quantile: Parameter | np.ndarray) -> torch.Tensor: #\n",
    "\n",
    "        \"\"\"\n",
    "        Forward pass of the quantile loss function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        quantile = self.convert_quantile(quantile, input_dtype=input.dtype, device=input.device)\n",
    "            \n",
    "        if not (target.shape == input.shape == quantile.shape):\n",
    "            warnings.warn(\n",
    "                f\"Mismatch in dimensions: target dimension ({target.shape}), input dimension ({input.shape}), \"\n",
    "                f\"and quantile dimension ({quantile.shape}) must be the same. \"\n",
    "                \"This will likely lead to incorrect results due to broadcasting. \"\n",
    "                \"Please ensure they have the same size.\",\n",
    "                stacklevel=2,\n",
    "            )\n",
    "\n",
    "        return quantile_loss(input, target, quantile, reduction=self.reduction)\n",
    "    \n",
    "    def convert_quantile(self, quantile: Parameter | np.ndarray, input_dtype: torch.dtype = torch.float32, device: torch.device = torch.device('cpu')) -> torch.Tensor:\n",
    "        \n",
    "        if isinstance(quantile, Parameter):\n",
    "            quantile =  quantile.get_value()\n",
    "        elif isinstance(quantile, np.ndarray):\n",
    "            quantile = torch.tensor(quantile, dtype=input_dtype, device=device)\n",
    "        elif isinstance(quantile, torch.Tensor):\n",
    "            # ensure dtype and device are the same as the input tensor\n",
    "            quantile = quantile.to(dtype=input_dtype, device=device)\n",
    "        else:\n",
    "            raise ValueError(f\"quantile must be of type Parameter, np.ndarray, or torch.Tensor, but got {type(quantile)}\")\n",
    "\n",
    "        return quantile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/torch_utils/loss_functions.py#L53){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## TorchQuantileLoss\n",
       "\n",
       ">      TorchQuantileLoss (reduction:str='mean')\n",
       "\n",
       "*Implmentation of the quantile loss in Pytorch.\n",
       "Unlike the Numpy-based implementation ```quantile_loss``` in the loss_functions module, this implementation\n",
       "this implementation reduces the results to a scalar value using the specified reduction method. This class is \n",
       "used to train Pytorch models using the quantile loss.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| reduction | str | mean |  |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/torch_utils/loss_functions.py#L53){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## TorchQuantileLoss\n",
       "\n",
       ">      TorchQuantileLoss (reduction:str='mean')\n",
       "\n",
       "*Implmentation of the quantile loss in Pytorch.\n",
       "Unlike the Numpy-based implementation ```quantile_loss``` in the loss_functions module, this implementation\n",
       "this implementation reduces the results to a scalar value using the specified reduction method. This class is \n",
       "used to train Pytorch models using the quantile loss.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| reduction | str | mean |  |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TorchQuantileLoss, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/torch_utils/loss_functions.py#L66){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TorchQuantileLoss.forward\n",
       "\n",
       ">      TorchQuantileLoss.forward (input:torch.Tensor, target:torch.Tensor,\n",
       ">                                 quantile:ddopnew.utils.Parameter|numpy.ndarray\n",
       ">                                 )\n",
       "\n",
       "*Forward pass of the quantile loss function.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| input | Tensor |  |\n",
       "| target | Tensor |  |\n",
       "| quantile | ddopnew.utils.Parameter \\| numpy.ndarray |  |\n",
       "| **Returns** | **Tensor** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/torch_utils/loss_functions.py#L66){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TorchQuantileLoss.forward\n",
       "\n",
       ">      TorchQuantileLoss.forward (input:torch.Tensor, target:torch.Tensor,\n",
       ">                                 quantile:ddopnew.utils.Parameter|numpy.ndarray\n",
       ">                                 )\n",
       "\n",
       "*Forward pass of the quantile loss function.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| input | Tensor |  |\n",
       "| target | Tensor |  |\n",
       "| quantile | ddopnew.utils.Parameter \\| numpy.ndarray |  |\n",
       "| **Returns** | **Tensor** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TorchQuantileLoss.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
