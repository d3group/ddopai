{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obsprocessors\n",
    "\n",
    "> Processors for observations can be used to process the input for an agent before it is being passed to the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp obsprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Union, Optional, List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "from ddopnew.utils import Parameter, check_parameter_types\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BaseProcessor():\n",
    "\n",
    "    def determine_output_shape(self,\n",
    "        sample_input: Dict, # sample input\n",
    "        ) -> Tuple | List:\n",
    "\n",
    "        \"\"\"\n",
    "        Determine the output shape based on the input dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        output = self.__call__(sample_input)\n",
    "\n",
    "        if isinstance(output, list):\n",
    "            return [output_element.shape for output_element in output]\n",
    "        else:\n",
    "            return output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class FlattenTimeDimNumpy():\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocessor to flatten the time and feature dimension of the input.\n",
    "    Used, e.g., to convert time-series data for models that cannot process\n",
    "    a time dimension such as MLPs or Regression models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        allow_2d: Optional[bool] = False, #\n",
    "        batch_dim_included: Optional[bool] = True\n",
    "        ):\n",
    "        self.allow_2d = allow_2d\n",
    "        self.batch_dim_included = batch_dim_included\n",
    "\n",
    "    def check_input(self,\n",
    "            input: np.ndarray #\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Check that the input is a Numpy array with the correct shape.\n",
    "        \"\"\"\n",
    "        # Check if the input is a Numpy array\n",
    "        if not isinstance(input, np.ndarray):\n",
    "            raise TypeError(f\"Expected input to be a numpy array, but got {type(input)} instead.\")\n",
    "\n",
    "        # Determine expected dimensions based on batch_dim_included\n",
    "        expected_ndim = 3 if self.batch_dim_included else 2\n",
    "        allow_ndim = 2 if self.batch_dim_included else 1\n",
    "\n",
    "        # Check if the input array has the correct dimensions\n",
    "        if input.ndim == expected_ndim:\n",
    "            # If the input is 3D, it is valid regardless of allow_2d\n",
    "            return\n",
    "        elif input.ndim == allow_ndim:\n",
    "            # If the input has fewer dimensions, check if the reduced dimension is allowed\n",
    "            if not self.allow_2d:\n",
    "                raise ValueError(\n",
    "                    f\"Expected input to have {expected_ndim} dimensions with shape \"\n",
    "                    f\"{'(batch_size, timesteps, features)' if self.batch_dim_included else '(timesteps, features)'}, \"\n",
    "                    f\"but got shape {input.shape} instead. \"\n",
    "                    f\"{allow_ndim}D inputs are not allowed when allow_2d is False.\"\n",
    "                )\n",
    "        else:\n",
    "            # If the input has an unexpected number of dimensions\n",
    "            expected_shape_msg = (\n",
    "                f\"Expected input to have {expected_ndim} dimensions with shape \"\n",
    "                f\"{'(batch_size, timesteps, features)' if self.batch_dim_included else '(timesteps, features)'}\"\n",
    "                if not self.allow_2d\n",
    "                else f\"Expected input to have {expected_ndim} dimensions with shape \"\n",
    "                     f\"{'(batch_size, timesteps, features)' if self.batch_dim_included else '(timesteps, features)'} \"\n",
    "                     f\"or {allow_ndim} dimensions with shape \"\n",
    "                     f\"{'(batch_size, features)' if self.batch_dim_included else '(features)'}\"\n",
    "            )\n",
    "            raise ValueError(f\"{expected_shape_msg}, but got shape {input.shape} instead.\")\n",
    "            \n",
    "    def __call__(self,\n",
    "                input: np.ndarray\n",
    "                ) -> np.ndarray:\n",
    "                \n",
    "        \"\"\"\n",
    "        Process the input array by keeping the batch dimension and flattening\n",
    "        the time and feature dimensions.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate the input tensor\n",
    "        self.check_input(input)\n",
    "\n",
    "        if self.batch_dim_included:\n",
    "            # If batch dimension is included\n",
    "            if input.ndim == 2:\n",
    "                output = input\n",
    "            else:\n",
    "                # Keep the batch dimension, flatten time and feature dimensions\n",
    "                batch_size, timesteps, features = input.shape\n",
    "                output = input.reshape(batch_size, -1)\n",
    "        else:\n",
    "            # If batch dimension is not included\n",
    "            if input.ndim == 1:\n",
    "                output = input\n",
    "            else:\n",
    "                # Flatten time and feature dimensions\n",
    "                timesteps, features = input.shape\n",
    "                output = input.reshape(-1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/obsprocessors.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## FlattenTimeDimNumpy\n",
       "\n",
       ">      FlattenTimeDimNumpy (allow_2d:Optional[bool]=False,\n",
       ">                           batch_dim_included:Optional[bool]=True)\n",
       "\n",
       "*Preprocessor to flatten the time and feature dimension of the input.\n",
       "Used, e.g., to convert time-series data for models that cannot process\n",
       "a time dimension such as MLPs or Regression models.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| allow_2d | Optional | False |  |\n",
       "| batch_dim_included | Optional | True |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/obsprocessors.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## FlattenTimeDimNumpy\n",
       "\n",
       ">      FlattenTimeDimNumpy (allow_2d:Optional[bool]=False,\n",
       ">                           batch_dim_included:Optional[bool]=True)\n",
       "\n",
       "*Preprocessor to flatten the time and feature dimension of the input.\n",
       "Used, e.g., to convert time-series data for models that cannot process\n",
       "a time dimension such as MLPs or Regression models.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| allow_2d | Optional | False |  |\n",
       "| batch_dim_included | Optional | True |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FlattenTimeDimNumpy, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/obsprocessors.py#L54){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FlattenTimeDimNumpy.check_input\n",
       "\n",
       ">      FlattenTimeDimNumpy.check_input (input:numpy.ndarray)\n",
       "\n",
       "*Check that the input is a Numpy array with the correct shape.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| input | ndarray |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/obsprocessors.py#L54){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FlattenTimeDimNumpy.check_input\n",
       "\n",
       ">      FlattenTimeDimNumpy.check_input (input:numpy.ndarray)\n",
       "\n",
       "*Check that the input is a Numpy array with the correct shape.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| input | ndarray |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FlattenTimeDimNumpy.check_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/obsprocessors.py#L94){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FlattenTimeDimNumpy.__call__\n",
       "\n",
       ">      FlattenTimeDimNumpy.__call__ (input:numpy.ndarray)\n",
       "\n",
       "*Process the input array by keeping the batch dimension and flattening\n",
       "the time and feature dimensions.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/obsprocessors.py#L94){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FlattenTimeDimNumpy.__call__\n",
       "\n",
       ">      FlattenTimeDimNumpy.__call__ (input:numpy.ndarray)\n",
       "\n",
       "*Process the input array by keeping the batch dimension and flattening\n",
       "the time and feature dimensions.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FlattenTimeDimNumpy.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvertDictSpace(BaseProcessor):\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    A utility class to process a dictionary of numpy arrays, with options to preserve or flatten the time dimension.\n",
    "\n",
    "    Note, this class is only used to preprocess output from the environment without batch dimension.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        keep_time_dim: Optional[bool] = False, #If time timension should be flattened as well.\n",
    "        hybrid_space_params: Optional[Dict] = None, # dict with keys \"time\" that is a list of observation keys that should keep the time dimension.\n",
    "        ):\n",
    "        self.keep_time_dim = keep_time_dim\n",
    "        self.hybrid_space_params = hybrid_space_params\n",
    "\n",
    "        if not keep_time_dim and hybrid_space_params is not None:\n",
    "            raise ValueError(\"If keep_time_dim is False, hybrid_space_params must be None.\")\n",
    "        if hybrid_space_params is not None and not isinstance(hybrid_space_params, dict):\n",
    "            raise ValueError(\"hybrid_space_params must be a dictionary if provided.\")\n",
    "\n",
    "\n",
    "    def __call__(self, \n",
    "                input: Dict, # Observation as dict of with numpy arrays\n",
    "                flatten: bool = True, # whether to flatten composite spaces (non-composite spaces will depend on self.keep_time_dim)\n",
    "                ) -> List[np.ndarray] | np.ndarray: \n",
    "\n",
    "        \"\"\"\n",
    "        Process the input dictionary by converting it to a numpy array.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.hybrid_space_params is not None:\n",
    "            obs_2d = [] # time X features\n",
    "            obs_1d = [] # features\n",
    "        else:\n",
    "            obs = [] # features or time X features\n",
    "\n",
    "        for counter, (key, value) in enumerate(input.items()):\n",
    "            if not isinstance(value, np.ndarray):\n",
    "                raise TypeError(f\"Expected input to be a dictionary of numpy arrays, but got {type(value)} instead.\")\n",
    "\n",
    "            if self.hybrid_space_params is not None:\n",
    "                if key in self.hybrid_space_params[\"time_series_input\"]:\n",
    "                    obs_2d.append(value)\n",
    "                    if counter != 0:\n",
    "                        assert obs_2d[counter].shape[0] == obs_2d[counter-1].shape[0], \"All time dimensions must be the same.\"\n",
    "                else:\n",
    "                    obs_1d.append(value.flatten())\n",
    "            else:\n",
    "                if self.keep_time_dim:\n",
    "                    obs.append(value)\n",
    "                    if counter != 0:\n",
    "                        assert obs[counter].shape[0] == obs[counter-1].shape[0], \"All time dimensions must be the same.\"\n",
    "                else:\n",
    "                    obs.append(value.flatten())\n",
    "\n",
    "        if self.hybrid_space_params is not None:\n",
    "            obs_2d = np.concatenate(obs_2d, axis=0)\n",
    "            obs_1d = np.concatenate(obs_1d, axis=0)\n",
    "            if flatten:\n",
    "                return np.concatenate([obs_2d.flatten(), obs_1d], axis=0)\n",
    "            else:\n",
    "                return [obs_2d, obs_1d]\n",
    "        else:\n",
    "            if obs[0].ndim == 1:\n",
    "                return np.concatenate(obs, axis=0)\n",
    "            else:\n",
    "                return np.concatenate(obs, axis=1)\n",
    "\n",
    "            return np.concatenate(obs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AddParamsToFeaturesLEGACY(BaseProcessor):\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    A utility class to process a dictionary of numpy arrays, with options to preserve or flatten the time dimension.\n",
    "    # TODO: Currently is mixes too many cases like batched input, hybrid input etc. Seperate into more specific obsprocessors.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        environment: object, # The environment object, needed to check if val or train mode,\n",
    "        keep_time_dim: Optional[bool] = False, #If time timension should be flattened as well.\n",
    "        hybrid: Optional[bool] = False, # If the param dim should be added as separate vector or concatenated to the features.\n",
    "        receive_batch_dim: Optional[bool] = False, # If the input contains a batch dimension.\n",
    "        ):\n",
    "\n",
    "        self.environment = environment\n",
    "        self.keep_time_dim = keep_time_dim\n",
    "        self.hybrid = hybrid\n",
    "        self.receive_batch_dim = receive_batch_dim\n",
    "\n",
    "        if not keep_time_dim and hybrid:\n",
    "            raise ValueError(\"For flattened vector, hybrid should be be merged with features directy.\")\n",
    "\n",
    "\n",
    "    def __call__(self, \n",
    "                input: Dict, # Observation as dict of with numpy arrays\n",
    "                flatten: bool = False, # whether to flatten composite spaces (non-composite spaces will depend on self.keep_time_dim)\n",
    "                ) -> List[np.ndarray] | np.ndarray: \n",
    "\n",
    "        \"\"\"\n",
    "        Process the input dictionary by converting it to a numpy array.\n",
    "        \"\"\"\n",
    "\n",
    "        input = input.copy()\n",
    "        if self.receive_batch_dim:\n",
    "            features = input[\"features\"]\n",
    "            print(features.shape)\n",
    "            if self.environment.mode == \"train\":\n",
    "                features = np.expand_dims(features, axis=0)\n",
    "                \n",
    "            if not self.keep_time_dim:\n",
    "                batch_size, time_steps, feature_dims = features.shape\n",
    "                new_shape = (batch_size, time_steps*feature_dims)\n",
    "                features = features.reshape(new_shape)\n",
    "        else:\n",
    "            features = input[\"features\"] if self.keep_time_dim else input[\"features\"].flatten()\n",
    "        del input[\"features\"]\n",
    "\n",
    "        if self.hybrid:\n",
    "            if receive_batch_dim:\n",
    "                raise NotImplementedError(\"Hybrid not implemented yet for batched input.\")\n",
    "            else:\n",
    "                obs_1d = [] # features or time X features\n",
    "                obs_2d = [] # time X features\n",
    "                obs.append(input[\"features\"])\n",
    "        \n",
    "        for counter, (key, value) in enumerate(input.items()):\n",
    "            if not isinstance(value, np.ndarray):\n",
    "                raise TypeError(f\"Expected input to be a dictionary of numpy arrays, but got {type(value)} instead.\")\n",
    "            \n",
    "            if value.ndim == 1:\n",
    "                if features.ndim == 1:\n",
    "                    features = np.concatenate([features, value])\n",
    "                else:\n",
    "                    if self.hybrid:\n",
    "                        raise NotImplementedError(\"Hybrid not implemented yet.\")\n",
    "                    # expand value to 2d by copy time dimension\n",
    "                    else:\n",
    "                        if self.receive_batch_dim:\n",
    "\n",
    "                            if features.ndim == 3: # then it is (batch x time x features)\n",
    "                                value = np.expand_dims(value, axis=0) # add batch dimension\n",
    "                                value = np.expand_dims(value, axis=1) # add time dimension\n",
    "\n",
    "                                value = np.repeat(value, features.shape[1], axis=1) # repeat for all time steps\n",
    "                        \n",
    "                            else:\n",
    "                                value = np.expand_dims(value, axis=0) # add batch dimension\n",
    "\n",
    "                            features = np.concatenate([features, value], axis=-1) # concatenate along feature dimension\n",
    "\n",
    "                        else:\n",
    "                            value = np.expand_dims(value, axis=0)\n",
    "                            value = np.repeat(value, features.shape[0], axis=0)\n",
    "\n",
    "                            features = np.concatenate([features, value.flatten()])\n",
    "            \n",
    "            else:\n",
    "                if value.shape == features.shape:\n",
    "                    features = np.concatenate([features, value.flatten()])\n",
    "                else:\n",
    "                    raise ValueError(f\"Expected input to have the same shape as features, but got {value.shape} instead (feature shape: {features.shape}).\")\n",
    "\n",
    "        print(features.shape)\n",
    "        \n",
    "        if self.environment.mode == \"train\":\n",
    "            if len(features.shape) == 3:\n",
    "                features = np.squeeze(features, axis=0) # remove batch dimension\n",
    "                \n",
    "        if self.hybrid:\n",
    "            if flatten:\n",
    "                raise NotImplementedError(\"Hybrid not implemented yet.\")\n",
    "            else:\n",
    "                raise NotImplementedError(\"Hybrid not implemented yet.\")\n",
    "        else:\n",
    "            return features\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AddParamsToFeatures(BaseProcessor):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    A utility class to process a dictionary of numpy arrays (from dict space), with options to preserve or flatten the time dimension.\n",
    "    It always adds the parameters to the appropriate dimension. For composite spaces (partially time-series, partially not), use the\n",
    "    separate AddParamsToFeaturesComposite class.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        environment: object, # The environment object, needed to check if val or train mode,\n",
    "        keep_time_dim: Optional[bool] = False, #If time timension should be flattened as well.\n",
    "        receive_batch_dim: Optional[bool] = False, # If the input contains a batch dimension.\n",
    "        ):\n",
    "\n",
    "        self.environment = environment\n",
    "        self.keep_time_dim = keep_time_dim\n",
    "        self.receive_batch_dim = receive_batch_dim\n",
    "\n",
    "    def __call__(self, \n",
    "                input: Dict, # Observation as dict of with numpy arrays\n",
    "                ) -> List[np.ndarray] | np.ndarray: \n",
    "\n",
    "        \"\"\"\n",
    "        Process the input dictionary by converting it to a numpy array.\n",
    "        \"\"\"\n",
    "\n",
    "        input = input.copy()\n",
    "        if self.receive_batch_dim:\n",
    "            features = input[\"features\"]\n",
    "                \n",
    "            if not self.keep_time_dim:\n",
    "                batch_size, time_steps, feature_dims = features.shape\n",
    "                new_shape = (batch_size, time_steps*feature_dims)\n",
    "                features = features.reshape(new_shape)\n",
    "        else:\n",
    "            features = input[\"features\"] if self.keep_time_dim else input[\"features\"].flatten()\n",
    "        del input[\"features\"]\n",
    "        \n",
    "        for counter, (key, value) in enumerate(input.items()):\n",
    "            if not isinstance(value, np.ndarray):\n",
    "                raise TypeError(f\"Expected input to be a dictionary of numpy arrays, but got {type(value)} instead.\")\n",
    "            \n",
    "            if value.ndim == 1:\n",
    "                if features.ndim == 1:\n",
    "                    features = np.concatenate([features, value])\n",
    "                else:\n",
    "                    if self.receive_batch_dim:\n",
    "\n",
    "                        if features.ndim == 3: # then it is (batch x time x features)\n",
    "                            value = np.expand_dims(value, axis=0) # add batch dimension\n",
    "                            value = np.expand_dims(value, axis=1) # add time dimension\n",
    "\n",
    "                            # TODO: check if it always should expand the features dimension\n",
    "                            value = np.repeat(value, features.shape[0], axis=0) # repeat for all time steps\n",
    "                            value = np.repeat(value, features.shape[1], axis=1) # repeat for all time steps\n",
    "                    \n",
    "                        else:\n",
    "                            value = np.expand_dims(value, axis=0) # add batch dimension\n",
    "\n",
    "                        features = np.concatenate([features, value], axis=-1) # concatenate along feature dimension\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        if features.ndim == 2: # then it is (time x features)\n",
    "                            value = np.expand_dims(value, axis=0)\n",
    "                            value = np.repeat(value, features.shape[0], axis=0)\n",
    "\n",
    "                        features = np.concatenate([features, value.flatten()], axis=-1)\n",
    "            \n",
    "            else:\n",
    "                if value.shape == features.shape:\n",
    "                    features = np.concatenate([features, value.flatten()])\n",
    "                else:\n",
    "                    raise ValueError(f\"Expected input to have the same shape as features, but got {value.shape} instead (feature shape: {features.shape}).\")\n",
    "\n",
    "        return features\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
