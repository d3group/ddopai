{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC agents\n",
    "\n",
    "> Soft Actor Critic based agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.rl.sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "\n",
    "# set logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional, List, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.rl.mushroom_rl import MushroomBaseAgent\n",
    "from ddopnew.utils import MDPInfo, Parameter\n",
    "from ddopnew.obsprocessors import FlattenTimeDimNumpy\n",
    "from ddopnew.RL_approximators import MLPStateAction, MLPActor\n",
    "from ddopnew.postprocessors import ClipAction\n",
    "\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "\n",
    "from mushroom_rl.algorithms.actor_critic.deep_actor_critic import SAC\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SACBaseAgent(MushroomBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    XXX\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "\n",
    "                learning_rate_actor: float = 3e-4,\n",
    "                learning_rate_critic: float | None = None, # If none, then it is set to learning_rate_actor\n",
    "                initial_replay_size: int = 64,\n",
    "                max_replay_size: int = 50000,\n",
    "                batch_size: int = 64,\n",
    "                hidden_layers: List = None, # if None, then default is [64, 64]\n",
    "                activation: str = \"relu\", # \"relu\", \"sigmoid\", \"tanh\", \"leakyrelu\", \"elu\"\n",
    "                warmup_transitions: int = 100,\n",
    "                lr_alpha: float = 3e-4,\n",
    "                tau: float = 0.005,\n",
    "                log_std_min: float = -20.0,\n",
    "                log_std_max: float = 2.0,\n",
    "                use_log_alpha_loss=False,\n",
    "                target_entropy: float | None = None,\n",
    "\n",
    "                drop_prob: float = 0.0,\n",
    "                batch_norm: bool = False,\n",
    "                init_method: str = \"xavier_uniform\", # \"xavier_uniform\", \"xavier_normal\", \"he_normal\", \"he_uniform\", \"normal\", \"uniform\"\n",
    "\n",
    "                optimizer: str = \"Adam\", # \"Adam\" or \"SGD\" or \"RMSprop\"  \n",
    "                loss: str = \"MSE\", # currently only MSE is supported     \n",
    "                obsprocessors: list | None = None,      # default: []\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = \"SAC\",\n",
    "                ):\n",
    "\n",
    "        # The standard SAC agent needs a 2D input, so we need to flatten the time dimension\n",
    "        flatten_time_dim_processor = FlattenTimeDimNumpy(allow_2d=True, batch_dim_included=False)\n",
    "        obsprocessors = (obsprocessors or []) + [flatten_time_dim_processor]\n",
    "\n",
    "        use_cuda = self.set_device(device)\n",
    "\n",
    "        hidden_layers = hidden_layers or [64, 64]\n",
    "        self.warmup_training_steps = initial_replay_size\n",
    "\n",
    "        OptimizerClass=self.get_optimizer_class(optimizer)\n",
    "        learning_rate_critic = learning_rate_critic or learning_rate_actor\n",
    "        lossfunction = self.get_loss_function(loss)\n",
    "\n",
    "        actor_input_shape = self.get_input_shape(environment_info.observation_space)\n",
    "        actor_output_shape = environment_info.action_space.shape\n",
    "        critic_input_shape = (actor_input_shape[0] + actor_output_shape[0],) # check how this works when RNN and mixed agents are used\n",
    "\n",
    "        actor_mu_params = dict(network=MLPActor,\n",
    "                                    input_shape=actor_input_shape,\n",
    "                                    output_shape=actor_output_shape,\n",
    "\n",
    "                                    hidden_layers=hidden_layers,\n",
    "                                    activation=activation,\n",
    "                                    drop_prob=drop_prob,\n",
    "                                    batch_norm=batch_norm,\n",
    "                                    init_method=init_method,\n",
    "\n",
    "                                    use_cuda=use_cuda,\n",
    "                                    dropout=self.dropout\n",
    "                                    )\n",
    "\n",
    "        actor_sigma_params = dict(network=MLPActor,\n",
    "                                    input_shape= actor_input_shape,\n",
    "                                    output_shape=actor_output_shape,\n",
    "\n",
    "                                    hidden_layers=hidden_layers,\n",
    "                                    activation=activation,\n",
    "                                    drop_prob=drop_prob,\n",
    "                                    batch_norm=batch_norm,\n",
    "                                    init_method=init_method,\n",
    "\n",
    "                                    use_cuda=use_cuda,\n",
    "                                    dropout=self.dropout \n",
    "                                    )\n",
    "        \n",
    "        actor_optimizer = {'class': OptimizerClass,\n",
    "            'params': {'lr': learning_rate_actor}} \n",
    "\n",
    "        critic_params = dict(network=MLPStateAction,\n",
    "                optimizer={'class': OptimizerClass,\n",
    "                        'params': {'lr': learning_rate_critic}}, \n",
    "                loss=lossfunction,\n",
    "                input_shape=critic_input_shape,\n",
    "                output_shape=(1,),\n",
    "\n",
    "                hidden_layers=hidden_layers,\n",
    "                activation=activation,\n",
    "                drop_prob=drop_prob,\n",
    "                batch_norm=batch_norm,\n",
    "                init_method=init_method,\n",
    "\n",
    "                use_cuda=use_cuda,\n",
    "                dropout=self.dropout,)\n",
    "\n",
    "        self.agent = SAC(\n",
    "            mdp_info=environment_info,\n",
    "            actor_mu_params=actor_mu_params,\n",
    "            actor_sigma_params=actor_sigma_params,\n",
    "            actor_optimizer=actor_optimizer,\n",
    "            critic_params=critic_params,\n",
    "            batch_size=batch_size,\n",
    "            initial_replay_size=initial_replay_size,\n",
    "            max_replay_size=max_replay_size,\n",
    "            warmup_transitions=warmup_transitions,\n",
    "            tau=tau,\n",
    "            lr_alpha=lr_alpha,\n",
    "            use_log_alpha_loss=use_log_alpha_loss,\n",
    "            log_std_min=log_std_min,\n",
    "            log_std_max=log_std_max,\n",
    "            target_entropy=target_entropy,\n",
    "            critic_fit_params=None\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name\n",
    "        )\n",
    "\n",
    "        logging.info(\"Actor network (mu network):\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "            summary(self.actor, input_size=actor_input_shape)\n",
    "            time.sleep(.2)\n",
    "\n",
    "        logging.info(\"Critic network:\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "            summary(self.critic, input_size=[actor_input_shape, actor_output_shape])\n",
    "\n",
    "    def get_network_list(self, set_actor_critic_attributes: bool = True):\n",
    "        \"\"\" Get the list of networks in the agent for the save and load functions\n",
    "        Get the actor for the predict function in eval mode \"\"\"\n",
    "\n",
    "        networks = []\n",
    "        ensemble_critic = self.agent._critic_approximator._impl.model\n",
    "        for i, model in enumerate(ensemble_critic):\n",
    "            networks.append(model.network)\n",
    "        networks.append(self.agent.policy._mu_approximator._impl.model.network)\n",
    "        networks.append(self.agent.policy._sigma_approximator._impl.model.network)\n",
    "\n",
    "        actor = self.agent.policy._mu_approximator._impl.model.network\n",
    "        critic = ensemble_critic[0].network\n",
    "\n",
    "        if set_actor_critic_attributes:\n",
    "            return networks, actor, critic\n",
    "        else:\n",
    "            return networks\n",
    "\n",
    "    def predict_(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \"\"\" Do one forward pass of the model directly and return the prediction.\n",
    "        Apply tanh as implemented for the SAC actor in mushroom_rl\"\"\"\n",
    "\n",
    "        # make observation torch tensor\n",
    "\n",
    "        observation = torch.tensor(observation, dtype=torch.float32).to(self.device)\n",
    "        action = self.actor.forward(observation)\n",
    "        # print(\"a before tanh: \", action)\n",
    "        action = torch.tanh(action)\n",
    "        # print(\"a after tanh: \", action)\n",
    "        action = action * self.agent.policy._delta_a + self.agent.policy._central_a\n",
    "        # print(\"a after scaling: \", action)\n",
    "        action = action.cpu().detach().numpy()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SACAgent(MushroomBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    XXX\n",
    "    \"\"\"\n",
    "\n",
    "    dropout = True # always keep in True for mushroom_RL, dropout is not desired set drop_prob=0.0\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "\n",
    "                learning_rate_actor: float = 3e-4,\n",
    "                learning_rate_critic: float | None = None, # If none, then it is set to learning_rate_actor\n",
    "                initial_replay_size: int = 64,\n",
    "                max_replay_size: int = 50000,\n",
    "                batch_size: int = 64,\n",
    "                hidden_layers: List = None, # if None, then default is [64, 64]\n",
    "                activation: str = \"relu\", # \"relu\", \"sigmoid\", \"tanh\", \"leakyrelu\", \"elu\"\n",
    "                warmup_transitions: int = 100,\n",
    "                lr_alpha: float = 3e-4,\n",
    "                tau: float = 0.005,\n",
    "                log_std_min: float = -20.0,\n",
    "                log_std_max: float = 2.0,\n",
    "                use_log_alpha_loss=False,\n",
    "                target_entropy: float | None = None,\n",
    "\n",
    "                drop_prob: float = 0.0,\n",
    "                batch_norm: bool = False,\n",
    "                init_method: str = \"xavier_uniform\", # \"xavier_uniform\", \"xavier_normal\", \"he_normal\", \"he_uniform\", \"normal\", \"uniform\"\n",
    "\n",
    "                optimizer: str = \"Adam\", # \"Adam\" or \"SGD\" or \"RMSprop\"  \n",
    "                loss: str = \"MSE\", # currently only MSE is supported     \n",
    "                obsprocessors: list | None = None,      # default: []\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = \"SAC\",\n",
    "                ):\n",
    "\n",
    "        # The standard SAC agent needs a 2D input, so we need to flatten the time dimension\n",
    "        flatten_time_dim_processor = FlattenTimeDimNumpy(allow_2d=True, batch_dim_included=False)\n",
    "        obsprocessors = (obsprocessors or []) + [flatten_time_dim_processor]\n",
    "\n",
    "        use_cuda = self.set_device(device)\n",
    "\n",
    "        hidden_layers = hidden_layers or [64, 64]\n",
    "        self.warmup_training_steps = initial_replay_size\n",
    "\n",
    "        OptimizerClass=self.get_optimizer_class(optimizer)\n",
    "        learning_rate_critic = learning_rate_critic or learning_rate_actor\n",
    "        lossfunction = self.get_loss_function(loss)\n",
    "\n",
    "        actor_input_shape = self.get_input_shape(environment_info.observation_space)\n",
    "        actor_output_shape = environment_info.action_space.shape\n",
    "        critic_input_shape = (actor_input_shape[0] + actor_output_shape[0],) # check how this works when RNN and mixed agents are used\n",
    "\n",
    "        actor_mu_params = dict(network=MLPActor,\n",
    "                                    input_shape=actor_input_shape,\n",
    "                                    output_shape=actor_output_shape,\n",
    "\n",
    "                                    hidden_layers=hidden_layers,\n",
    "                                    activation=activation,\n",
    "                                    drop_prob=drop_prob,\n",
    "                                    batch_norm=batch_norm,\n",
    "                                    init_method=init_method,\n",
    "\n",
    "                                    use_cuda=use_cuda,\n",
    "                                    dropout=self.dropout\n",
    "                                    )\n",
    "\n",
    "        actor_sigma_params = dict(network=MLPActor,\n",
    "                                    input_shape= actor_input_shape,\n",
    "                                    output_shape=actor_output_shape,\n",
    "\n",
    "                                    hidden_layers=hidden_layers,\n",
    "                                    activation=activation,\n",
    "                                    drop_prob=drop_prob,\n",
    "                                    batch_norm=batch_norm,\n",
    "                                    init_method=init_method,\n",
    "\n",
    "                                    use_cuda=use_cuda,\n",
    "                                    dropout=self.dropout \n",
    "                                    )\n",
    "        \n",
    "        actor_optimizer = {'class': OptimizerClass,\n",
    "            'params': {'lr': learning_rate_actor}} \n",
    "\n",
    "        critic_params = dict(network=MLPStateAction,\n",
    "                optimizer={'class': OptimizerClass,\n",
    "                        'params': {'lr': learning_rate_critic}}, \n",
    "                loss=lossfunction,\n",
    "                input_shape=critic_input_shape,\n",
    "                output_shape=(1,),\n",
    "\n",
    "                hidden_layers=hidden_layers,\n",
    "                activation=activation,\n",
    "                drop_prob=drop_prob,\n",
    "                batch_norm=batch_norm,\n",
    "                init_method=init_method,\n",
    "\n",
    "                use_cuda=use_cuda,\n",
    "                dropout=self.dropout,)\n",
    "\n",
    "        self.agent = SAC(\n",
    "            mdp_info=environment_info,\n",
    "            actor_mu_params=actor_mu_params,\n",
    "            actor_sigma_params=actor_sigma_params,\n",
    "            actor_optimizer=actor_optimizer,\n",
    "            critic_params=critic_params,\n",
    "            batch_size=batch_size,\n",
    "            initial_replay_size=initial_replay_size,\n",
    "            max_replay_size=max_replay_size,\n",
    "            warmup_transitions=warmup_transitions,\n",
    "            tau=tau,\n",
    "            lr_alpha=lr_alpha,\n",
    "            use_log_alpha_loss=use_log_alpha_loss,\n",
    "            log_std_min=log_std_min,\n",
    "            log_std_max=log_std_max,\n",
    "            target_entropy=target_entropy,\n",
    "            critic_fit_params=None\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name\n",
    "        )\n",
    "\n",
    "        logging.info(\"Actor network (mu network):\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "            summary(self.actor, input_size=actor_input_shape)\n",
    "            time.sleep(.2)\n",
    "\n",
    "        logging.info(\"Critic network:\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "            summary(self.critic, input_size=[actor_input_shape, actor_output_shape])\n",
    "\n",
    "    def get_network_list(self, set_actor_critic_attributes: bool = True):\n",
    "        \"\"\" Get the list of networks in the agent for the save and load functions\n",
    "        Get the actor for the predict function in eval mode \"\"\"\n",
    "\n",
    "        networks = []\n",
    "        ensemble_critic = self.agent._critic_approximator._impl.model\n",
    "        for i, model in enumerate(ensemble_critic):\n",
    "            networks.append(model.network)\n",
    "        networks.append(self.agent.policy._mu_approximator._impl.model.network)\n",
    "        networks.append(self.agent.policy._sigma_approximator._impl.model.network)\n",
    "\n",
    "        actor = self.agent.policy._mu_approximator._impl.model.network\n",
    "        critic = ensemble_critic[0].network\n",
    "\n",
    "        if set_actor_critic_attributes:\n",
    "            return networks, actor, critic\n",
    "        else:\n",
    "            return networks\n",
    "\n",
    "    def predict_(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \"\"\" Do one forward pass of the model directly and return the prediction.\n",
    "        Apply tanh as implemented for the SAC actor in mushroom_rl\"\"\"\n",
    "\n",
    "        # make observation torch tensor\n",
    "\n",
    "        observation = torch.tensor(observation, dtype=torch.float32).to(self.device)\n",
    "        action = self.actor.forward(observation)\n",
    "        # print(\"a before tanh: \", action)\n",
    "        action = torch.tanh(action)\n",
    "        # print(\"a after tanh: \", action)\n",
    "        action = action * self.agent.policy._delta_a + self.agent.policy._central_a\n",
    "        # print(\"a after scaling: \", action)\n",
    "        action = action.cpu().detach().numpy()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(10000, 2) (10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnus/miniforge3/envs/inventory_gym_2/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "INFO:root:Actor network (mu network):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             192\n",
      "              ReLU-2                   [-1, 64]               0\n",
      "           Dropout-3                   [-1, 64]               0\n",
      "            Linear-4                   [-1, 64]           4,160\n",
      "              ReLU-5                   [-1, 64]               0\n",
      "           Dropout-6                   [-1, 64]               0\n",
      "            Linear-7                    [-1, 1]              65\n",
      "          Identity-8                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 4,417\n",
      "Trainable params: 4,417\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Critic network:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             256\n",
      "              ReLU-2                   [-1, 64]               0\n",
      "           Dropout-3                   [-1, 64]               0\n",
      "            Linear-4                   [-1, 64]           4,160\n",
      "              ReLU-5                   [-1, 64]               0\n",
      "           Dropout-6                   [-1, 64]               0\n",
      "            Linear-7                    [-1, 1]              65\n",
      "          Identity-8                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n",
      "-118.82040518889363 -73.82324158885496\n",
      "-118.82040518889363 -73.82324158885496\n"
     ]
    }
   ],
   "source": [
    "from ddopnew.envs.inventory import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.experiment_functions import run_experiment, test_agent\n",
    "\n",
    "val_index_start = 8000 #90_000\n",
    "test_index_start = 9000 #100_000\n",
    "\n",
    "X = np.random.standard_normal((10000, 2))\n",
    "Y = np.random.standard_normal((10000, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "Y = X[:,0].reshape(-1, 1)\n",
    "# truncate Y at 0:\n",
    "Y = np.maximum(Y, 0)\n",
    "# normalize Y max to 1\n",
    "Y = Y/np.max(Y)\n",
    "\n",
    "print(np.max(Y))\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "clip_action = ClipAction(0., 1.)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start, lag_window_params =  {'lag_window': 0, 'include_y': False, 'pre_calc': True})\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    "    q_bound_high = 1.0,\n",
    "    q_bound_low = -0.1,\n",
    "    postprocessors = [clip_action],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "agent = SACAgent(environment.mdp_info,\n",
    "                obsprocessors = None,      # default: []\n",
    "                device=\"cpu\", # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "environment.train()\n",
    "agent.train()\n",
    "environment.print=False\n",
    "\n",
    "# run_experiment(agent, environment, n_epochs=50, n_steps=1000, run_id = \"test\", save_best=True, print_freq=1) # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
