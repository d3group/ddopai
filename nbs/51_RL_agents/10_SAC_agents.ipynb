{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC agents\n",
    "\n",
    "> Soft Actor Critic based agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.rl.sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "\n",
    "# set logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional, List, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.rl.mushroom_rl import MushroomBaseAgent\n",
    "from ddopnew.utils import MDPInfo, Parameter, merge_dictionaries\n",
    "from ddopnew.obsprocessors import FlattenTimeDimNumpy\n",
    "from ddopnew.RL_approximators import MLPStateAction, MLPActor\n",
    "from ddopnew.postprocessors import ClipAction\n",
    "\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "\n",
    "from mushroom_rl.algorithms.actor_critic.deep_actor_critic import SAC\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SACBaseAgent(MushroomBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    XXX\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "\n",
    "                # Not input regarding network architecture --> to be set in subclass\n",
    "\n",
    "                learning_rate_actor: float = 3e-4,\n",
    "                learning_rate_critic: float | None = None, # If none, then it is set to learning_rate_actor\n",
    "                initial_replay_size: int = 64,\n",
    "                max_replay_size: int = 50000,\n",
    "                batch_size: int = 64,\n",
    "                warmup_transitions: int = 100,\n",
    "                lr_alpha: float = 3e-4,\n",
    "                tau: float = 0.005,\n",
    "                log_std_min: float = -20.0,\n",
    "                log_std_max: float = 2.0,\n",
    "                use_log_alpha_loss=False,\n",
    "                target_entropy: float | None = None,\n",
    "                \n",
    "                drop_prob: float = 0.0,\n",
    "                batch_norm: bool = False,\n",
    "                init_method: str = \"xavier_uniform\", # \"xavier_uniform\", \"xavier_normal\", \"he_normal\", \"he_uniform\", \"normal\", \"uniform\"\n",
    "\n",
    "                optimizer: str = \"Adam\", # \"Adam\" or \"SGD\" or \"RMSprop\"  \n",
    "                loss: str = \"MSE\", # currently only MSE is supported     \n",
    "                obsprocessors: list | None = None,      # default: []\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = \"SAC\",\n",
    "\n",
    "                network_actor_mu_params: dict = None,\n",
    "                network_actor_sigma_params: dict = None,\n",
    "                network_critic_params: dict = None,\n",
    "\n",
    "                ):\n",
    "\n",
    "        #### To set in the subclass:\n",
    "        # potential pre-processor\n",
    "        # input shapes for actor and critic, output shape for actor\n",
    "        # actor and critic network classes\n",
    "        # actor and critic network parameters\n",
    "\n",
    "        use_cuda = self.set_device(device)\n",
    "\n",
    "        self.warmup_training_steps = initial_replay_size\n",
    "\n",
    "        OptimizerClass=self.get_optimizer_class(optimizer)\n",
    "        learning_rate_critic = learning_rate_critic or learning_rate_actor\n",
    "        lossfunction = self.get_loss_function(loss)\n",
    "\n",
    "        actor_mu_params = dict(\n",
    "                                    drop_prob=drop_prob,\n",
    "                                    batch_norm=batch_norm,\n",
    "                                    init_method=init_method,\n",
    "\n",
    "                                    use_cuda=use_cuda,\n",
    "                                    dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        actor_sigma_params = dict(\n",
    "                                    drop_prob=drop_prob,\n",
    "                                    batch_norm=batch_norm,\n",
    "                                    init_method=init_method,\n",
    "\n",
    "                                    use_cuda=use_cuda,\n",
    "                                    dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        actor_mu_params = merge_dictionaries(actor_mu_params, network_actor_mu_params)\n",
    "        actor_sigma_params = merge_dictionaries(actor_sigma_params, network_actor_sigma_params)\n",
    "\n",
    "        actor_optimizer = {'class': OptimizerClass,\n",
    "            'params': {'lr': learning_rate_actor}} \n",
    "\n",
    "        critic_optimizer = {'class': OptimizerClass,\n",
    "            'params': {'lr': learning_rate_critic}}\n",
    "\n",
    "        critic_params = dict(\n",
    "                                optimizer = critic_optimizer,\n",
    "                                loss=lossfunction,\n",
    "                                drop_prob=drop_prob,\n",
    "                                batch_norm=batch_norm,\n",
    "                                init_method=init_method,\n",
    "\n",
    "                                use_cuda=use_cuda,\n",
    "                                dropout=self.dropout,)\n",
    "                            \n",
    "        critic_params = merge_dictionaries(critic_params, network_critic_params)\n",
    "\n",
    "        self.agent = SAC(\n",
    "            mdp_info=environment_info,\n",
    "            actor_mu_params=actor_mu_params,\n",
    "            actor_sigma_params=actor_sigma_params,\n",
    "            actor_optimizer=actor_optimizer,\n",
    "            critic_params=critic_params,\n",
    "            batch_size=batch_size,\n",
    "            initial_replay_size=initial_replay_size,\n",
    "            max_replay_size=max_replay_size,\n",
    "            warmup_transitions=warmup_transitions,\n",
    "            tau=tau,\n",
    "            lr_alpha=lr_alpha,\n",
    "            use_log_alpha_loss=use_log_alpha_loss,\n",
    "            log_std_min=log_std_min,\n",
    "            log_std_max=log_std_max,\n",
    "            target_entropy=target_entropy,\n",
    "            critic_fit_params=None\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name\n",
    "        )\n",
    "\n",
    "        logging.info(\"Actor network (mu network):\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "            summary(self.actor, input_size=actor_mu_params[\"input_shape\"])\n",
    "            time.sleep(.2)\n",
    "\n",
    "        logging.info(\"Critic network:\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "            summary(self.critic, input_size=critic_params[\"input_shape\"])\n",
    "\n",
    "    def get_network_list(self, set_actor_critic_attributes: bool = True):\n",
    "        \"\"\" Get the list of networks in the agent for the save and load functions\n",
    "        Get the actor for the predict function in eval mode \"\"\"\n",
    "\n",
    "        networks = []\n",
    "        ensemble_critic = self.agent._critic_approximator._impl.model\n",
    "        for i, model in enumerate(ensemble_critic):\n",
    "            networks.append(model.network)\n",
    "        networks.append(self.agent.policy._mu_approximator._impl.model.network)\n",
    "        networks.append(self.agent.policy._sigma_approximator._impl.model.network)\n",
    "\n",
    "        actor = self.agent.policy._mu_approximator._impl.model.network\n",
    "        critic = ensemble_critic[0].network\n",
    "\n",
    "        if set_actor_critic_attributes:\n",
    "            return networks, actor, critic\n",
    "        else:\n",
    "            return networks\n",
    "\n",
    "    def predict_(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \"\"\" Do one forward pass of the model directly and return the prediction.\n",
    "        Apply tanh as implemented for the SAC actor in mushroom_rl\"\"\"\n",
    "\n",
    "        # make observation torch tensor\n",
    "\n",
    "        observation = torch.tensor(observation, dtype=torch.float32).to(self.device)\n",
    "        action = self.actor.forward(observation)\n",
    "        # print(\"a before tanh: \", action)\n",
    "        action = torch.tanh(action)\n",
    "        # print(\"a after tanh: \", action)\n",
    "        action = action * self.agent.policy._delta_a + self.agent.policy._central_a\n",
    "        # print(\"a after scaling: \", action)\n",
    "        action = action.cpu().detach().numpy()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SACAgent(SACBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    XXX\n",
    "    \"\"\"\n",
    "\n",
    "    dropout = True # always keep in True for mushroom_RL, dropout is not desired set drop_prob=0.0\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "\n",
    "                hidden_layers: List = None, # if None, then default is [64, 64]\n",
    "                activation: str = \"relu\", # \"relu\", \"sigmoid\", \"tanh\", \"leakyrelu\", \"elu\"\n",
    "\n",
    "                learning_rate_actor: float = 3e-4,\n",
    "                learning_rate_critic: float | None = None, # If none, then it is set to learning_rate_actor\n",
    "                initial_replay_size: int = 64,\n",
    "                max_replay_size: int = 50000,\n",
    "                batch_size: int = 64,\n",
    "                warmup_transitions: int = 100,\n",
    "                lr_alpha: float = 3e-4,\n",
    "                tau: float = 0.005,\n",
    "                log_std_min: float = -20.0,\n",
    "                log_std_max: float = 2.0,\n",
    "                use_log_alpha_loss=False,\n",
    "                target_entropy: float | None = None,\n",
    "\n",
    "                drop_prob: float = 0.0,\n",
    "                batch_norm: bool = False,\n",
    "                init_method: str = \"xavier_uniform\", # \"xavier_uniform\", \"xavier_normal\", \"he_normal\", \"he_uniform\", \"normal\", \"uniform\"\n",
    "\n",
    "                optimizer: str = \"Adam\", # \"Adam\" or \"SGD\" or \"RMSprop\"  \n",
    "                loss: str = \"MSE\", # currently only MSE is supported     \n",
    "                obsprocessors: list | None = None,      # default: []\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = \"SAC\",\n",
    "                ):\n",
    "\n",
    "        # The standard SAC agent needs a 2D input, so we need to flatten the time dimension\n",
    "        flatten_time_dim_processor = FlattenTimeDimNumpy(allow_2d=True, batch_dim_included=False)\n",
    "        obsprocessors = (obsprocessors or []) + [flatten_time_dim_processor]\n",
    "\n",
    "        # determine shapes\n",
    "        actor_input_shape = self.get_input_shape(environment_info.observation_space) # Note: This can be a list or tuple\n",
    "        actor_output_shape = environment_info.action_space.shape # Note: This can be a list or tuple\n",
    "        critic_input_shape = [actor_input_shape, actor_output_shape,] # Note: This can be a list or tuple\n",
    "\n",
    "        # Set networks (use classes, not instances)\n",
    "        actor_mu_network = MLPActor\n",
    "        actor_sigma_network = MLPActor\n",
    "        critic_network = MLPStateAction\n",
    "\n",
    "        # Set default for network architecture\n",
    "        hidden_layers = hidden_layers or [64, 64]\n",
    "\n",
    "        # Set network parameters\n",
    "        network_actor_mu_params = dict(\n",
    "                                    network = MLPActor,\n",
    "                                    input_shape=actor_input_shape,\n",
    "                                    output_shape=actor_output_shape,\n",
    "                                    hidden_layers=hidden_layers,\n",
    "                                    activation=activation,\n",
    "        )\n",
    "\n",
    "        network_actor_sigma_params = dict(\n",
    "                                    network = MLPActor,\n",
    "                                    input_shape=actor_input_shape,\n",
    "                                    output_shape=actor_output_shape,\n",
    "                                    hidden_layers=hidden_layers,\n",
    "                                    activation=activation,\n",
    "        )\n",
    "\n",
    "        network_critic_params = dict(\n",
    "                                    network = MLPStateAction,\n",
    "                                    input_shape=critic_input_shape,\n",
    "                                    output_shape=(1,),\n",
    "                                    hidden_layers=hidden_layers,\n",
    "                                    activation=activation,\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "\n",
    "            learning_rate_actor=learning_rate_actor,\n",
    "            learning_rate_critic=learning_rate_critic,\n",
    "            initial_replay_size=initial_replay_size,\n",
    "            max_replay_size=max_replay_size,\n",
    "            batch_size=batch_size,\n",
    "            warmup_transitions=warmup_transitions,\n",
    "            lr_alpha=lr_alpha,\n",
    "            tau=tau,\n",
    "            log_std_min=log_std_min,\n",
    "            log_std_max=log_std_max,\n",
    "            use_log_alpha_loss=use_log_alpha_loss,\n",
    "            target_entropy=target_entropy,\n",
    "\n",
    "            drop_prob=drop_prob,\n",
    "            batch_norm=batch_norm,\n",
    "            init_method=init_method,\n",
    "\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "\n",
    "            network_actor_mu_params=network_actor_mu_params,\n",
    "            network_actor_sigma_params=network_actor_sigma_params,\n",
    "            network_critic_params=network_critic_params,\n",
    "        )\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(10000, 2) (10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnus/miniforge3/envs/inventory_gym_2/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m\n\u001b[1;32m     23\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m XYDataLoader(X, Y, val_index_start, test_index_start, lag_window_params \u001b[38;5;241m=\u001b[39m  {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlag_window\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_y\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_calc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[1;32m     25\u001b[0m environment \u001b[38;5;241m=\u001b[39m NewsvendorEnv(\n\u001b[1;32m     26\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m dataloader,\n\u001b[1;32m     27\u001b[0m     underage_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.42857\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     postprocessors \u001b[38;5;241m=\u001b[39m [clip_action],\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mSACAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmdp_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                \u001b[49m\u001b[43mobsprocessors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# default: []\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# \"cuda\" or \"cpu\"\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m environment\u001b[38;5;241m.\u001b[39mtest()\n\u001b[1;32m     43\u001b[0m agent\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[5], line 83\u001b[0m, in \u001b[0;36mSACAgent.__init__\u001b[0;34m(self, environment_info, hidden_layers, activation, learning_rate_actor, learning_rate_critic, initial_replay_size, max_replay_size, batch_size, warmup_transitions, lr_alpha, tau, log_std_min, log_std_max, use_log_alpha_loss, target_entropy, drop_prob, batch_norm, init_method, optimizer, loss, obsprocessors, device, agent_name)\u001b[0m\n\u001b[1;32m     67\u001b[0m network_actor_sigma_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     68\u001b[0m                             network \u001b[38;5;241m=\u001b[39m MLPActor,\n\u001b[1;32m     69\u001b[0m                             input_shape\u001b[38;5;241m=\u001b[39mactor_input_shape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m                             activation\u001b[38;5;241m=\u001b[39mactivation,\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     75\u001b[0m network_critic_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     76\u001b[0m                             network \u001b[38;5;241m=\u001b[39m MLPStateAction,\n\u001b[1;32m     77\u001b[0m                             input_shape\u001b[38;5;241m=\u001b[39mcritic_input_shape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m                             activation\u001b[38;5;241m=\u001b[39mactivation,\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate_actor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate_actor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate_critic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate_critic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_replay_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_replay_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_replay_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_replay_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_transitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_transitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_std_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_std_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_std_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_std_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_log_alpha_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_log_alpha_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_entropy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_entropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobsprocessors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobsprocessors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork_actor_mu_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork_actor_mu_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork_actor_sigma_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork_actor_sigma_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork_critic_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork_critic_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 96\u001b[0m, in \u001b[0;36mSACBaseAgent.__init__\u001b[0;34m(self, environment_info, learning_rate_actor, learning_rate_critic, initial_replay_size, max_replay_size, batch_size, warmup_transitions, lr_alpha, tau, log_std_min, log_std_max, use_log_alpha_loss, target_entropy, drop_prob, batch_norm, init_method, optimizer, loss, obsprocessors, device, agent_name, network_actor_mu_params, network_actor_sigma_params, network_critic_params)\u001b[0m\n\u001b[1;32m     84\u001b[0m critic_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     85\u001b[0m                         optimizer \u001b[38;5;241m=\u001b[39m critic_optimizer,\n\u001b[1;32m     86\u001b[0m                         loss\u001b[38;5;241m=\u001b[39mlossfunction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m                         use_cuda\u001b[38;5;241m=\u001b[39muse_cuda,\n\u001b[1;32m     92\u001b[0m                         dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,)\n\u001b[1;32m     94\u001b[0m critic_params \u001b[38;5;241m=\u001b[39m merge_dictionaries(critic_params, network_critic_params)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent \u001b[38;5;241m=\u001b[39m \u001b[43mSAC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmdp_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_mu_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactor_mu_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_sigma_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactor_sigma_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactor_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcritic_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcritic_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_replay_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_replay_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_replay_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_replay_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_transitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_transitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_log_alpha_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_log_alpha_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_std_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_std_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_std_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_std_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_entropy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_entropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcritic_fit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    116\u001b[0m     environment_info\u001b[38;5;241m=\u001b[39menvironment_info,\n\u001b[1;32m    117\u001b[0m     obsprocessors\u001b[38;5;241m=\u001b[39mobsprocessors,\n\u001b[1;32m    118\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    119\u001b[0m     agent_name\u001b[38;5;241m=\u001b[39magent_name\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    122\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActor network (mu network):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/inventory_gym_2/lib/python3.11/site-packages/mushroom_rl/algorithms/actor_critic/deep_actor_critic/sac.py:242\u001b[0m, in \u001b[0;36mSAC.__init__\u001b[0;34m(self, mdp_info, actor_mu_params, actor_sigma_params, actor_optimizer, critic_params, batch_size, initial_replay_size, max_replay_size, warmup_transitions, tau, lr_alpha, use_log_alpha_loss, log_std_min, log_std_max, target_entropy, critic_fit_params)\u001b[0m\n\u001b[1;32m    239\u001b[0m     critic_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_models\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    241\u001b[0m target_critic_params \u001b[38;5;241m=\u001b[39m deepcopy(critic_params)\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_critic_approximator \u001b[38;5;241m=\u001b[39m \u001b[43mRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTorchApproximator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcritic_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_critic_approximator \u001b[38;5;241m=\u001b[39m Regressor(TorchApproximator, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtarget_critic_params)\n\u001b[1;32m    245\u001b[0m actor_mu_approximator \u001b[38;5;241m=\u001b[39m Regressor(TorchApproximator, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mactor_mu_params)\n",
      "File \u001b[0;32m~/miniforge3/envs/inventory_gym_2/lib/python3.11/site-packages/mushroom_rl/approximators/regressor.py:73\u001b[0m, in \u001b[0;36mRegressor.__init__\u001b[0;34m(self, approximator, input_shape, output_shape, n_actions, n_models, **params)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl \u001b[38;5;241m=\u001b[39m ActionRegressor(approximator, n_actions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl \u001b[38;5;241m=\u001b[39m \u001b[43mGenericRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapproximator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/inventory_gym_2/lib/python3.11/site-packages/mushroom_rl/approximators/_implementations/generic_regressor.py:22\u001b[0m, in \u001b[0;36mGenericRegressor.__init__\u001b[0;34m(self, approximator, n_inputs, **params)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mConstructor.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_inputs \u001b[38;5;241m=\u001b[39m n_inputs\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mapproximator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_save_attr(\n\u001b[1;32m     25\u001b[0m     _n_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimitive\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_serialization_method(approximator)\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/inventory_gym_2/lib/python3.11/site-packages/mushroom_rl/approximators/_implementations/ensemble.py:30\u001b[0m, in \u001b[0;36mEnsemble.__init__\u001b[0;34m(self, model, n_models, prediction, **params)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prediction \u001b[38;5;241m=\u001b[39m prediction\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_models):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_save_attr(\n\u001b[1;32m     33\u001b[0m     _model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_serialization_method(model),\n\u001b[1;32m     34\u001b[0m     _prediction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimitive\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/inventory_gym_2/lib/python3.11/site-packages/mushroom_rl/approximators/parametric/torch_approximator.py:55\u001b[0m, in \u001b[0;36mTorchApproximator.__init__\u001b[0;34m(self, input_shape, output_shape, network, optimizer, loss, batch_size, n_fit_targets, use_cuda, reinitialize, dropout, quiet, **params)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_quiet \u001b[38;5;241m=\u001b[39m quiet\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_fit_targets \u001b[38;5;241m=\u001b[39m n_fit_targets\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_cuda:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/Documents/02_PhD/Other_python_projects/00_ddop_new/ddopnew/ddopnew/RL_approximators.py:149\u001b[0m, in \u001b[0;36mMLPStateAction.__init__\u001b[0;34m(self, input_shape, output_shape, hidden_layers, activation, drop_prob, batch_norm, final_activation, init_method, use_cuda, dropout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# if input shape is list, then concatenate the elements\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_shape, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 149\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28msum\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m]\u001b[49m),)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_MLP(    input_shape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    152\u001b[0m                                 output_shape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    153\u001b[0m                                 hidden_layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m                                 final_activation,\n\u001b[1;32m    158\u001b[0m                                 init_method)\n",
      "File \u001b[0;32m~/Documents/02_PhD/Other_python_projects/00_ddop_new/ddopnew/ddopnew/RL_approximators.py:149\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# if input shape is list, then concatenate the elements\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_shape, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 149\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28msum\u001b[39m([\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m input_shape]),)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_MLP(    input_shape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    152\u001b[0m                                 output_shape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    153\u001b[0m                                 hidden_layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m                                 final_activation,\n\u001b[1;32m    158\u001b[0m                                 init_method)\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from ddopnew.envs.inventory import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.experiment_functions import run_experiment, test_agent\n",
    "\n",
    "val_index_start = 8000 #90_000\n",
    "test_index_start = 9000 #100_000\n",
    "\n",
    "X = np.random.standard_normal((10000, 2))\n",
    "Y = np.random.standard_normal((10000, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "Y = X[:,0].reshape(-1, 1)\n",
    "# truncate Y at 0:\n",
    "Y = np.maximum(Y, 0)\n",
    "# normalize Y max to 1\n",
    "Y = Y/np.max(Y)\n",
    "\n",
    "print(np.max(Y))\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "clip_action = ClipAction(0., 1.)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start, lag_window_params =  {'lag_window': 0, 'include_y': False, 'pre_calc': True})\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    "    q_bound_high = 1.0,\n",
    "    q_bound_low = -0.1,\n",
    "    postprocessors = [clip_action],\n",
    ")\n",
    "\n",
    "\n",
    "agent = SACAgent(environment.mdp_info,\n",
    "                obsprocessors = None,      # default: []\n",
    "                device=\"cpu\", # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "environment.train()\n",
    "agent.train()\n",
    "environment.print=False\n",
    "\n",
    "# run_experiment(agent, environment, n_epochs=50, n_steps=1000, run_id = \"test\", save_best=True, print_freq=1) # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
