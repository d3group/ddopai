{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3 agents\n",
    "\n",
    "> TD3 based agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.rl.td3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "\n",
    "# set logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional, List, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.rl.mushroom_rl import MushroomBaseAgent\n",
    "from ddopnew.utils import MDPInfo, Parameter\n",
    "from ddopnew.obsprocessors import FlattenTimeDimNumpy\n",
    "from ddopnew.RL_approximators import MLPStateAction, MLPActor\n",
    "from ddopnew.postprocessors import ClipAction\n",
    "\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "\n",
    "from mushroom_rl.algorithms.actor_critic.deep_actor_critic import TD3\n",
    "from mushroom_rl.policy import OrnsteinUhlenbeckPolicy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TD3Agent(MushroomBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    XXX\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Make structure same to SAC with TD3 base class\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "\n",
    "                learning_rate_actor: float = 3e-4,\n",
    "                learning_rate_critic: float | None = None, # If none, then it is set to learning_rate_actor\n",
    "                initial_replay_size: int = 1024,\n",
    "                max_replay_size: int = 50000,\n",
    "                batch_size: int = 64,\n",
    "                hidden_layers: List = None, # if None, then default is [64, 64]\n",
    "                activation: str = \"relu\", # \"relu\", \"sigmoid\", \"tanh\", \"leakyrelu\", \"elu\"\n",
    "                tau: float = 0.005,\n",
    "                policy_delay: int = 2,\n",
    "                noise_std: float = 0.2,\n",
    "                sigma_scale: float = 0.5,\n",
    "                theta: float = 0.15,\n",
    "                dt=0.02,\n",
    "\n",
    "                drop_prob: float = 0.0,\n",
    "                batch_norm: bool = False,\n",
    "                init_method: str = \"xavier_uniform\", # \"xavier_uniform\", \"xavier_normal\", \"he_normal\", \"he_uniform\", \"normal\", \"uniform\"\n",
    "\n",
    "                optimizer: str = \"Adam\", # \"Adam\" or \"SGD\" or \"RMSprop\"  \n",
    "                loss: str = \"MSE\", # currently only MSE is supported     \n",
    "                obsprocessors: list | None = None,      # default: []\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = \"SAC\",\n",
    "                ):\n",
    "\n",
    "        # The standard TD3 agent needs a 2D input, so we need to flatten the time dimension\n",
    "        flatten_time_dim_processor = FlattenTimeDimNumpy(allow_2d=True, batch_dim_included=False)\n",
    "        obsprocessors = (obsprocessors or []) + [flatten_time_dim_processor]\n",
    "\n",
    "        # TODO: Add option to also use Gaussian policy\n",
    "        self.policy_class = OrnsteinUhlenbeckPolicy\n",
    "        self.policy_params = dict(sigma=np.ones(1) * sigma_scale, theta=theta, dt=dt)\n",
    "\n",
    "        use_cuda = self.set_device(device)\n",
    "\n",
    "        hidden_layers = hidden_layers or [64, 64]\n",
    "        self.warmup_training_steps = initial_replay_size\n",
    "\n",
    "\n",
    "        OptimizerClass=self.get_optimizer_class(optimizer)\n",
    "        learning_rate_critic = learning_rate_critic or learning_rate_actor\n",
    "        lossfunction = self.get_loss_function(loss)\n",
    "\n",
    "        input_shape = self.get_input_shape(environment_info.observation_space)\n",
    "        output_shape = environment_info.action_space.shape\n",
    "\n",
    "        input_shape = self.convert_recursively_to_int(input_shape)\n",
    "        output_shape = self.convert_recursively_to_int(output_shape)\n",
    "\n",
    "        actor_input_shape = input_shape\n",
    "        actor_output_shape = output_shape\n",
    "        critic_input_shape = (actor_input_shape[0] + actor_output_shape[0],) # check how this works when RNN and mixed agents are used\n",
    "\n",
    "        actor_params = dict(network=MLPActor,\n",
    "\n",
    "                                input_shape=actor_input_shape,\n",
    "                                output_shape=actor_output_shape,\n",
    "\n",
    "                                hidden_layers=hidden_layers,\n",
    "                                activation=activation,\n",
    "                                drop_prob=drop_prob,\n",
    "                                batch_norm=batch_norm,\n",
    "                                init_method=init_method,\n",
    "\n",
    "                                use_cuda=use_cuda,\n",
    "                                dropout=self.dropout,\n",
    "\n",
    "                                )\n",
    "\n",
    "        actor_optimizer = {'class': OptimizerClass,\n",
    "            'params': {'lr': learning_rate_actor}} \n",
    "\n",
    "        critic_params = dict(network=MLPStateAction,\n",
    "                optimizer={'class': OptimizerClass,\n",
    "                        'params': {'lr': learning_rate_critic}}, \n",
    "                loss=lossfunction,\n",
    "                input_shape=critic_input_shape,\n",
    "                output_shape=(1,),\n",
    "\n",
    "                hidden_layers=hidden_layers,\n",
    "                activation=activation,\n",
    "                drop_prob=drop_prob,\n",
    "                batch_norm=batch_norm,\n",
    "                init_method=init_method,\n",
    "\n",
    "                use_cuda=use_cuda,\n",
    "                dropout=self.dropout,)\n",
    "\n",
    "        self.agent = TD3(\n",
    "            mdp_info=environment_info,\n",
    "            policy_class=self.policy_class,\n",
    "            policy_params=self.policy_params,\n",
    "            actor_params=actor_params,\n",
    "            actor_optimizer=actor_optimizer,\n",
    "            critic_params=critic_params,\n",
    "            batch_size=batch_size,\n",
    "            initial_replay_size=initial_replay_size,\n",
    "            max_replay_size=max_replay_size,\n",
    "            tau=tau,\n",
    "            policy_delay=policy_delay,\n",
    "            noise_std=noise_std,\n",
    "            critic_fit_params=None\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name\n",
    "        )\n",
    "\n",
    "        logging.info(\"Actor network:\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "            input_size = self.add_batch_dimension_for_shape(actor_input_shape)\n",
    "            print(summary(self.actor, input_size=input_size))\n",
    "            time.sleep(.2)\n",
    "\n",
    "        logging.info(\"Critic network:\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "            input_size = self.add_batch_dimension_for_shape([actor_input_shape, actor_output_shape])\n",
    "            print(summary(self.critic, input_size=input_size))\n",
    "\n",
    "    def get_network_list(self, set_actor_critic_attributes: bool = True):\n",
    "        \"\"\" Get the list of networks in the agent for the save and load functions\n",
    "        Get the actor for the predict function in eval mode \"\"\"\n",
    "\n",
    "        networks = []\n",
    "        ensemble_critic = self.agent._critic_approximator._impl.model\n",
    "        for i, model in enumerate(ensemble_critic):\n",
    "            networks.append(model.network)\n",
    "        networks.append(self.agent.policy._approximator._impl.model.network)\n",
    "\n",
    "        actor = self.agent.policy._approximator._impl.model.network\n",
    "        critic = ensemble_critic[0].network\n",
    "\n",
    "        if set_actor_critic_attributes:\n",
    "            return networks, actor, critic\n",
    "        else:\n",
    "            return networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# class TD3Agent():\n",
    "\n",
    "#     train_mode = \"env_interaction\"\n",
    "\n",
    "#     \"\"\"\n",
    "#     Soft Actor Critic (SAC) agent with hybrid action, both based on Gaussian. The binary action is \n",
    "#     0 if the output of the network is less or equal than 0, and 1 otherwise.\n",
    "\n",
    "#     Args:\n",
    "#         mdp_info (MDPInfo): Contains relevant information about the environment.\n",
    "#         learning_rate_actor (float): Learning rate for the actor.\n",
    "#         learning_rate_critic (float): Learning rate for the critic.\n",
    "#         learning_rate_alpha (float): Learning rate for the temperature parameter.\n",
    "#         initial_replay_size (int): Number of transitions to save in the replay buffer during startup.\n",
    "#         max_replay_size (int): Maximum number of transitions to save in the replay buffer.\n",
    "#         batch_size (int): Number of transitions to sample each time experience is replayed.\n",
    "#         n_features (int): Number of features for the hidden layers of the networks.\n",
    "#         lr_alpha (float): Learning rate for the temperature parameter.\n",
    "#         tau (float): Parameter for the soft update of the target networks.\n",
    "#         optimizer (torch.optim): Optimizer to use for the networks.\n",
    "#         squeeze_output (bool): Whether to squeeze the output of the actor network or not.\n",
    "#         use_cuda (bool): Whether to use CUDA or not. If True and not available, it will use CPU.\n",
    "#         agent_name (str): Name of the agent. If set to None will use some default name.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             environment_info: MDPInfo,\n",
    "#             learning_rate_actor = 3e-4,\n",
    "#             learning_rate_critic = None,\n",
    "#             initial_replay_size = 1024,\n",
    "#             max_replay_size = 50000,\n",
    "#             batch_size = 64,\n",
    "#             hidden_layers = [64, 64],\n",
    "#             tau = 0.005,\n",
    "#             policy_delay = 2,\n",
    "#             noise_std = 0.2,\n",
    "#             optimizer = optim.Adam,\n",
    "#             sigma_scale = 0.5,\n",
    "\n",
    "#             loss = \"MSE\",\n",
    "\n",
    "#             theta=0.15,\n",
    "#             dt=0.02,\n",
    "#             squeeze_output = True,\n",
    "#             device = \"cuda\",\n",
    "#             agent_name = None): \n",
    "        \n",
    "#         # print(\"in init fubction\")\n",
    "\n",
    "#         self.warmup_training_steps = initial_replay_size\n",
    "\n",
    "#         mdp_info = environment_info\n",
    "#         optimizer = optim.Adam\n",
    "        \n",
    "#         self.policy_class = OrnsteinUhlenbeckPolicy\n",
    "#         self.policy_params = dict(sigma=np.ones(1) * sigma_scale, theta=theta, dt=dt)\n",
    "\n",
    "#         if len(mdp_info.observation_space.shape) == 2:\n",
    "#             input_shape = (mdp_info.observation_space.shape[0]*mdp_info.observation_space.shape[1],)\n",
    "#         else:\n",
    "#             input_shape = mdp_info.observation_space.shape\n",
    "\n",
    "#         actor_output_shape = (mdp_info.action_space.shape[0],) \n",
    "\n",
    "#         print(input_shape)\n",
    "\n",
    "#         use_cuda = False\n",
    "\n",
    "#         if learning_rate_critic is None:\n",
    "#             learning_rate_critic = learning_rate_actor\n",
    "\n",
    "#         actor_params = dict(network=MLPActor,\n",
    "#                                 hidden_layers=hidden_layers,\n",
    "#                                 input_shape=input_shape,\n",
    "#                                 output_shape=actor_output_shape,\n",
    "#                                 use_cuda=use_cuda)\n",
    "        \n",
    "#         # print(\"setting optimizer class\")\n",
    "#         actor_optimizer = {'class': optimizer,\n",
    "#                     'params': {'lr': learning_rate_actor}} \n",
    "        \n",
    "#         critic_input_shape = (input_shape[0] + actor_output_shape[0],)\n",
    "#         critic_params = dict(network=MLPStateAction,\n",
    "#                         optimizer={'class': optimizer,\n",
    "#                                 'params': {'lr': learning_rate_critic}}, \n",
    "#                         loss=F.mse_loss,\n",
    "#                         hidden_layers=hidden_layers,\n",
    "#                         input_shape=critic_input_shape,\n",
    "#                         output_shape=(1,),\n",
    "#                         squeeze_output=squeeze_output,\n",
    "#                         use_cuda=use_cuda)\n",
    "        \n",
    "#         # print(\"creating agent from mushroom\")\n",
    "        \n",
    "#         self.agent = TD3(mdp_info, self.policy_class, self.policy_params,\n",
    "#                     actor_params, actor_optimizer, critic_params, batch_size,\n",
    "#                     initial_replay_size, max_replay_size, tau, policy_delay, noise_std)\n",
    "                \n",
    "#         self.network_list, self.actor, self.critic = self.get_network_list(set_actor_critic_attributes=True)\n",
    "    \n",
    "#         # print(\"created agent from mushroom\")\n",
    "\n",
    "#         if agent_name is None:\n",
    "#             self.agent.name = 'TD3_classic'\n",
    "#         else:\n",
    "#             self.agent.name = agent_name\n",
    "\n",
    "#     def __getattr__(self, attr):\n",
    "#         return getattr(self.agent, attr)\n",
    "\n",
    "#     def train(self,):\n",
    "#         self.agent.policy.train()\n",
    "    \n",
    "#     def eval(self,):\n",
    "#         self.agent.policy.eval()\n",
    "\n",
    "#     def get_network_list(self, set_actor_critic_attributes: bool = True):\n",
    "#         \"\"\" Get the list of networks in the agent for the save and load functions\n",
    "#         Get the actor for the predict function in eval mode \"\"\"\n",
    "\n",
    "#         networks = []\n",
    "#         ensemble_critic = self.agent._critic_approximator._impl.model\n",
    "#         for i, model in enumerate(ensemble_critic):\n",
    "#             networks.append(model.network)\n",
    "#         networks.append(self.agent.policy._approximator._impl.model.network)\n",
    "\n",
    "#         actor = self.agent.policy._approximator._impl.model.network\n",
    "#         critic = ensemble_critic[0].network\n",
    "\n",
    "#         if set_actor_critic_attributes:\n",
    "#             return networks, actor, critic\n",
    "#         else:\n",
    "#             return networks\n",
    "        \n",
    "#     def save(self,\n",
    "#                 path: str, # The directory where the file will be saved.\n",
    "#                 overwrite: bool=True): # Allow overwriting; if False, a FileExistsError will be raised if the file exists.\n",
    "        \n",
    "#         \"\"\"\n",
    "#         Save the PyTorch model to a file in the specified directory.\n",
    "\n",
    "#         \"\"\"\n",
    "        \n",
    "#         if not hasattr(self, 'network_list') or self.network_list is None:\n",
    "#             raise AttributeError(\"Cannot find networks.\")\n",
    "\n",
    "#         # Create the directory path if it does not exist\n",
    "#         os.makedirs(path, exist_ok=True)\n",
    "\n",
    "#         # Construct the file path using os.path.join for better cross-platform compatibility\n",
    "\n",
    "#         for network_number, network in enumerate(self.network_list):\n",
    "#             full_path = os.path.join(path, f\"network_{network_number}.pth\")\n",
    "\n",
    "#             if os.path.exists(full_path):\n",
    "#                 if not overwrite:\n",
    "#                     raise FileExistsError(f\"The file {full_path} already exists and will not be overwritten.\")\n",
    "#                 else:\n",
    "#                     logging.debug(f\"Overwriting file {full_path}\") # Only log with info as during training we will continuously overwrite the model\n",
    "            \n",
    "#             # Save the model's state_dict using torch.save\n",
    "#             torch.save(network.state_dict(), full_path)\n",
    "#         logging.debug(f\"Model saved successfully to {full_path}\")\n",
    "    \n",
    "#     def load(self, path: str):\n",
    "#         \"\"\"\n",
    "#         Load the PyTorch models from files in the specified directory.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         if not hasattr(self, 'network_list') or self.network_list is None:\n",
    "#             raise AttributeError(\"Cannot find networks to load.\")\n",
    "\n",
    "#         # Check for the presence of model files\n",
    "#         for network_number, network in enumerate(self.network_list):\n",
    "#             full_path = os.path.join(path, f\"network_{network_number}.pth\")\n",
    "\n",
    "#             if not os.path.exists(full_path):\n",
    "#                 raise FileNotFoundError(f\"The file {full_path} does not exist.\")\n",
    "            \n",
    "#             try:\n",
    "#                 # Load each network's state_dict\n",
    "#                 network.load_state_dict(torch.load(full_path))\n",
    "#                 logging.info(f\"Network {network_number} loaded successfully from {full_path}\")\n",
    "#             except Exception as e:\n",
    "#                 raise RuntimeError(f\"An error occurred while loading network {network_number}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(10000, 2) (10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnus/miniforge3/envs/inventory_gym_2/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "INFO:root:Actor network:\n",
      "/Users/magnus/miniforge3/envs/inventory_gym_2/lib/python3.11/site-packages/torchinfo/torchinfo.py:462: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking tuple: (2,)\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MLPActor                                 [1, 1]                    --\n",
      "├─Sequential: 1-1                        [1, 1]                    --\n",
      "│    └─Linear: 2-1                       [1, 64]                   192\n",
      "│    └─ReLU: 2-2                         [1, 64]                   --\n",
      "│    └─Dropout: 2-3                      [1, 64]                   --\n",
      "│    └─Linear: 2-4                       [1, 64]                   4,160\n",
      "│    └─ReLU: 2-5                         [1, 64]                   --\n",
      "│    └─Dropout: 2-6                      [1, 64]                   --\n",
      "│    └─Linear: 2-7                       [1, 1]                    65\n",
      "│    └─Identity: 2-8                     [1, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 4,417\n",
      "Trainable params: 4,417\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Critic network:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking tuple: (2,)\n",
      "Checking tuple: (1,)\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MLPStateAction                           --                        --\n",
      "├─Sequential: 1-1                        [1, 1]                    --\n",
      "│    └─Linear: 2-1                       [1, 64]                   256\n",
      "│    └─ReLU: 2-2                         [1, 64]                   --\n",
      "│    └─Dropout: 2-3                      [1, 64]                   --\n",
      "│    └─Linear: 2-4                       [1, 64]                   4,160\n",
      "│    └─ReLU: 2-5                         [1, 64]                   --\n",
      "│    └─Dropout: 2-6                      [1, 64]                   --\n",
      "│    └─Linear: 2-7                       [1, 1]                    65\n",
      "│    └─Identity: 2-8                     [1, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "==========================================================================================\n",
      "-779.2586167634846 -492.39378518242427\n",
      "-779.2586167634846 -492.39378518242427\n"
     ]
    }
   ],
   "source": [
    "from ddopnew.envs.inventory.single_period import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.experiment_functions import run_experiment, test_agent\n",
    "\n",
    "val_index_start = 8000 #90_000\n",
    "test_index_start = 9000 #100_000\n",
    "\n",
    "X = np.random.standard_normal((10000, 2))\n",
    "Y = np.random.standard_normal((10000, 1))\n",
    "Y += 2*X[:,0].reshape(-1, 1) + 3*X[:,1].reshape(-1, 1)\n",
    "Y = X[:,0].reshape(-1, 1)\n",
    "# truncate Y at 0:\n",
    "Y = np.maximum(Y, 0)\n",
    "# normalize Y max to 1\n",
    "Y = Y/np.max(Y)\n",
    "\n",
    "print(np.max(Y))\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "clip_action = ClipAction(0., 1.)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start, lag_window_params =  {'lag_window': 0, 'include_y': False, 'pre_calc': True})\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    "    q_bound_high = 1.0,\n",
    "    q_bound_low = -0.1,\n",
    "    postprocessors = [clip_action],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "agent = TD3Agent(environment.mdp_info,\n",
    "                obsprocessors = None,      # default: []\n",
    "                device=\"cpu\", # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "environment.train()\n",
    "agent.train()\n",
    "environment.print=False\n",
    "\n",
    "# run_experiment(agent, environment, n_epochs=50, n_steps=1000, run_id = \"test\", save_best=True, print_freq=1) # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
