{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom base agent\n",
    "\n",
    "> Base agent for the integration of mushroom_rl-based agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.rl.mushroom_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "\n",
    "# set logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional, List, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from ddopnew.agents.base import BaseAgent\n",
    "from ddopnew.utils import MDPInfo, Parameter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MushroomBaseAgent(BaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Base class for Agents that integrate MushroomRL agents.\n",
    "    \"\"\"\n",
    "\n",
    "    train_mode = \"env_interaction\"\n",
    "    dropout = True # always keep in True for mushroom_RL, dropout is not desired set drop_prob=0.0\n",
    "    \n",
    "    def __init__(self, \n",
    "            environment_info: MDPInfo,\n",
    "            obsprocessors: Optional[List] = None,     # default: []\n",
    "            device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "            agent_name: str | None = None\n",
    "            ):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.network_list, self.actor, self.critic = self.get_network_list(set_actor_critic_attributes=True)\n",
    "\n",
    "        super().__init__(environment_info = environment_info, obsprocessors = obsprocessors, agent_name = agent_name)\n",
    "\n",
    "        self.transfer_obs_processors_to_mushroom_agent()\n",
    "\n",
    "    def transfer_obs_processors_to_mushroom_agent(self):\n",
    "    \n",
    "        \"\"\" Transfer the obs-processors to the MushroomRL agent preprocessors\"\"\"\n",
    "\n",
    "        for obsprocessor in self.obsprocessors:\n",
    "            self.add_obsprocessor(obsprocessor)\n",
    "        self.obsprocessors = []\n",
    "\n",
    "    def add_obsprocessor(self, obsprocessor: object): \n",
    "        \"\"\"Add an obsprocessor to the agent - overwrites the base\n",
    "        class method to add the obsprocessor to the MushroomRL agent\n",
    "        as preprocessor. Postprocessors stay with the base class\"\"\"\n",
    "        self.agent.add_preprocessor(obsprocessor)\n",
    "\n",
    "    @property\n",
    "    def preprocessors(self):\n",
    "\n",
    "        \"\"\" Return the obsprocessors of the agent,\n",
    "        which are the preprocessors of the MushroomRL agent \"\"\"\n",
    "\n",
    "        return self.agent.preprocessors\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_model(self, input_shape: Tuple, output_shape: Tuple):\n",
    "        \"\"\" Set the model for the agent \"\"\"\n",
    "        pass\n",
    "\n",
    "    def set_optimizer(self, optimizer_params: dict): # dict with keys: optimizer, lr, weight_decay\n",
    "        \n",
    "        \"\"\" Set the optimizer for the model \"\"\"\n",
    "        optimizer = optimizer_params[\"optimizer\"]\n",
    "        optimizer_params_copy = optimizer_params.copy()\n",
    "        del optimizer_params_copy[\"optimizer\"]\n",
    "\n",
    "        if optimizer == \"Adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), **optimizer_params_copy)\n",
    "        elif optimizer == \"SGD\":\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), **optimizer_params_copy)\n",
    "        elif optimizer == \"RMSprop\":\n",
    "            self.optimizer = torch.optim.RMSprop(self.model.parameters(), **optimizer_params_copy)\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer {optimizer} not supported\")\n",
    "\n",
    "    def draw_action_(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \n",
    "        \"\"\" \n",
    "        Draw an action based on the fitted model (see predict method)\n",
    "        \"\"\"\n",
    "\n",
    "        # Remove batch dimension if it is one\n",
    "        if observation.shape[0] == 1:\n",
    "            observation = observation[0]\n",
    "\n",
    "        if self.mode==\"train\":\n",
    "            action = self.agent.draw_action(observation)\n",
    "        else:\n",
    "            action = self.predict(observation) # bypass the agent's draw_action method and directly get prediction from policy network\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def predict(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \"\"\" Do one forward pass of the model directly and return the prediction\"\"\"\n",
    "\n",
    "        if self.mode==\"eval\":\n",
    "\n",
    "            # Apply pre-processors of the mushroom agent\n",
    "            for preprocessor in self.agent.preprocessors:\n",
    "                observation = preprocessor(observation)\n",
    "            \n",
    "            # add batch dimension back to mimic mushroom_rl library\n",
    "            observation = np.expand_dims(observation, axis=0)\n",
    "            action = self.predict_(observation)\n",
    "\n",
    "            return action\n",
    "        else:\n",
    "            raise ValueError(\"Model is in train mode. Use draw_action method instead.\")\n",
    "\n",
    "    def predict_(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \"\"\" Do one forward pass of the model directly and return the prediction\n",
    "        Overwrite for agents that have additional steps such as SAC\"\"\"\n",
    "\n",
    "        observation = torch.tensor(observation, dtype=torch.float32).to(self.device)\n",
    "        action = self.actor.forward(observation)\n",
    "        action = action.cpu().detach().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"set the internal state of the agent and its model to train\"\"\"\n",
    "        self.mode = \"train\"\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"set the internal state of the agent and its model to eval\"\"\"\n",
    "        self.mode = \"eval\"\n",
    "    \n",
    "\n",
    "    def to(self, device: str): #\n",
    "        \"\"\"Move the model to the specified device\"\"\"\n",
    "\n",
    "        # check if self.model or something else\n",
    "        self.model.to(device)\n",
    "\n",
    "    def save(self,\n",
    "                path: str, # The directory where the file will be saved.\n",
    "                overwrite: bool=True): # Allow overwriting; if False, a FileExistsError will be raised if the file exists.\n",
    "        \n",
    "        \"\"\"\n",
    "        Save the PyTorch model to a file in the specified directory.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'network_list') or self.network_list is None:\n",
    "            raise AttributeError(\"Cannot find networks.\")\n",
    "\n",
    "        # Create the directory path if it does not exist\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Construct the file path using os.path.join for better cross-platform compatibility\n",
    "\n",
    "        for network_number, network in enumerate(self.network_list):\n",
    "            full_path = os.path.join(path, f\"network_{network_number}.pth\")\n",
    "\n",
    "            if os.path.exists(full_path):\n",
    "                if not overwrite:\n",
    "                    raise FileExistsError(f\"The file {full_path} already exists and will not be overwritten.\")\n",
    "                else:\n",
    "                    logging.debug(f\"Overwriting file {full_path}\") # Only log with info as during training we will continuously overwrite the model\n",
    "            \n",
    "            # Save the model's state_dict using torch.save\n",
    "            torch.save(network.state_dict(), full_path)\n",
    "        logging.debug(f\"Model saved successfully to {full_path}\")\n",
    "\n",
    "    def load(self, path: str):\n",
    "        \"\"\"\n",
    "        Load the PyTorch models from files in the specified directory.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'network_list') or self.network_list is None:\n",
    "            raise AttributeError(\"Cannot find networks to load.\")\n",
    "\n",
    "        # Check for the presence of model files\n",
    "        for network_number, network in enumerate(self.network_list):\n",
    "            full_path = os.path.join(path, f\"network_{network_number}.pth\")\n",
    "\n",
    "            if not os.path.exists(full_path):\n",
    "                raise FileNotFoundError(f\"The file {full_path} does not exist.\")\n",
    "            \n",
    "            try:\n",
    "                # Load each network's state_dict\n",
    "                network.load_state_dict(torch.load(full_path))\n",
    "                logging.info(f\"Network {network_number} loaded successfully from {full_path}\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"An error occurred while loading network {network_number}: {e}\")\n",
    "\n",
    "    def set_device(self, device: str):\n",
    "\n",
    "        \"\"\" Set the device for the model \"\"\"\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            if torch.cuda.is_available():\n",
    "                use_cuda = True\n",
    "            else:\n",
    "                logging.warning(\"CUDA is not available. Using CPU instead.\")\n",
    "                use_cuda = False\n",
    "        elif device == \"cpu\":\n",
    "            use_cuda = False\n",
    "        else:\n",
    "            raise ValueError(f\"Device {device} not currently not supported, use 'cuda' or 'cpu'\")\n",
    "\n",
    "        return use_cuda\n",
    "\n",
    "    @staticmethod\n",
    "    def get_optimizer_class(optimizer_name: str): #\n",
    "\n",
    "        \"\"\" Get optimizer class based on the optimizer name \"\"\"\n",
    "\n",
    "        if optimizer_name == \"Adam\":\n",
    "            return torch.optim.Adam\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            return torch.optim.SGD\n",
    "        elif optimizer_name == \"RMSprop\":\n",
    "            return torch.optim.RMSprop\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer {optimizer_name} not supported\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss_function(loss: str): #\n",
    "\n",
    "        \"\"\" Get optimizer class based on the optimizer name \"\"\"\n",
    "\n",
    "        if loss == \"MSE\":\n",
    "            return F.mse_loss\n",
    "        else:\n",
    "            raise ValueError(f\"Loss {loss} not supported\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_input_shape(observation_space: object, flatten_time_dim: bool = True): #\n",
    "\n",
    "        \"\"\" Get the input shape of the model based on the environment info \"\"\"\n",
    "\n",
    "        # TODO: Account for more complex spaces like dicts\n",
    "\n",
    "        observation_space_shape = observation_space.shape\n",
    "\n",
    "        if flatten_time_dim:\n",
    "            input_shape = (np.prod(observation_space_shape),)\n",
    "        else:\n",
    "            input_shape = observation_space_shape\n",
    "\n",
    "        return input_shape\n",
    "\n",
    "    def episode_start(self):\n",
    "\n",
    "        \"\"\" What to do if a new episode starts, e.g., reset policy of the agent\n",
    "        Often this does not need to do anything (default), otherwise this funciton \n",
    "        needs to be overwritten in the subclass. \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def fit(self, dataset, **dataset_info):\n",
    "\n",
    "        \"\"\" Hand the fit mehtod to the mushroom agent \"\"\"\n",
    "\n",
    "        self.agent.fit(dataset, **dataset_info)\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\" Stop the agent \"\"\"\n",
    "\n",
    "        self.agent.stop()\n",
    "\n",
    "    @staticmethod\n",
    "    # input tuple or list of tuples\n",
    "    def add_batch_dimension_for_shape(\n",
    "                        input_shape: Tuple | List[Tuple],\n",
    "                        batch_dim: int = 1,\n",
    "                        ) -> Tuple | List[Tuple]:\n",
    "\n",
    "        \"\"\" Add batch dimension to the shape of the input to \n",
    "        ensure torchinfo works correctly \"\"\"\n",
    "\n",
    "        if isinstance(input_shape, tuple):\n",
    "            input_shape = (batch_dim,) + input_shape\n",
    "        elif isinstance(input_shape, list):\n",
    "            input_shape = [(batch_dim,) + shape for shape in input_shape]\n",
    "        else:\n",
    "            raise ValueError(\"Input shape must be tuple or list of tuples\")\n",
    "\n",
    "        return input_shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/rl.py#L35){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## MushroomBaseAgent\n",
       "\n",
       ">      MushroomBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                         obsprocessors:Optional[List]=None, device:str='cpu',\n",
       ">                         agent_name:str|None=None)\n",
       "\n",
       "*Base class for Agents that integrate MushroomRL agents.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| obsprocessors | Optional | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/rl.py#L35){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## MushroomBaseAgent\n",
       "\n",
       ">      MushroomBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                         obsprocessors:Optional[List]=None, device:str='cpu',\n",
       ">                         agent_name:str|None=None)\n",
       "\n",
       "*Base class for Agents that integrate MushroomRL agents.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| obsprocessors | Optional | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MushroomBaseAgent, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XXX\n",
    "\n",
    "XXX\n",
    "\n",
    "**XXXs**:\n",
    "\n",
    "* XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
