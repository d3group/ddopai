{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment functions\n",
    "\n",
    "> Functions to run experiments more efficiently. The usage of these functions is optional and they are only compatible with agents defined in this package. Using agents from other packages such as Stable Baselines or RLlib may require using their own experiment functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, List, Tuple, Dict, Literal\n",
    "import logging\n",
    "from datetime import datetime  \n",
    "import numpy as np\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.base import BaseAgent\n",
    "\n",
    "import importlib\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Think about how to handle mushroom integration.\n",
    "from mushroom_rl.core import Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class EarlyStoppingHandler():\n",
    "\n",
    "    '''\n",
    "    Class to handle early stopping during experiments. The EarlyStoppingHandler handler calculates the average\n",
    "    score over the last \"patience\" epochs and compares it to the average score over the previous \"patience\" epochs.\n",
    "    Note that one epoch we define here as time in between evaluating on a validation set, for supervised learning\n",
    "    typically one epoch is one pass through the training data. For reinforcement learning, in between each evaluation\n",
    "    epoch there may be less than one, one, or many episodes played in the training environment.\n",
    "\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        patience: int = 50, # Number of epochs to evaluate for stopping\n",
    "        warmup: int = 100, # How many initial epochs to wait before evaluating\n",
    "        criteria: str = \"J\",  # Whether to use discounted rewards J or total rewards R as criteria\n",
    "        direction: str = \"max\"  # Whether reward shall be maximized or minimized\n",
    "    ):\n",
    "\n",
    "        self.history = list()\n",
    "        self.patience = patience\n",
    "        if warmup is None or warmup < patience * 2:\n",
    "            warmup = patience * 2\n",
    "        self.warmup = warmup\n",
    "        self.criteria = criteria\n",
    "        self.direction = direction\n",
    "\n",
    "    def add_result(self,\n",
    "                    J: float, # Return (discounted rewards) of the last epoch\n",
    "                    R: float, # Total rewards of the last epoch\n",
    "                    ) -> bool:\n",
    "\n",
    "        \"\"\"\n",
    "        Add the result of the last epoch to the history and check if the experiment should be stopped.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.criteria == \"J\":\n",
    "            self.history.append(J)\n",
    "        elif self.criteria == \"R\":\n",
    "            self.history.append(R)\n",
    "        else:\n",
    "            raise ValueError(\"Criteria must be J or R\")\n",
    "        \n",
    "        if len(self.history) >= self.warmup:\n",
    "            if self.direction == \"max\":\n",
    "                if sum(self.history[-self.patience*2:-self.patience]) >= sum(self.history[-self.patience:]):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            elif self.direction == \"min\":\n",
    "                if sum(self.history[-self.patience*2:-self.patience]) <= sum(self.history[-self.patience:]):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                raise ValueError(\"Direction must be max or min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L27){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## EarlyStoppingHandler\n",
       "\n",
       ">      EarlyStoppingHandler (patience:int=50, warmup:int=100, criteria:str='J',\n",
       ">                            direction:str='max')\n",
       "\n",
       "*Class to handle early stopping during experiments. The EarlyStoppingHandler handler calculates the average\n",
       "score over the last \"patience\" epochs and compares it to the average score over the previous \"patience\" epochs.\n",
       "Note that one epoch we define here as time in between evaluating on a validation set, for supervised learning\n",
       "typically one epoch is one pass through the training data. For reinforcement learning, in between each evaluation\n",
       "epoch there may be less than one, one, or many episodes played in the training environment.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| patience | int | 50 | Number of epochs to evaluate for stopping |\n",
       "| warmup | int | 100 | How many initial epochs to wait before evaluating |\n",
       "| criteria | str | J | Whether to use discounted rewards J or total rewards R as criteria |\n",
       "| direction | str | max | Whether reward shall be maximized or minimized |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L27){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## EarlyStoppingHandler\n",
       "\n",
       ">      EarlyStoppingHandler (patience:int=50, warmup:int=100, criteria:str='J',\n",
       ">                            direction:str='max')\n",
       "\n",
       "*Class to handle early stopping during experiments. The EarlyStoppingHandler handler calculates the average\n",
       "score over the last \"patience\" epochs and compares it to the average score over the previous \"patience\" epochs.\n",
       "Note that one epoch we define here as time in between evaluating on a validation set, for supervised learning\n",
       "typically one epoch is one pass through the training data. For reinforcement learning, in between each evaluation\n",
       "epoch there may be less than one, one, or many episodes played in the training environment.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| patience | int | 50 | Number of epochs to evaluate for stopping |\n",
       "| warmup | int | 100 | How many initial epochs to wait before evaluating |\n",
       "| criteria | str | J | Whether to use discounted rewards J or total rewards R as criteria |\n",
       "| direction | str | max | Whether reward shall be maximized or minimized |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(EarlyStoppingHandler, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L53){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EarlyStoppingHandler.add_result\n",
       "\n",
       ">      EarlyStoppingHandler.add_result (J:float, R:float)\n",
       "\n",
       "*Add the result of the last epoch to the history and check if the experiment should be stopped.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| J | float | Return (discounted rewards) of the last epoch |\n",
       "| R | float | Total rewards of the last epoch |\n",
       "| **Returns** | **bool** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L53){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EarlyStoppingHandler.add_result\n",
       "\n",
       ">      EarlyStoppingHandler.add_result (J:float, R:float)\n",
       "\n",
       "*Add the result of the last epoch to the history and check if the experiment should be stopped.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| J | float | Return (discounted rewards) of the last epoch |\n",
       "| R | float | Total rewards of the last epoch |\n",
       "| **Returns** | **bool** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(EarlyStoppingHandler.add_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "> Some functions that are needed to run an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calculate_score(\n",
    "                    dataset: List,\n",
    "                    env: BaseEnvironment, # Any environment inheriting from BaseEnvironment\n",
    "                    ) -> Tuple[float, float]:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Calculate the total rewards R and the discounted rewards J of a dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    R = sum([row[0][2] for row in dataset])\n",
    "    gamma = env.mdp_info.gamma\n",
    "    J = sum([gamma**(t) * row[0][2] for t, row in enumerate(dataset)]) # Note: t starts at 1 so the first reward is already discounted\n",
    "\n",
    "    return R, J\n",
    "\n",
    "def log_info(R: float,\n",
    "                J: float,\n",
    "                n_epochs: int,\n",
    "                tracking: Literal[\"wandb\"], # only wandb implemented so far\n",
    "                mode: Literal[\"train\", \"val\", \"test\"]\n",
    "                ):\n",
    "    \n",
    "    '''\n",
    "    Logs the same R, J information repeatedly for n_epoochs.\n",
    "    E.g., to draw a straight line in wandb for algorithmes\n",
    "    such as XGB, RF, etc. that can be comparared to the learning\n",
    "    curves of supervised or reinforcement learning algorithms.\n",
    "    '''\n",
    "\n",
    "    if tracking == \"wandb\":\n",
    "        for epoch in range(n_epochs):\n",
    "            wandb.log({f\"{mode}/R\": R, f\"{mode}/J\": J})\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def update_best(R: float, J: float, best_R: float, best_J: float): # \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Update the best total rewards R and the best discounted rewards J.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if R > best_R:\n",
    "        best_R = R\n",
    "    if J > best_J:\n",
    "        best_J = J\n",
    "\n",
    "    return best_R, best_J\n",
    "\n",
    "def save_agent(agent: BaseAgent, # Any agent inheriting from BaseAgent\n",
    "                experiment_dir: str, # Directory to save the agent, \n",
    "                save_best: bool,\n",
    "                R: float,\n",
    "                J: float,\n",
    "                best_R: float,\n",
    "                best_J: float,\n",
    "                criteria: str = \"J\",\n",
    "                force_save = False,\n",
    "                ):\n",
    "\n",
    "    \"\"\"\n",
    "    Save the agent if it has improved either R or J, depending on the criteria argument,\n",
    "    vs. the previous epochs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if save_best:\n",
    "        if criteria == \"R\":\n",
    "            if R == best_R:\n",
    "                save_dir = f\"{experiment_dir}/saved_models/best\"\n",
    "                agent.save(save_dir)\n",
    "            elif force_save:\n",
    "                save_dir = f\"{experiment_dir}/saved_models/best\"\n",
    "                agent.save(save_dir)\n",
    "        elif criteria == \"J\":\n",
    "            if J == best_J:\n",
    "                save_dir = f\"{experiment_dir}/saved_models/best\"\n",
    "                agent.save(save_dir)\n",
    "            elif force_save:\n",
    "                save_dir = f\"{experiment_dir}/saved_models/best\"\n",
    "\n",
    "                # print(f\"Saved agent to {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment functions\n",
    "\n",
    "> Functions to run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def test_agent(agent: BaseAgent,\n",
    "            env: BaseEnvironment,\n",
    "            return_dataset = False,\n",
    "            save_features = False,\n",
    "            tracking = None, # other: \"wandb\",\n",
    "            eval_step_info = False,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Tests the agent on the environment for a single episode\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO make it possible to save dataset via tracking tool\n",
    "\n",
    "    # Run the test episode\n",
    "    dataset = run_test_episode(env, agent, eval_step_info, save_features = save_features)\n",
    "\n",
    "    # Calculate the score\n",
    "    R, J = calculate_score(dataset, env)\n",
    "\n",
    "    if tracking == \"wandb\":\n",
    "        mode = env.mode\n",
    "        wandb.log({f\"{mode}/R\": R, f\"{mode}/J\": J})\n",
    "\n",
    "    if return_dataset:\n",
    "        return R, J, dataset\n",
    "    else:\n",
    "        return R, J\n",
    "\n",
    "def run_test_episode(   env: BaseEnvironment, # Any environment inheriting from BaseEnvironment\n",
    "                        agent: BaseAgent, # Any agent inheriting from BaseAgent\n",
    "                        eval_step_info: bool = False, # Print step info during evaluation\n",
    "                        save_features: bool = False, # Save features (observation) of the dataset. Can be turned off since they sometimes become very large with many lag information\n",
    "\n",
    "                ):\n",
    "\n",
    "    \"\"\"\n",
    "    Runs an episode to test the agent's performance.\n",
    "    It assumes, that agent and environment are initialized, in test/val mode\n",
    "    and have done reset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get initial observation\n",
    "    obs = env.reset()\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    finished = False\n",
    "    step = 0\n",
    "\n",
    "    horizon = env.mdp_info.horizon\n",
    "    \n",
    "    while not finished:\n",
    "        \n",
    "        # Sample action from agent\n",
    "        action = agent.draw_action(obs)\n",
    "\n",
    "        # Take a step in the environment\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        logging.debug(\"##### STEP: %d #####\", env.index)\n",
    "        logging.debug(\"reward: %s\", reward)\n",
    "        logging.debug(\"info: %s\", info)\n",
    "        logging.debug(\"next observation: %s\", obs)\n",
    "        logging.debug(\"truncated: %s\", truncated)\n",
    "\n",
    "        if save_features:\n",
    "            sample = (obs, action, reward, next_obs, terminated, truncated) # unlike mushroom do not include policy_state\n",
    "        else:\n",
    "            sample = (None, action, reward, None, terminated, truncated)\n",
    "\n",
    "        obs = next_obs\n",
    "        \n",
    "        dataset.append((sample, info))\n",
    "\n",
    "        finished = terminated or truncated\n",
    "\n",
    "        if eval_step_info:\n",
    "            step += 1\n",
    "            sys.stdout.write(f\"\\rStep {step}\")\n",
    "            sys.stdout.flush()\n",
    "    if eval_step_info:\n",
    "        print()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def run_experiment( agent: BaseAgent,\n",
    "                    env: BaseEnvironment,\n",
    "\n",
    "                    n_epochs: int,\n",
    "                    n_steps: int = None, # Number of steps to interact with the environment per epoch. Will be ignored for direct_fit and epchos_fit agents\n",
    "\n",
    "                    early_stopping_handler: Union[EarlyStoppingHandler, None] = None,\n",
    "                    save_best: bool = True,\n",
    "                    performance_criterion: str = \"J\", # other: \"R\"\n",
    "\n",
    "                    tracking: Union[str, None]  = None, # other: \"wandb\"\n",
    "\n",
    "                    results_dir: str = \"results\",\n",
    "\n",
    "                    run_id: Union[str, None] = None,\n",
    "\n",
    "                    print_freq: int = 10,\n",
    "\n",
    "                    eval_step_info = False,\n",
    "                ):\n",
    "\n",
    "    \"\"\"\n",
    "    Run an experiment with the given agent and environment for n_epochs. It automaticall dedects if the train mode\n",
    "    of the agent is direct, epochs_fit or env_interaction and runs the experiment accordingly.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # use start_time as id if no run_id is given\n",
    "    if run_id is None:\n",
    "        run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "    experiment_dir = f\"{results_dir}/{run_id}\"\n",
    "\n",
    "    print(f\"Experiment directory: {experiment_dir}\")\n",
    "\n",
    "    logging.info(\"Starting experiment\")\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    # initial evaluation\n",
    "    env.val()\n",
    "    agent.eval()\n",
    "    R, J = test_agent(agent, env, tracking = tracking)\n",
    "\n",
    "    env.train()\n",
    "    agent.train()\n",
    "\n",
    "    logging.info(f\"Initial evaluation: R={R}, J={J}\")\n",
    "\n",
    "    best_J = J \n",
    "    best_R = R\n",
    "\n",
    "    if agent.train_mode == \"direct_fit\":\n",
    "        \n",
    "        logging.info(\"Starting training with direct fit\")\n",
    "        agent.fit(X=env.dataloader.get_all_X(\"train\"), Y=env.dataloader.get_all_Y(\"train\"))\n",
    "        logging.info(\"Finished training with direct fit\")\n",
    "\n",
    "        env.val()\n",
    "        agent.eval()\n",
    "\n",
    "        R, J = test_agent(agent, env, tracking = tracking, eval_step_info=eval_step_info)\n",
    "        best_R, best_J = update_best(R, J, best_R, best_J)\n",
    "\n",
    "        logging.info(f\"Evaluation after training: R={R}, J={J}\")\n",
    "\n",
    "        save_agent(agent, experiment_dir, save_best, R, J, best_R, best_J, performance_criterion, force_save = True) # save even if not best\n",
    "\n",
    "        log_info(R, J, n_epochs-1, tracking, \"val\")\n",
    "\n",
    "    elif agent.train_mode == \"epochs_fit\":\n",
    "\n",
    "        # save initial agent\n",
    "        save_dir = f\"{experiment_dir}/saved_models/best\"\n",
    "        agent.save(save_dir)\n",
    "        \n",
    "        logging.info(\"Starting training with epochs fit\")\n",
    "        for epoch in trange(n_epochs):\n",
    "            \n",
    "            agent.fit_epoch() # Access to dataloader provided to the agent at initialization\n",
    "\n",
    "            env.val()\n",
    "            agent.eval()\n",
    "\n",
    "            R, J = test_agent(agent, env, tracking = tracking, eval_step_info=eval_step_info)\n",
    "            \n",
    "            if ((epoch+1) % print_freq) == 0:\n",
    "                logging.info(f\"Epoch {epoch+1}: R={R}, J={J}\")\n",
    "            \n",
    "            best_R, best_J = update_best(R, J, best_R, best_J)\n",
    "            save_agent(agent, experiment_dir, save_best, R, J, best_R, best_J, performance_criterion)\n",
    "            \n",
    "            if early_stopping_handler is not None:\n",
    "                stop = early_stopping_handler.add_result(J, R)\n",
    "            else:\n",
    "                stop = False\n",
    "\n",
    "            if stop:\n",
    "                log_info(R, J, n_epochs-epoch-1, tracking, \"val\")\n",
    "                logging.info(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "            env.train()\n",
    "            agent.train()\n",
    "\n",
    "        logging.info(\"Finished training with epochs fit\")\n",
    "\n",
    "    elif agent.train_mode == \"env_interaction\":\n",
    "\n",
    "        # save initial agent\n",
    "        save_dir = f\"{experiment_dir}/saved_models/best\"\n",
    "        agent.save(save_dir)\n",
    "\n",
    "        logging.info(\"Starting training with env_interaction\")\n",
    "\n",
    "        core = Core(agent, env)\n",
    "\n",
    "        agent.train()\n",
    "        env.train()\n",
    "\n",
    "        if hasattr(agent, \"warmup_training_steps\"):\n",
    "            warmup_training = True\n",
    "            warmup_training_steps = agent.warmup_training_steps\n",
    "        else:\n",
    "            warmup_training = False\n",
    "        \n",
    "        if hasattr(agent, \"n_steps_per_fit\"):\n",
    "            n_steps_per_fit = agent.n_steps_per_fit\n",
    "        else:\n",
    "            n_steps_per_fit = 1\n",
    "\n",
    "        if warmup_training:\n",
    "            env.set_return_truncation(False) # For mushroom Core to work, the step function should not return the truncation flag\n",
    "            core.learn(n_steps=warmup_training_steps, n_steps_per_fit=warmup_training_steps, quiet=True)\n",
    "        \n",
    "        for epoch in trange(n_epochs):\n",
    "\n",
    "            env.set_return_truncation(False) # For mushroom Core to work, the step function should not return the truncation flag\n",
    "            agent.train()\n",
    "            core.learn(n_steps=n_steps, n_steps_per_fit=n_steps_per_fit, quiet=True)\n",
    "            env.set_return_truncation(True) # Set back to standard gynmasium behavior\n",
    "\n",
    "            env.val()\n",
    "            agent.eval()\n",
    "\n",
    "            R, J = test_agent(agent, env, tracking = tracking, eval_step_info=eval_step_info)\n",
    "\n",
    "            if ((epoch+1) % print_freq) == 0:\n",
    "                logging.info(f\"Epoch {epoch+1}: R={R}, J={J}\")\n",
    "            \n",
    "            best_R, best_J = update_best(R, J, best_R, best_J)\n",
    "            save_agent(agent, experiment_dir, save_best, R, J, best_R, best_J, performance_criterion)\n",
    "\n",
    "            if early_stopping_handler is not None:\n",
    "                stop = early_stopping_handler.add_result(J, R)\n",
    "            else:\n",
    "                stop = False\n",
    "\n",
    "            if stop:\n",
    "                log_info(R, J, n_epochs-epoch-1, tracking, \"val\")\n",
    "                logging.info(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "            env.train()\n",
    "            agent.train()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown train mode\")\n",
    "\n",
    "    logging.info(f\"Evaluation after training: R={R}, J={J}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L245){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### run_experiment\n",
       "\n",
       ">      run_experiment (agent:ddopnew.agents.base.BaseAgent,\n",
       ">                      env:ddopnew.envs.base.BaseEnvironment, n_epochs:int,\n",
       ">                      n_steps:int=None, early_stopping_handler:Optional[__main_\n",
       ">                      _.EarlyStoppingHandler]=None, save_best:bool=True,\n",
       ">                      performance_criterion:str='J',\n",
       ">                      tracking:Optional[str]=None, results_dir:str='results',\n",
       ">                      run_id:Optional[str]=None, print_freq:int=10,\n",
       ">                      eval_step_info=False)\n",
       "\n",
       "*Run an experiment with the given agent and environment for n_epochs. It automaticall dedects if the train mode\n",
       "of the agent is direct, epochs_fit or env_interaction and runs the experiment accordingly.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| agent | BaseAgent |  |  |\n",
       "| env | BaseEnvironment |  |  |\n",
       "| n_epochs | int |  |  |\n",
       "| n_steps | int | None | Number of steps to interact with the environment per epoch. Will be ignored for direct_fit and epchos_fit agents |\n",
       "| early_stopping_handler | Optional | None |  |\n",
       "| save_best | bool | True |  |\n",
       "| performance_criterion | str | J | other: \"R\" |\n",
       "| tracking | Optional | None | other: \"wandb\" |\n",
       "| results_dir | str | results |  |\n",
       "| run_id | Optional | None |  |\n",
       "| print_freq | int | 10 |  |\n",
       "| eval_step_info | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L245){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### run_experiment\n",
       "\n",
       ">      run_experiment (agent:ddopnew.agents.base.BaseAgent,\n",
       ">                      env:ddopnew.envs.base.BaseEnvironment, n_epochs:int,\n",
       ">                      n_steps:int=None, early_stopping_handler:Optional[__main_\n",
       ">                      _.EarlyStoppingHandler]=None, save_best:bool=True,\n",
       ">                      performance_criterion:str='J',\n",
       ">                      tracking:Optional[str]=None, results_dir:str='results',\n",
       ">                      run_id:Optional[str]=None, print_freq:int=10,\n",
       ">                      eval_step_info=False)\n",
       "\n",
       "*Run an experiment with the given agent and environment for n_epochs. It automaticall dedects if the train mode\n",
       "of the agent is direct, epochs_fit or env_interaction and runs the experiment accordingly.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| agent | BaseAgent |  |  |\n",
       "| env | BaseEnvironment |  |  |\n",
       "| n_epochs | int |  |  |\n",
       "| n_steps | int | None | Number of steps to interact with the environment per epoch. Will be ignored for direct_fit and epchos_fit agents |\n",
       "| early_stopping_handler | Optional | None |  |\n",
       "| save_best | bool | True |  |\n",
       "| performance_criterion | str | J | other: \"R\" |\n",
       "| tracking | Optional | None | other: \"wandb\" |\n",
       "| results_dir | str | results |  |\n",
       "| run_id | Optional | None |  |\n",
       "| print_freq | int | 10 |  |\n",
       "| eval_step_info | bool | False |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(run_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important notes on running experiments\n",
    "\n",
    "\n",
    "**Training mode**:\n",
    "\n",
    "* Agents have either a training mode ```direct_fit``` or ```epochs_fit``` or ```env_interaction```. ```direct_fit``` means that agents are called with a single call to the fit method, providing the full X and Y dataset. ```epochs_fit``` means that agents are training iteratively via epochs. It is assumed that they then have access to the dataloader.\n",
    "\n",
    "**Train, val, test mode**:\n",
    "\n",
    "* The function always sets the agent and environment to the approproate dataset mode (and thereofore indirectly the dataloader via then environment).\n",
    "\n",
    "**Early stopping**:\n",
    "\n",
    "* Can be optionally applied for ```epochs_fit``` and ```env_interaction``` agents.\n",
    "\n",
    "**Save best agent**:\n",
    "\n",
    "* The ```save_agent()``` functions, given the ```save_best```param is ```True```, will save the best agent based on the validation score.\n",
    "\n",
    "* At test time at a later point, one can then load the best agent and evaluate it on the test set (not done automatically by this function).\n",
    "\n",
    "**Logging**:\n",
    "\n",
    "* By setting logging to ```\"wandb\"``` the function will log J and R to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L163){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### test_agent\n",
       "\n",
       ">      test_agent (agent:ddopnew.agents.base.BaseAgent,\n",
       ">                  env:ddopnew.envs.base.BaseEnvironment, return_dataset=False,\n",
       ">                  tracking=None, eval_step_info=False)\n",
       "\n",
       "*Tests the agent on the environment for a single episode*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| agent | BaseAgent |  |  |\n",
       "| env | BaseEnvironment |  |  |\n",
       "| return_dataset | bool | False |  |\n",
       "| tracking | NoneType | None | other: \"wandb\", |\n",
       "| eval_step_info | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L163){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### test_agent\n",
       "\n",
       ">      test_agent (agent:ddopnew.agents.base.BaseAgent,\n",
       ">                  env:ddopnew.envs.base.BaseEnvironment, return_dataset=False,\n",
       ">                  tracking=None, eval_step_info=False)\n",
       "\n",
       "*Tests the agent on the environment for a single episode*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| agent | BaseAgent |  |  |\n",
       "| env | BaseEnvironment |  |  |\n",
       "| return_dataset | bool | False |  |\n",
       "| tracking | NoneType | None | other: \"wandb\", |\n",
       "| eval_step_info | bool | False |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(test_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L191){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### run_test_episode\n",
       "\n",
       ">      run_test_episode (env:ddopnew.envs.base.BaseEnvironment,\n",
       ">                        agent:ddopnew.agents.base.BaseAgent,\n",
       ">                        eval_step_info:bool=False)\n",
       "\n",
       "*Runs an episode to test the agent's performance.\n",
       "It assumes, that agent and environment are initialized, in test/val mode\n",
       "and have done reset.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| env | BaseEnvironment |  | Any environment inheriting from BaseEnvironment |\n",
       "| agent | BaseAgent |  | Any agent inheriting from BaseAgent |\n",
       "| eval_step_info | bool | False | Print step info during evaluation |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L191){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### run_test_episode\n",
       "\n",
       ">      run_test_episode (env:ddopnew.envs.base.BaseEnvironment,\n",
       ">                        agent:ddopnew.agents.base.BaseAgent,\n",
       ">                        eval_step_info:bool=False)\n",
       "\n",
       "*Runs an episode to test the agent's performance.\n",
       "It assumes, that agent and environment are initialized, in test/val mode\n",
       "and have done reset.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| env | BaseEnvironment |  | Any environment inheriting from BaseEnvironment |\n",
       "| agent | BaseAgent |  | Any agent inheriting from BaseAgent |\n",
       "| eval_step_info | bool | False | Print step info during evaluation |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(run_test_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage example for ```test_agent()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: [0.76942194]\n",
      "action: [0.91951203]\n",
      "action: [0.42798653]\n",
      "action: [0.10505912]\n",
      "action: [0.39162073]\n",
      "action: [2.2743504]\n",
      "action: [0.46696663]\n",
      "action: [0.5055452]\n",
      "action: [5.5919414]\n",
      "action: [1.2253612]\n",
      "R: -8.823590313371968, J: -8.767755363059834\n"
     ]
    }
   ],
   "source": [
    "from ddopnew.envs.inventory.single_period import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.agents.basic import RandomAgent\n",
    "\n",
    "\n",
    "val_index_start = 80 #90_000\n",
    "test_index_start = 90 #100_000\n",
    "\n",
    "X = np.random.rand(100, 2)\n",
    "Y = np.random.rand(100, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start)\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    ")\n",
    "\n",
    "agent = RandomAgent(environment.mdp_info)\n",
    "\n",
    "environment.test()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(f\"R: {R}, J: {J}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
