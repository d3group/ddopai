{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment functions\n",
    "\n",
    "> Functions to run experiments more efficiently. The usage of these functions is optional and they are only compatible with agents defined in this package. Using agents from other packages such as Stable Baselines or RLlib may require using their own experiment functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, List, Tuple, Dict, Literal\n",
    "import logging\n",
    "from datetime import datetime  \n",
    "import numpy as np\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.base import BaseAgent\n",
    "\n",
    "import importlib\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Think about how to handle mushroom integration.\n",
    "from mushroom_rl.core import Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class EarlyStoppingHandler():\n",
    "\n",
    "    '''\n",
    "    Class to handle early stopping during experiments. The EarlyStoppingHandler handler calculates the average\n",
    "    score over the last \"patience\" epochs and compares it to the average score over the previous \"patience\" epochs.\n",
    "    Note that one epoch we define here as time in between evaluating on a validation set, for supervised learning\n",
    "    typically one epoch is one pass through the training data. For reinforcement learning, in between each evaluation\n",
    "    epoch there may be less than one, one, or many episodes played in the training environment.\n",
    "\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        patience: int = 50, # Number of epochs to evaluate for stopping\n",
    "        warmup: int = 100, # How many initial epochs to wait before evaluating\n",
    "        criteria: str = \"J\",  # Whether to use discounted rewards J or total rewards R as criteria\n",
    "        direction: str = \"max\"  # Whether reward shall be maximized or minimized\n",
    "    ):\n",
    "\n",
    "        self.history = list()\n",
    "        self.patience = patience\n",
    "        if warmup is None or warmup < patience * 2:\n",
    "            warmup = patience * 2\n",
    "        self.warmup = warmup\n",
    "        self.criteria = criteria\n",
    "        self.direction = direction\n",
    "\n",
    "    def add_result(self,\n",
    "                    J: float, # Return (discounted rewards) of the last epoch\n",
    "                    R: float, # Total rewards of the last epoch\n",
    "                    ) -> bool:\n",
    "\n",
    "        \"\"\"\n",
    "        Add the result of the last epoch to the history and check if the experiment should be stopped.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.criteria == \"J\":\n",
    "            self.history.append(J)\n",
    "        elif self.criteria == \"R\":\n",
    "            self.history.append(R)\n",
    "        else:\n",
    "            raise ValueError(\"Criteria must be J or R\")\n",
    "        \n",
    "        if len(self.history) >= self.warmup:\n",
    "            if self.direction == \"max\":\n",
    "                if sum(self.history[-self.patience*2:-self.patience]) >= sum(self.history[-self.patience:]):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            elif self.direction == \"min\":\n",
    "                if sum(self.history[-self.patience*2:-self.patience]) <= sum(self.history[-self.patience:]):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                raise ValueError(\"Direction must be max or min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L27){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## EarlyStoppingHandler\n",
       "\n",
       ">      EarlyStoppingHandler (patience:int=50, warmup:int=100, criteria:str='J',\n",
       ">                            direction:str='max')\n",
       "\n",
       "*Class to handle early stopping during experiments. The EarlyStoppingHandler handler calculates the average\n",
       "score over the last \"patience\" epochs and compares it to the average score over the previous \"patience\" epochs.\n",
       "Note that one epoch we define here as time in between evaluating on a validation set, for supervised learning\n",
       "typically one epoch is one pass through the training data. For reinforcement learning, in between each evaluation\n",
       "epoch there may be less than one, one, or many episodes played in the training environment.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| patience | int | 50 | Number of epochs to evaluate for stopping |\n",
       "| warmup | int | 100 | How many initial epochs to wait before evaluating |\n",
       "| criteria | str | J | Whether to use discounted rewards J or total rewards R as criteria |\n",
       "| direction | str | max | Whether reward shall be maximized or minimized |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L27){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## EarlyStoppingHandler\n",
       "\n",
       ">      EarlyStoppingHandler (patience:int=50, warmup:int=100, criteria:str='J',\n",
       ">                            direction:str='max')\n",
       "\n",
       "*Class to handle early stopping during experiments. The EarlyStoppingHandler handler calculates the average\n",
       "score over the last \"patience\" epochs and compares it to the average score over the previous \"patience\" epochs.\n",
       "Note that one epoch we define here as time in between evaluating on a validation set, for supervised learning\n",
       "typically one epoch is one pass through the training data. For reinforcement learning, in between each evaluation\n",
       "epoch there may be less than one, one, or many episodes played in the training environment.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| patience | int | 50 | Number of epochs to evaluate for stopping |\n",
       "| warmup | int | 100 | How many initial epochs to wait before evaluating |\n",
       "| criteria | str | J | Whether to use discounted rewards J or total rewards R as criteria |\n",
       "| direction | str | max | Whether reward shall be maximized or minimized |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(EarlyStoppingHandler, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L53){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EarlyStoppingHandler.add_result\n",
       "\n",
       ">      EarlyStoppingHandler.add_result (J:float, R:float)\n",
       "\n",
       "*Add the result of the last epoch to the history and check if the experiment should be stopped.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| J | float | Return (discounted rewards) of the last epoch |\n",
       "| R | float | Total rewards of the last epoch |\n",
       "| **Returns** | **bool** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L53){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EarlyStoppingHandler.add_result\n",
       "\n",
       ">      EarlyStoppingHandler.add_result (J:float, R:float)\n",
       "\n",
       "*Add the result of the last epoch to the history and check if the experiment should be stopped.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| J | float | Return (discounted rewards) of the last epoch |\n",
       "| R | float | Total rewards of the last epoch |\n",
       "| **Returns** | **bool** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(EarlyStoppingHandler.add_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "> Some functions that are needed to run an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calculate_score(\n",
    "                    dataset: List,\n",
    "                    env: BaseEnvironment, # Any environment inheriting from BaseEnvironment\n",
    "                    ) -> Tuple[float, float]:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Calculate the total rewards R and the discounted rewards J of a dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    R = sum([row[0][2] for row in dataset])\n",
    "    gamma = env.mdp_info.gamma\n",
    "    J = sum([gamma**(t) * row[0][2] for t, row in enumerate(dataset)]) # Note: t starts at 1 so the first reward is already discounted\n",
    "\n",
    "    return R, J\n",
    "\n",
    "def log_info(R: float,\n",
    "                J: float,\n",
    "                n_epochs: int,\n",
    "                tracking: Literal[\"wandb\"], # only wandb implemented so far\n",
    "                mode: Literal[\"train\", \"val\", \"test\"]\n",
    "                ):\n",
    "    \n",
    "    '''\n",
    "    Logs the same R, J information repeatedly for n_epoochs.\n",
    "    E.g., to draw a straight line in wandb for algorithmes\n",
    "    such as XGB, RF, etc. that can be comparared to the learning\n",
    "    curves of supervised or reinforcement learning algorithms.\n",
    "    '''\n",
    "\n",
    "    if tracking == \"wandb\":\n",
    "        for epoch in range(n_epochs):\n",
    "            wandb.log({f\"{mode}/R\": R, f\"{mode}/J\": J})\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def update_best(R: float, J: float, best_R: float, best_J: float): # \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Update the best total rewards R and the best discounted rewards J.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if R > best_R:\n",
    "        best_R = R\n",
    "    if J > best_J:\n",
    "        best_J = J\n",
    "\n",
    "    return best_R, best_J\n",
    "\n",
    "def save_agent(agent: BaseAgent, # Any agent inheriting from BaseAgent\n",
    "                experiment_dir: str, # Directory to save the agent, \n",
    "                save_best: bool,\n",
    "                R: float,\n",
    "                J: float,\n",
    "                best_R: float,\n",
    "                best_J: float,\n",
    "                criteria: str = \"J\"\n",
    "                ):\n",
    "\n",
    "    \"\"\"\n",
    "    Save the agent if it has improved either R or J, depending on the criteria argument,\n",
    "    vs. the previous epochs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if save_best:\n",
    "        if criteria == \"R\":\n",
    "            if R == best_R:\n",
    "                save_dir = f\"{experiment_dir}/saved_models/best\"\n",
    "                agent.save(save_dir)\n",
    "        elif criteria == \"J\":\n",
    "            if J == best_J:\n",
    "                save_dir = f\"{experiment_dir}/saved_models/best\"\n",
    "                agent.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment functions\n",
    "\n",
    "> Functions to run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def test_agent(agent: BaseAgent,\n",
    "            env: BaseEnvironment,\n",
    "            return_dataset = False,\n",
    "            tracking = None, # other: \"wandb\",\n",
    "            eval_step_info = False,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Tests the agent on the environment for a single episode\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO make it possible to save dataset via tracking tool\n",
    "\n",
    "    # Run the test episode\n",
    "    dataset = run_test_episode(env, agent, eval_step_info)\n",
    "\n",
    "    # Calculate the score\n",
    "    R, J = calculate_score(dataset, env)\n",
    "\n",
    "    if tracking == \"wandb\":\n",
    "        mode = env.mode\n",
    "        wandb.log({f\"{mode}/R\": R, f\"{mode}/J\": J})\n",
    "\n",
    "    if return_dataset:\n",
    "        return R, J, dataset\n",
    "    else:\n",
    "        return R, J\n",
    "\n",
    "def run_test_episode(   env: BaseEnvironment, # Any environment inheriting from BaseEnvironment\n",
    "                        agent: BaseAgent, # Any agent inheriting from BaseAgent\n",
    "                        eval_step_info: bool = False, # Print step info during evaluation\n",
    "\n",
    "                ):\n",
    "\n",
    "    \"\"\"\n",
    "    Runs an episode to test the agent's performance.\n",
    "    It assumes, that agent and environment are initialized, in test/val mode\n",
    "    and have done reset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get initial observation\n",
    "    obs = env.reset()\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    finished = False\n",
    "    step = 0\n",
    "\n",
    "    horizon = env.mdp_info.horizon\n",
    "    \n",
    "    while not finished:\n",
    "        \n",
    "        # Sample action from agent\n",
    "        action = agent.draw_action(obs)\n",
    "\n",
    "        # Take a step in the environment\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        logging.debug(\"##### STEP: %d #####\", env.index)\n",
    "        logging.debug(\"reward: %s\", reward)\n",
    "        logging.debug(\"info: %s\", info)\n",
    "        logging.debug(\"next observation: %s\", obs)\n",
    "        logging.debug(\"truncated: %s\", truncated)\n",
    "\n",
    "        sample = (obs, action, reward, next_obs, terminated, truncated) # unlike mushroom do not include policy_state\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        dataset.append((sample, info))\n",
    "\n",
    "        finished = terminated or truncated\n",
    "\n",
    "        if eval_step_info:\n",
    "            step += 1\n",
    "            sys.stdout.write(f\"\\rStep {step}\")\n",
    "            sys.stdout.flush()\n",
    "    if eval_step_info:\n",
    "        print()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def run_experiment( agent: BaseAgent,\n",
    "                    env: BaseEnvironment,\n",
    "\n",
    "                    n_epochs: int,\n",
    "                    n_steps: int = None, # Number of steps to interact with the environment per epoch. Will be ignored for direct_fit and epchos_fit agents\n",
    "\n",
    "                    early_stopping_handler: Union[EarlyStoppingHandler, None] = None,\n",
    "                    save_best: bool = True,\n",
    "                    performance_criterion: str = \"J\", # other: \"R\"\n",
    "\n",
    "                    tracking: Union[str, None]  = None, # other: \"wandb\"\n",
    "\n",
    "                    results_dir: str = \"results\",\n",
    "\n",
    "                    run_id: Union[str, None] = None,\n",
    "\n",
    "                    print_freq: int = 10,\n",
    "\n",
    "                    eval_step_info = False,\n",
    "                ):\n",
    "\n",
    "    \"\"\"\n",
    "    Run an experiment with the given agent and environment for n_epochs. It automaticall dedects if the train mode\n",
    "    of the agent is direct, epochs_fit or env_interaction and runs the experiment accordingly.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # use start_time as id if no run_id is given\n",
    "    if run_id is None:\n",
    "        run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "    experiment_dir = f\"{results_dir}/{run_id}\"\n",
    "\n",
    "    logging.info(\"Starting experiment\")\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    # initial evaluation\n",
    "    env.val()\n",
    "    agent.eval()\n",
    "    R, J = test_agent(agent, env, tracking = tracking)\n",
    "\n",
    "    env.train()\n",
    "    agent.train()\n",
    "\n",
    "    logging.info(f\"Initial evaluation: R={R}, J={J}\")\n",
    "\n",
    "    best_J = J \n",
    "    best_R = R\n",
    "\n",
    "    if agent.train_mode == \"direct_fit\":\n",
    "        \n",
    "        logging.info(\"Starting training with direct fit\")\n",
    "        agent.fit(X=env.dataloader.get_all_X(\"train\"), Y=env.dataloader.get_all_Y(\"train\"))\n",
    "        logging.info(\"Finished training with direct fit\")\n",
    "\n",
    "        env.val()\n",
    "        agent.eval()\n",
    "\n",
    "        R, J = test_agent(agent, env, tracking = tracking, eval_step_info=eval_step_info)\n",
    "        best_R, best_J = update_best(R, J, best_R, best_J)\n",
    "\n",
    "        logging.info(f\"Evaluation after training: R={R}, J={J}\")\n",
    "\n",
    "        save_agent(agent, experiment_dir, save_best, R, J, best_R, best_J, performance_criterion)\n",
    "\n",
    "        log_info(R, J, n_epochs-1, tracking, \"val\")\n",
    "\n",
    "    elif agent.train_mode == \"epochs_fit\":\n",
    "        \n",
    "        logging.info(\"Starting training with epochs fit\")\n",
    "        for epoch in trange(n_epochs):\n",
    "\n",
    "            agent.fit_epoch() # Access to dataloader provided to the agent at initialization\n",
    "\n",
    "            env.val()\n",
    "            agent.eval()\n",
    "\n",
    "            R, J = test_agent(agent, env, tracking = tracking, eval_step_info=eval_step_info)\n",
    "\n",
    "            if ((epoch+1) % print_freq) == 0:\n",
    "                logging.info(f\"Epoch {epoch+1}: R={R}, J={J}\")\n",
    "            \n",
    "            best_R, best_J = update_best(R, J, best_R, best_J)\n",
    "            save_agent(agent, experiment_dir, save_best, R, J, best_R, best_J, performance_criterion)\n",
    "            \n",
    "            if early_stopping_handler is not None:\n",
    "                stop = early_stopping_handler.add_result(J, R)\n",
    "            else:\n",
    "                stop = False\n",
    "\n",
    "            if stop:\n",
    "                log_info(R, J, n_epochs-epoch-1, tracking, \"val\")\n",
    "                logging.info(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "            env.train()\n",
    "            agent.train()\n",
    "\n",
    "        logging.info(\"Finished training with epochs fit\")\n",
    "\n",
    "    elif agent.train_mode == \"env_interaction\":\n",
    "\n",
    "        logging.info(\"Starting training with env_interaction\")\n",
    "\n",
    "        core = Core(agent, env)\n",
    "\n",
    "        agent.train()\n",
    "        env.train()\n",
    "\n",
    "        if hasattr(agent, \"warmup_training_steps\"):\n",
    "            warmup_training = True\n",
    "            warmup_training_steps = agent.warmup_training_steps\n",
    "        else:\n",
    "            warmup_training = False\n",
    "        \n",
    "        if hasattr(agent, \"n_steps_per_fit\"):\n",
    "            n_steps_per_fit = agent.n_steps_per_fit\n",
    "        else:\n",
    "            n_steps_per_fit = 1\n",
    "\n",
    "        if warmup_training:\n",
    "            env.set_return_truncation(False) # For mushroom Core to work, the step function should not return the truncation flag\n",
    "            core.learn(n_steps=warmup_training_steps, n_steps_per_fit=warmup_training_steps, quiet=True)\n",
    "        \n",
    "        for epoch in trange(n_epochs):\n",
    "\n",
    "            env.set_return_truncation(False) # For mushroom Core to work, the step function should not return the truncation flag\n",
    "            agent.train()\n",
    "            core.learn(n_steps=n_steps, n_steps_per_fit=n_steps_per_fit, quiet=True)\n",
    "            env.set_return_truncation(True) # Set back to standard gynmasium behavior\n",
    "\n",
    "            env.val()\n",
    "            agent.eval()\n",
    "\n",
    "            R, J = test_agent(agent, env, tracking = tracking, eval_step_info=eval_step_info)\n",
    "\n",
    "            if ((epoch+1) % print_freq) == 0:\n",
    "                logging.info(f\"Epoch {epoch+1}: R={R}, J={J}\")\n",
    "            \n",
    "            best_R, best_J = update_best(R, J, best_R, best_J)\n",
    "            save_agent(agent, experiment_dir, save_best, R, J, best_R, best_J, performance_criterion)\n",
    "\n",
    "            if early_stopping_handler is not None:\n",
    "                stop = early_stopping_handler.add_result(J, R)\n",
    "            else:\n",
    "                stop = False\n",
    "\n",
    "            if stop:\n",
    "                log_info(R, J, n_epochs-epoch-1, tracking, \"val\")\n",
    "                logging.info(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "            env.train()\n",
    "            agent.train()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown train mode\")\n",
    "\n",
    "    logging.info(f\"Evaluation after training: R={R}, J={J}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L245){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### run_experiment\n",
       "\n",
       ">      run_experiment (agent:ddopnew.agents.base.BaseAgent,\n",
       ">                      env:ddopnew.envs.base.BaseEnvironment, n_epochs:int,\n",
       ">                      n_steps:int=None, early_stopping_handler:Optional[__main_\n",
       ">                      _.EarlyStoppingHandler]=None, save_best:bool=True,\n",
       ">                      performance_criterion:str='J',\n",
       ">                      tracking:Optional[str]=None, results_dir:str='results',\n",
       ">                      run_id:Optional[str]=None, print_freq:int=10,\n",
       ">                      eval_step_info=False)\n",
       "\n",
       "*Run an experiment with the given agent and environment for n_epochs. It automaticall dedects if the train mode\n",
       "of the agent is direct, epochs_fit or env_interaction and runs the experiment accordingly.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| agent | BaseAgent |  |  |\n",
       "| env | BaseEnvironment |  |  |\n",
       "| n_epochs | int |  |  |\n",
       "| n_steps | int | None | Number of steps to interact with the environment per epoch. Will be ignored for direct_fit and epchos_fit agents |\n",
       "| early_stopping_handler | Optional | None |  |\n",
       "| save_best | bool | True |  |\n",
       "| performance_criterion | str | J | other: \"R\" |\n",
       "| tracking | Optional | None | other: \"wandb\" |\n",
       "| results_dir | str | results |  |\n",
       "| run_id | Optional | None |  |\n",
       "| print_freq | int | 10 |  |\n",
       "| eval_step_info | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L245){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### run_experiment\n",
       "\n",
       ">      run_experiment (agent:ddopnew.agents.base.BaseAgent,\n",
       ">                      env:ddopnew.envs.base.BaseEnvironment, n_epochs:int,\n",
       ">                      n_steps:int=None, early_stopping_handler:Optional[__main_\n",
       ">                      _.EarlyStoppingHandler]=None, save_best:bool=True,\n",
       ">                      performance_criterion:str='J',\n",
       ">                      tracking:Optional[str]=None, results_dir:str='results',\n",
       ">                      run_id:Optional[str]=None, print_freq:int=10,\n",
       ">                      eval_step_info=False)\n",
       "\n",
       "*Run an experiment with the given agent and environment for n_epochs. It automaticall dedects if the train mode\n",
       "of the agent is direct, epochs_fit or env_interaction and runs the experiment accordingly.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| agent | BaseAgent |  |  |\n",
       "| env | BaseEnvironment |  |  |\n",
       "| n_epochs | int |  |  |\n",
       "| n_steps | int | None | Number of steps to interact with the environment per epoch. Will be ignored for direct_fit and epchos_fit agents |\n",
       "| early_stopping_handler | Optional | None |  |\n",
       "| save_best | bool | True |  |\n",
       "| performance_criterion | str | J | other: \"R\" |\n",
       "| tracking | Optional | None | other: \"wandb\" |\n",
       "| results_dir | str | results |  |\n",
       "| run_id | Optional | None |  |\n",
       "| print_freq | int | 10 |  |\n",
       "| eval_step_info | bool | False |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(run_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important notes on running experiments\n",
    "\n",
    "\n",
    "**Training mode**:\n",
    "\n",
    "* Agents have either a training mode ```direct_fit``` or ```epochs_fit``` or ```env_interaction```. ```direct_fit``` means that agents are called with a single call to the fit method, providing the full X and Y dataset. ```epochs_fit``` means that agents are training iteratively via epochs. It is assumed that they then have access to the dataloader.\n",
    "\n",
    "**Train, val, test mode**:\n",
    "\n",
    "* The function always sets the agent and environment to the approproate dataset mode (and thereofore indirectly the dataloader via then environment).\n",
    "\n",
    "**Early stopping**:\n",
    "\n",
    "* Can be optionally applied for ```epochs_fit``` and ```env_interaction``` agents.\n",
    "\n",
    "**Save best agent**:\n",
    "\n",
    "* The ```save_agent()``` functions, given the ```save_best```param is ```True```, will save the best agent based on the validation score.\n",
    "\n",
    "* At test time at a later point, one can then load the best agent and evaluate it on the test set (not done automatically by this function).\n",
    "\n",
    "**Logging**:\n",
    "\n",
    "* By setting logging to ```\"wandb\"``` the function will log J and R to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L163){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### test_agent\n",
       "\n",
       ">      test_agent (agent:ddopnew.agents.base.BaseAgent,\n",
       ">                  env:ddopnew.envs.base.BaseEnvironment, return_dataset=False,\n",
       ">                  tracking=None, eval_step_info=False)\n",
       "\n",
       "*Tests the agent on the environment for a single episode*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| agent | BaseAgent |  |  |\n",
       "| env | BaseEnvironment |  |  |\n",
       "| return_dataset | bool | False |  |\n",
       "| tracking | NoneType | None | other: \"wandb\", |\n",
       "| eval_step_info | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L163){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### test_agent\n",
       "\n",
       ">      test_agent (agent:ddopnew.agents.base.BaseAgent,\n",
       ">                  env:ddopnew.envs.base.BaseEnvironment, return_dataset=False,\n",
       ">                  tracking=None, eval_step_info=False)\n",
       "\n",
       "*Tests the agent on the environment for a single episode*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| agent | BaseAgent |  |  |\n",
       "| env | BaseEnvironment |  |  |\n",
       "| return_dataset | bool | False |  |\n",
       "| tracking | NoneType | None | other: \"wandb\", |\n",
       "| eval_step_info | bool | False |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(test_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L191){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### run_test_episode\n",
       "\n",
       ">      run_test_episode (env:ddopnew.envs.base.BaseEnvironment,\n",
       ">                        agent:ddopnew.agents.base.BaseAgent,\n",
       ">                        eval_step_info:bool=False)\n",
       "\n",
       "*Runs an episode to test the agent's performance.\n",
       "It assumes, that agent and environment are initialized, in test/val mode\n",
       "and have done reset.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| env | BaseEnvironment |  | Any environment inheriting from BaseEnvironment |\n",
       "| agent | BaseAgent |  | Any agent inheriting from BaseAgent |\n",
       "| eval_step_info | bool | False | Print step info during evaluation |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/experiment_functions.py#L191){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### run_test_episode\n",
       "\n",
       ">      run_test_episode (env:ddopnew.envs.base.BaseEnvironment,\n",
       ">                        agent:ddopnew.agents.base.BaseAgent,\n",
       ">                        eval_step_info:bool=False)\n",
       "\n",
       "*Runs an episode to test the agent's performance.\n",
       "It assumes, that agent and environment are initialized, in test/val mode\n",
       "and have done reset.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| env | BaseEnvironment |  | Any environment inheriting from BaseEnvironment |\n",
       "| agent | BaseAgent |  | Any agent inheriting from BaseAgent |\n",
       "| eval_step_info | bool | False | Print step info during evaluation |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(run_test_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage example for ```test_agent()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "y_true and y_pred must have the same shape, but got (1,) and ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m agent \u001b[38;5;241m=\u001b[39m RandomAgent(environment\u001b[38;5;241m.\u001b[39mmdp_info)\n\u001b[1;32m     24\u001b[0m environment\u001b[38;5;241m.\u001b[39mtest()\n\u001b[0;32m---> 26\u001b[0m R, J \u001b[38;5;241m=\u001b[39m \u001b[43mtest_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, J: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mJ\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mtest_agent\u001b[0;34m(agent, env, return_dataset, tracking, eval_step_info)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mTests the agent on the environment for a single episode\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# TODO make it possible to save dataset via tracking tool\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run the test episode\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mrun_test_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_step_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate the score\u001b[39;00m\n\u001b[1;32m     20\u001b[0m R, J \u001b[38;5;241m=\u001b[39m calculate_score(dataset, env)\n",
      "Cell \u001b[0;32mIn[8], line 60\u001b[0m, in \u001b[0;36mrun_test_episode\u001b[0;34m(env, agent, eval_step_info)\u001b[0m\n\u001b[1;32m     56\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdraw_action(obs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Take a step in the environment\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m##### STEP: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m #####\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m     63\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, reward)\n",
      "File \u001b[0;32m~/Documents/02_PhD/Other_python_projects/00_ddop_new/ddopnew/ddopnew/envs/base.py:90\u001b[0m, in \u001b[0;36mBaseEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m postprocessor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocessors:\n\u001b[1;32m     88\u001b[0m     action \u001b[38;5;241m=\u001b[39m postprocessor(action)\n\u001b[0;32m---> 90\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_truncation_handler(observation, reward, terminated, truncated, info)\n",
      "File \u001b[0;32m~/Documents/02_PhD/Other_python_projects/00_ddop_new/ddopnew/ddopnew/envs/inventory/single_period.py:91\u001b[0m, in \u001b[0;36mNewsvendorEnv.step_\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     87\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(action)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# print(\"action:\", action)\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m cost_per_SKU \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(cost_per_SKU) \u001b[38;5;66;03m# negative because we want to minimize the cost\u001b[39;00m\n\u001b[1;32m     94\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# in this problem there is no termination condition\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/02_PhD/Other_python_projects/00_ddop_new/ddopnew/ddopnew/envs/inventory/single_period.py:131\u001b[0m, in \u001b[0;36mNewsvendorEnv.determine_cost\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03mDetermine the cost per SKU given the action taken. The cost is the sum of underage and overage costs.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Compute the cost per SKU\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpinball_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdemand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munderage_cost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverage_cost\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/02_PhD/Other_python_projects/00_ddop_new/ddopnew/ddopnew/loss_functions.py:39\u001b[0m, in \u001b[0;36mpinball_loss\u001b[0;34m(Y_true, Y_pred, underage_cost, overage_cost)\u001b[0m\n\u001b[1;32m     36\u001b[0m check_parameter_types(Y_true, Y_pred, underage_cost, overage_cost)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# assert shapes\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Y_true\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m Y_pred\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true and y_pred must have the same shape, but got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(Y_true\u001b[38;5;241m.\u001b[39mshape, Y_pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(Y_true \u001b[38;5;241m-\u001b[39m Y_pred, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m underage_cost \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(Y_pred \u001b[38;5;241m-\u001b[39m Y_true, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m overage_cost\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mAssertionError\u001b[0m: y_true and y_pred must have the same shape, but got (1,) and ()"
     ]
    }
   ],
   "source": [
    "from ddopnew.envs.inventory.single_period import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.agents.basic import RandomAgent\n",
    "\n",
    "\n",
    "val_index_start = 80 #90_000\n",
    "test_index_start = 90 #100_000\n",
    "\n",
    "X = np.random.rand(100, 2)\n",
    "Y = np.random.rand(100, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start)\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    ")\n",
    "\n",
    "agent = RandomAgent(environment.mdp_info)\n",
    "\n",
    "environment.test()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(f\"R: {R}, J: {J}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
