{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment functions\n",
    "\n",
    "> To be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union\n",
    "import logging\n",
    "from datetime import datetime  \n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.base import BaseAgent\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Think about how to handle mushroom integration.\n",
    "from mushroom_rl.core import Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class EarlyStoppingHandler():\n",
    "\n",
    "    '''\n",
    "    Class to handle early stopping\n",
    "\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        patience: int = 50,\n",
    "        warmup: int = 100,\n",
    "        criteria: str = \"J\",  # Whether to use discounted rewards J or total rewards R as criteria\n",
    "        direction: str = \"max\"  # Whether reward shall be maximized or minimized\n",
    "    ):\n",
    "\n",
    "        self.history = list()\n",
    "        self.patience = patience\n",
    "        if warmup is None or warmup < patience * 2:\n",
    "            warmup = patience * 2\n",
    "        self.warmup = warmup\n",
    "        self.criteria = criteria\n",
    "        self.direction = direction\n",
    "\n",
    "    def add_result(self, J, R):\n",
    "        if self.criteria == \"J\":\n",
    "            self.history.append(J)\n",
    "        elif self.criteria == \"R\":\n",
    "            self.history.append(R)\n",
    "        else:\n",
    "            raise ValueError(\"Criteria must be J or R\")\n",
    "        \n",
    "        if len(self.history) >= self.warmup:\n",
    "            if self.direction == \"max\":\n",
    "                if sum(self.history[-self.patience*2:-self.patience]) >= sum(self.history[-self.patience:]):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            elif self.direction == \"min\":\n",
    "                if sum(self.history[-self.patience*2:-self.patience]) <= sum(self.history[-self.patience:]):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                raise ValueError(\"Direction must be max or min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "* Some functions that are needed to run an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calculate_score(dataset, env):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    XXX\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    R = sum([row[0][2] for row in dataset])\n",
    "    gamma = env.mdp_info.gamma\n",
    "    J = sum([gamma**(t) * row[0][2] for t, row in enumerate(dataset)]) # Note: t starts at 1 so the first reward is already discounted\n",
    "\n",
    "    return R, J\n",
    "\n",
    "def log_info(R, J, n_epochs, logging, mode):\n",
    "    \n",
    "    '''\n",
    "    Logs the R, J information repeatedly for n_epoochs.\n",
    "    E.g., to draw a straight line in wandb for algorithmes\n",
    "    such as XGB, RF, etc. that can be comparared to the learning\n",
    "    curves of supervised or reinforcement learning algorithms.\n",
    "    '''\n",
    "\n",
    "    if logging == \"wandb\":\n",
    "        for epoch in range(n_epochs):\n",
    "            wandb.log({f\"{mode}/R\": R, f\"{mode}/J\": J})\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def update_best(R, J, best_R, best_J):\n",
    "    if R > best_R:\n",
    "        best_R = R\n",
    "    if J > best_J:\n",
    "        best_J = J\n",
    "\n",
    "    return best_R, best_J\n",
    "\n",
    "def save_agent(agent, experiment_dir, save_best, R, J, best_R, best_J, criteria=\"J\"):\n",
    "    if save_best:\n",
    "        if criteria == \"R\":\n",
    "            if R == best_R:\n",
    "                save_dir = f\"{experiment_dir}/saved_models/best\"\n",
    "                agent.save(save_dir)\n",
    "        elif criteria == \"J\":\n",
    "            if J == best_J:\n",
    "                save_dir = f\"{experiment_dir}/saved_models/best\"\n",
    "                agent.save(save_dir)\n",
    "\n",
    "def test_agent(agent: BaseAgent,\n",
    "            env: BaseEnvironment,\n",
    "            return_dataset = False,\n",
    "            tracking = None, # other: \"wandb\",\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Tests the agent on the environment for a single episode\n",
    "\n",
    "    # OPEN TODO: Make possible to save dataset via wandb\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the test episode\n",
    "    dataset = run_test_episode(env, agent)\n",
    "\n",
    "    # Calculate the score\n",
    "    R, J = calculate_score(dataset, env)\n",
    "\n",
    "    if tracking == \"wandb\":\n",
    "        mode = env.mode\n",
    "        wandb.log({f\"{mode}/R\": R, f\"{mode}/J\": J})\n",
    "\n",
    "    if return_dataset:\n",
    "        return R, J, dataset\n",
    "    else:\n",
    "        return R, J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def run_test_episode(   env: BaseEnvironment,\n",
    "                        agent: BaseAgent,\n",
    "                ):\n",
    "\n",
    "    \"\"\"\n",
    "    Runs and episode to test the agent's performance.\n",
    "    It assumes, that agent and environment are initialized, in test/val mode\n",
    "    and have done reset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get initial observation\n",
    "    obs, *_ = env.reset()\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    finished = False\n",
    "    step = 0\n",
    "    while not finished:\n",
    "        \n",
    "        # Sample action from agent\n",
    "        action = agent.draw_action(obs)\n",
    "\n",
    "        # Take a step in the environment\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        logging.debug(\"##### STEP: %d #####\", env.index)\n",
    "        logging.debug(\"reward: %s\", reward)\n",
    "        logging.debug(\"info: %s\", info)\n",
    "        logging.debug(\"next observation: %s\", obs)\n",
    "        logging.debug(\"truncated: %s\", truncated)\n",
    "\n",
    "        sample = (obs, action, reward, next_obs, terminated, truncated) # unlike mushroom do not include policy_state\n",
    "\n",
    "        obs = next_obs\n",
    "        \n",
    "        dataset.append((sample, info))\n",
    "\n",
    "        finished = terminated or truncated\n",
    "\n",
    "        step += 1\n",
    "        sys.stdout.write(f\"\\rStep {step}\")\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def run_experiment( agent: BaseAgent,\n",
    "                    env: BaseEnvironment,\n",
    "\n",
    "                    n_epochs: int,\n",
    "\n",
    "                    early_stopping_handler: Union[EarlyStoppingHandler, None] = None,\n",
    "                    save_best: bool = True,\n",
    "\n",
    "                    tracking: Union[str, None]  = None, # other: \"wandb\"\n",
    "\n",
    "                    results_dir: str = \"results\",\n",
    "\n",
    "                    run_id: Union[str, None] = None,\n",
    "\n",
    "                    logging_level =  logging.WARNING\n",
    "                ):\n",
    "\n",
    "    logging.basicConfig(level=logging_level)\n",
    "\n",
    "    # use start_time as id if no run_id is given\n",
    "    if run_id is None:\n",
    "        run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "\n",
    "    experiment_dir = f\"{results_dir}/{run_id}\"\n",
    "\n",
    "    logging.info(\"Starting experiment\")\n",
    "\n",
    "    env.reset()\n",
    "    env.train()\n",
    "    agent.train()\n",
    "\n",
    "    core = Core(agent, env)\n",
    "\n",
    "    # initial evaluation\n",
    "    env.val()\n",
    "    agent.eval()\n",
    "    R, J = test_agent(agent, env, tracking = tracking)\n",
    "\n",
    "    env.train()\n",
    "    agent.train()\n",
    "\n",
    "    logging.info(f\"Initial evaluation: R={R}, J={J}\")\n",
    "\n",
    "    best_J = J \n",
    "    best_R = R\n",
    "\n",
    "    if agent.train_mode == \"direct_fit\":\n",
    "        \n",
    "        logging.info(\"Starting training with direct fit\")\n",
    "        agent.fit(X=env.dataloader.get_all_X(), Y=env.dataloader.get_all_Y())\n",
    "        logging.info(\"Finished training with direct fit\")\n",
    "\n",
    "        env.val()\n",
    "        agent.eval()\n",
    "\n",
    "        R, J = test_agent(agent, env, tracking = tracking)\n",
    "        best_J, best_R = update_best(R, J, best_R, best_J)\n",
    "\n",
    "        log_info(R, J, n_epochs-1, logging, \"val\")\n",
    "\n",
    "    elif agent.train_mode == \"epochs_fit\":\n",
    "        \n",
    "        logging.info(\"Starting training with epochs fit\")\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            agent.fit_epoch(X=env.dataloader.get_X_train(), Y=env.dataloader.get_Y_train())\n",
    "\n",
    "            env.val()\n",
    "            agent.eval()\n",
    "\n",
    "            R, J = test_agent(agent, env, tracking = tracking)\n",
    "            \n",
    "            best_J, best_R = update_best(R, J, best_R, best_J)\n",
    "            save_agent(agent, experiment_dir, save_best, R, J, best_R, best_J, performance_criteria)\n",
    "            if early_stopping_handler is not None:\n",
    "                stop = early_stopping_handler(J, R)\n",
    "\n",
    "            if stop:\n",
    "                logging.info(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "            env.train()\n",
    "            agent.train()\n",
    "\n",
    "        logging.info(\"Finished training with epochs fit\")\n",
    "\n",
    "    elif agent.train_mode == \"env_interaction\":\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown train mode\")\n",
    "\n",
    "    logging.info(f\"Evaluation after training: R={R}, J={J}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10\n",
      "R: -6.604986797188729, J: -6.578556148664966\n"
     ]
    }
   ],
   "source": [
    "from ddopnew.envs.inventory import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.agents.basic import RandomAgent\n",
    "\n",
    "\n",
    "val_index_start = 80 #90_000\n",
    "test_index_start = 90 #100_000\n",
    "\n",
    "X = np.random.rand(100, 2)\n",
    "Y = np.random.rand(100, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start)\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    ")\n",
    "\n",
    "agent = RandomAgent(environment.mdp_info)\n",
    "\n",
    "environment.test()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(f\"R: {R}, J: {J}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
