{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERM agents\n",
    "\n",
    "> Newsvendor agents based on Empirical Risk Minimization (ERM) principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.newsvendor.erm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional, List, Tuple, Literal\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython import get_ipython\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.base import BaseAgent\n",
    "from ddopnew.utils import MDPInfo, Parameter, DatasetWrapper, DatasetWrapperMeta\n",
    "from ddopnew.torch_utils.loss_functions import TorchQuantileLoss, TorchPinballLoss\n",
    "from ddopnew.torch_utils.obsprocessors import FlattenTimeDim\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SGDBaseAgent(BaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Base class for Agents that are trained using Stochastic Gradient Descent (SGD) on PyTorch models.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Remove input shapes as input end get from MDPInfo\n",
    "\n",
    "    train_mode = \"epochs_fit\"\n",
    "    \n",
    "    def __init__(self, \n",
    "            environment_info: MDPInfo,\n",
    "            dataloader: BaseDataLoader,\n",
    "            input_shape: Tuple,\n",
    "            output_shape: Tuple,\n",
    "            optimizer_params: Optional[dict] = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "            learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "            dataloader_params: Optional[dict] = None, # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "            obsprocessors: Optional[List] = None,     # default: []\n",
    "            torch_obsprocessors: Optional[List] = None,     # default: []\n",
    "            device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "            agent_name: str | None = None,\n",
    "            test_batch_size: int = 1024,\n",
    "            receive_batch_dim: bool = False,\n",
    "            ):\n",
    "\n",
    "        # Initialize default values for mutable arguments\n",
    "        optimizer_params = optimizer_params or {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "        dataloader_params = dataloader_params or {\"batch_size\": 32, \"shuffle\": True}\n",
    "        self.torch_obsprocessors = torch_obsprocessors or []\n",
    "\n",
    "        self.device = self.set_device(device)\n",
    "        \n",
    "        self.set_dataloader(dataloader, dataloader_params)\n",
    "\n",
    "        self.set_model(input_shape, output_shape)\n",
    "        self.loss_function_params=None # default\n",
    "        self.set_loss_function()\n",
    "        self.set_optimizer(optimizer_params)\n",
    "        self.set_learning_rate_scheduler(learning_rate_scheduler)\n",
    "        self.test_batch_size = test_batch_size\n",
    "\n",
    "        super().__init__(environment_info = environment_info, obsprocessors = obsprocessors, agent_name = agent_name, receive_batch_dim = receive_batch_dim)\n",
    "\n",
    "        batch_dim = 1\n",
    "        logging.info(\"Network architecture:\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "\n",
    "            self.model.eval()\n",
    "            if any(isinstance(obsprocessor, FlattenTimeDim) for obsprocessor in self.torch_obsprocessors):\n",
    "                input_size = (batch_dim, int(np.prod(input_shape)))\n",
    "            else:\n",
    "                input_size = (batch_dim, *input_shape)\n",
    "\n",
    "            input_tensor = torch.randn(batch_dim, *input_size).to(self.device)\n",
    "            input_tuple = (input_tensor,)\n",
    "            if get_ipython() is not None:\n",
    "                print(summary(self.model, input_data=input_tuple, device=self.device))\n",
    "            else:\n",
    "                summary(self.model, input_data=input_tuple, device=self.device)\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def set_device(self, device: str):\n",
    "\n",
    "        \"\"\" Set the device for the model \"\"\"\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            if torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "            else:\n",
    "                logging.warning(\"CUDA is not available. Using CPU instead.\")\n",
    "                return \"cpu\"\n",
    "        elif device == \"cpu\":\n",
    "            return \"cpu\"\n",
    "        else:\n",
    "            raise ValueError(f\"Device {device} not currently not supported, use 'cuda' or 'cpu'\")\n",
    "\n",
    "\n",
    "    def set_dataloader(self,\n",
    "                        dataloader: BaseDataLoader,\n",
    "                        dataloader_params: dict, # dict with keys: batch_size, shuffle\n",
    "                        ) -> None: \n",
    "\n",
    "        \"\"\"\n",
    "        Set the dataloader for the agent by wrapping it into a Torch Dataset\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # check if class already have a dataloader\n",
    "        if not hasattr(self, 'dataloader'):\n",
    "\n",
    "            dataset = DatasetWrapper(dataloader)\n",
    "            self.dataloader = torch.utils.data.DataLoader(dataset, **dataloader_params)\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_loss_function(self):\n",
    "        \"\"\" Set loss function for the model \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_model(self, input_shape: Tuple, output_shape: Tuple):\n",
    "        \"\"\" Set the model for the agent \"\"\"\n",
    "        pass\n",
    "\n",
    "    def set_optimizer(self, optimizer_params: dict): # dict with keys: optimizer, lr, weight_decay\n",
    "        \n",
    "        \"\"\" Set the optimizer for the model \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'optimizer'):\n",
    "            \n",
    "            optimizer = optimizer_params[\"optimizer\"]\n",
    "            optimizer_params_copy = optimizer_params.copy()\n",
    "            del optimizer_params_copy[\"optimizer\"]\n",
    "\n",
    "            if optimizer == \"Adam\":\n",
    "                self.optimizer = torch.optim.Adam(self.model.parameters(), **optimizer_params_copy)\n",
    "            elif optimizer == \"SGD\":\n",
    "                self.optimizer = torch.optim.SGD(self.model.parameters(), **optimizer_params_copy)\n",
    "            elif optimizer == \"RMSprop\":\n",
    "                self.optimizer = torch.optim.RMSprop(self.model.parameters(), **optimizer_params_copy)\n",
    "            else:\n",
    "                raise ValueError(f\"Optimizer {optimizer} not supported\")\n",
    "        \n",
    "    def set_learning_rate_scheduler(self, learning_rate_scheduler: None = None): #\n",
    "        \"\"\" Set learning rate scheudler (can be None) \"\"\"\n",
    "        if learning_rate_scheduler is not None:\n",
    "            raise NotImplementedError(\"Learning rate scheduler not implemented yet\")\n",
    "        else:\n",
    "            self.learning_rate_scheduler = None\n",
    "\n",
    "    def fit_epoch(self):\n",
    "\n",
    "        \"\"\" Fit the model for one epoch using the dataloader \"\"\"\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        self.model.train()\n",
    "        total_loss=0\n",
    "\n",
    "        for i, output in enumerate(tqdm(self.dataloader)):\n",
    "            \n",
    "            if len(output)==3:\n",
    "                X, y, loss_function_params = output\n",
    "            else:\n",
    "                X, y = output\n",
    "                loss_function_params = None\n",
    "\n",
    "            # convert X and y to float32\n",
    "            X = X.type(torch.float32)\n",
    "            y = y.type(torch.float32)\n",
    "\n",
    "            for torch_obsprocessor in self.torch_obsprocessors:\n",
    "                X = torch_obsprocessor(X)\n",
    "            \n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            y_pred = self.model(X)\n",
    "\n",
    "            if loss_function_params is not None:\n",
    "                loss = self.loss_function(y_pred, y, **loss_function_params)\n",
    "            elif self.loss_function_params is not None:\n",
    "                loss = self.loss_function(y_pred, y, **self.loss_function_params)\n",
    "            else:\n",
    "                loss = self.loss_function(y_pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def draw_action_(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \n",
    "        \"\"\" \n",
    "        Draw an action based on the fitted model (see predict method)\n",
    "        \"\"\"\n",
    "        \n",
    "        action = self.predict(observation)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_into_batches(X: np.ndarray, batch_size: int) -> List[np.ndarray]: #\n",
    "        \"\"\" Split the input into batches of the specified size \"\"\"\n",
    "        return [X[i:i+batch_size] for i in range(0, len(X), batch_size)]\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray: #\n",
    "        \"\"\" Do one forward pass of the model and return the prediction \"\"\"\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        self.model.eval()\n",
    "\n",
    "        batches = self.split_into_batches(X, self.test_batch_size)\n",
    "\n",
    "        y_pred_full = []\n",
    "        for batch in batches:\n",
    "            X = torch.tensor(X, dtype=torch.float32)\n",
    "            for torch_obsprocessor in self.torch_obsprocessors:\n",
    "                X = torch_obsprocessor(X)\n",
    "            X = X.to(device)\n",
    "\n",
    "            torch.set_printoptions(sci_mode=False)\n",
    "            print(X)\n",
    "            assert False\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                y_pred = self.model(X)\n",
    "\n",
    "            y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "            y_pred_full.append(y_pred)\n",
    "        \n",
    "        y_pred_full = np.concatenate(y_pred_full, axis=0)\n",
    "\n",
    "        return y_pred_full\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"set the internal state of the agent and its model to train\"\"\"\n",
    "        self.mode = \"train\"\n",
    "        self.model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"set the internal state of the agent and its model to eval\"\"\"\n",
    "        self.mode = \"eval\"\n",
    "        self.model.eval()\n",
    "\n",
    "    def to(self, device: str): #\n",
    "        \"\"\"Move the model to the specified device\"\"\"\n",
    "        self.model.to(device)\n",
    "\n",
    "    def save(self,\n",
    "                path: str, # The directory where the file will be saved.\n",
    "                overwrite: bool=True): # Allow overwriting; if False, a FileExistsError will be raised if the file exists.\n",
    "        \n",
    "        \"\"\"\n",
    "        Save the PyTorch model to a file in the specified directory.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            raise AttributeError(\"Model is not defined in the class.\")\n",
    "\n",
    "        # Create the directory path if it does not exist\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Construct the file path using os.path.join for better cross-platform compatibility\n",
    "        full_path = os.path.join(path, \"model.pth\")\n",
    "\n",
    "        if os.path.exists(full_path):\n",
    "            if not overwrite:\n",
    "                raise FileExistsError(f\"The file {full_path} already exists and will not be overwritten.\")\n",
    "            else:\n",
    "                logging.debug(f\"Overwriting file {full_path}\") # Only log with info as during training we will continuously overwrite the model\n",
    "        \n",
    "        # Save the model's state_dict using torch.save\n",
    "        torch.save(self.model.state_dict(), full_path)\n",
    "        logging.debug(f\"Model saved successfully to {full_path}\")\n",
    "\n",
    "    def load(self, path: str): # Only the path to the folder is needed, not the file itself\n",
    " \n",
    "        \"\"\"\n",
    "        Load the PyTorch model from a file.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            raise AttributeError(\"Model is not defined in the class.\")\n",
    "\n",
    "        # Construct the file path\n",
    "        full_path = os.path.join(path, \"model.pth\")\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            raise FileNotFoundError(f\"The file {full_path} does not exist.\")\n",
    "\n",
    "        try:\n",
    "            # Load the model's state_dict using torch.load\n",
    "            self.model.load_state_dict(torch.load(full_path))\n",
    "            logging.debug(f\"Model loaded successfully from {full_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred while loading the model: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L30){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## SGDBaseAgent\n",
       "\n",
       ">      SGDBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                    dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                    input_shape:Tuple, output_shape:Tuple,\n",
       ">                    optimizer_params:Optional[dict]=None,\n",
       ">                    learning_rate_scheduler=None,\n",
       ">                    dataloader_params:Optional[dict]=None,\n",
       ">                    obsprocessors:Optional[List]=None,\n",
       ">                    torch_obsprocessors:Optional[List]=None, device:str='cpu',\n",
       ">                    agent_name:str|None=None, test_batch_size:int=1024,\n",
       ">                    receive_batch_dim:bool=False)\n",
       "\n",
       "*Base class for Agents that are trained using Stochastic Gradient Descent (SGD) on PyTorch models.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| optimizer_params | Optional | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| dataloader_params | Optional | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| obsprocessors | Optional | None | default: [] |\n",
       "| torch_obsprocessors | Optional | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L30){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## SGDBaseAgent\n",
       "\n",
       ">      SGDBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                    dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                    input_shape:Tuple, output_shape:Tuple,\n",
       ">                    optimizer_params:Optional[dict]=None,\n",
       ">                    learning_rate_scheduler=None,\n",
       ">                    dataloader_params:Optional[dict]=None,\n",
       ">                    obsprocessors:Optional[List]=None,\n",
       ">                    torch_obsprocessors:Optional[List]=None, device:str='cpu',\n",
       ">                    agent_name:str|None=None, test_batch_size:int=1024,\n",
       ">                    receive_batch_dim:bool=False)\n",
       "\n",
       "*Base class for Agents that are trained using Stochastic Gradient Descent (SGD) on PyTorch models.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| optimizer_params | Optional | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| dataloader_params | Optional | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| obsprocessors | Optional | None | default: [] |\n",
       "| torch_obsprocessors | Optional | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important notes:\n",
    "\n",
    "SGD-based agents are all agents that are trained via SGD such as Linear Models or Neural Networks. Some specific requirements are necessary to make them interface properly with the environment.\n",
    "\n",
    "**Torch perprocessors**:\n",
    "\n",
    "* In addition to the general Numpy-based pre-processor, we also provide pre-processors that work on tensor level within the ```fit_epoch``` method and the ```predict``` method. They can be used in addition to the numpy-based pre-processors or instead of them. It's important to ensure that the shape of observations (after pre-processing) is the same for those from the environemnt and those from the dataloader during training.\n",
    "\n",
    "**Dataloader**:\n",
    "\n",
    "* As for normal supervised learning via Torch, we make use of the Torch dataloader to load the data. Instead of defining a custom dataset class, we provide a Wrapper that can be used around our dataloader to make its output and interface the same as a Torch dataset. The dataloader is then initialized when the agent is created such that the agent has access to the same dataloader as the environment.\n",
    " \n",
    "**Training process**:\n",
    "\n",
    "* The outper loop of the training process (epochs) is handled outside the agent by the ```run_experiment```functions (or can also be customized). The agent needs to have a ```fit_epoch``` method that tells the agent what to do within an epoch. \n",
    "This includes:\n",
    "    * Getting the data from the dataloader\n",
    "    * Pre-processing the data\n",
    "    * Forward pass\n",
    "    * Loss calculation\n",
    "    * Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L110){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_dataloader\n",
       "\n",
       ">      SGDBaseAgent.set_dataloader\n",
       ">                                   (dataloader:ddopnew.dataloaders.base.BaseDat\n",
       ">                                   aLoader, dataloader_params:dict)\n",
       "\n",
       "*Set the dataloader for the agent by wrapping it into a Torch Dataset*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dataloader | BaseDataLoader |  |\n",
       "| dataloader_params | dict | dict with keys: batch_size, shuffle |\n",
       "| **Returns** | **None** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L110){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_dataloader\n",
       "\n",
       ">      SGDBaseAgent.set_dataloader\n",
       ">                                   (dataloader:ddopnew.dataloaders.base.BaseDat\n",
       ">                                   aLoader, dataloader_params:dict)\n",
       "\n",
       "*Set the dataloader for the agent by wrapping it into a Torch Dataset*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dataloader | BaseDataLoader |  |\n",
       "| dataloader_params | dict | dict with keys: batch_size, shuffle |\n",
       "| **Returns** | **None** |  |"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.set_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_loss_function\n",
       "\n",
       ">      SGDBaseAgent.set_loss_function ()\n",
       "\n",
       "*Set loss function for the model*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_loss_function\n",
       "\n",
       ">      SGDBaseAgent.set_loss_function ()\n",
       "\n",
       "*Set loss function for the model*"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.set_loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L132){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_model\n",
       "\n",
       ">      SGDBaseAgent.set_model (input_shape:Tuple, output_shape:Tuple)\n",
       "\n",
       "*Set the model for the agent*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L132){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_model\n",
       "\n",
       ">      SGDBaseAgent.set_model (input_shape:Tuple, output_shape:Tuple)\n",
       "\n",
       "*Set the model for the agent*"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.set_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L136){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_optimizer\n",
       "\n",
       ">      SGDBaseAgent.set_optimizer (optimizer_params:dict)\n",
       "\n",
       "*Set the optimizer for the model*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| optimizer_params | dict | dict with keys: optimizer, lr, weight_decay |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L136){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_optimizer\n",
       "\n",
       ">      SGDBaseAgent.set_optimizer (optimizer_params:dict)\n",
       "\n",
       "*Set the optimizer for the model*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| optimizer_params | dict | dict with keys: optimizer, lr, weight_decay |"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.set_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L155){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_learning_rate_scheduler\n",
       "\n",
       ">      SGDBaseAgent.set_learning_rate_scheduler\n",
       ">                                                (learning_rate_scheduler:None=N\n",
       ">                                                one)\n",
       "\n",
       "*Set learning rate scheudler (can be None)*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| learning_rate_scheduler | None | None |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L155){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_learning_rate_scheduler\n",
       "\n",
       ">      SGDBaseAgent.set_learning_rate_scheduler\n",
       ">                                                (learning_rate_scheduler:None=N\n",
       ">                                                one)\n",
       "\n",
       "*Set learning rate scheudler (can be None)*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| learning_rate_scheduler | None | None |  |"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.set_learning_rate_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L162){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.fit_epoch\n",
       "\n",
       ">      SGDBaseAgent.fit_epoch ()\n",
       "\n",
       "*Fit the model for one epoch using the dataloader*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L162){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.fit_epoch\n",
       "\n",
       ">      SGDBaseAgent.fit_epoch ()\n",
       "\n",
       "*Fit the model for one epoch using the dataloader*"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.fit_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L207){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.draw_action_\n",
       "\n",
       ">      SGDBaseAgent.draw_action_ (observation:numpy.ndarray)\n",
       "\n",
       "*Draw an action based on the fitted model (see predict method)*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| observation | ndarray |  |\n",
       "| **Returns** | **ndarray** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L207){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.draw_action_\n",
       "\n",
       ">      SGDBaseAgent.draw_action_ (observation:numpy.ndarray)\n",
       "\n",
       "*Draw an action based on the fitted model (see predict method)*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| observation | ndarray |  |\n",
       "| **Returns** | **ndarray** |  |"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.draw_action_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L222){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.predict\n",
       "\n",
       ">      SGDBaseAgent.predict (X:numpy.ndarray)\n",
       "\n",
       "*Do one forward pass of the model and return the prediction*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | ndarray |  |\n",
       "| **Returns** | **ndarray** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L222){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.predict\n",
       "\n",
       ">      SGDBaseAgent.predict (X:numpy.ndarray)\n",
       "\n",
       "*Do one forward pass of the model and return the prediction*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | ndarray |  |\n",
       "| **Returns** | **ndarray** |  |"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L253){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.train\n",
       "\n",
       ">      SGDBaseAgent.train ()\n",
       "\n",
       "*set the internal state of the agent and its model to train*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L253){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.train\n",
       "\n",
       ">      SGDBaseAgent.train ()\n",
       "\n",
       "*set the internal state of the agent and its model to train*"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L258){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.eval\n",
       "\n",
       ">      SGDBaseAgent.eval ()\n",
       "\n",
       "*set the internal state of the agent and its model to eval*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L258){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.eval\n",
       "\n",
       ">      SGDBaseAgent.eval ()\n",
       "\n",
       "*set the internal state of the agent and its model to eval*"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L263){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.to\n",
       "\n",
       ">      SGDBaseAgent.to (device:str)\n",
       "\n",
       "*Move the model to the specified device*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| device | str |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L263){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.to\n",
       "\n",
       ">      SGDBaseAgent.to (device:str)\n",
       "\n",
       "*Move the model to the specified device*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| device | str |  |"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L267){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.save\n",
       "\n",
       ">      SGDBaseAgent.save (path:str, overwrite:bool=True)\n",
       "\n",
       "*Save the PyTorch model to a file in the specified directory.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| path | str |  | The directory where the file will be saved. |\n",
       "| overwrite | bool | True | Allow overwriting; if False, a FileExistsError will be raised if the file exists. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L267){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.save\n",
       "\n",
       ">      SGDBaseAgent.save (path:str, overwrite:bool=True)\n",
       "\n",
       "*Save the PyTorch model to a file in the specified directory.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| path | str |  | The directory where the file will be saved. |\n",
       "| overwrite | bool | True | Allow overwriting; if False, a FileExistsError will be raised if the file exists. |"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L295){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.load\n",
       "\n",
       ">      SGDBaseAgent.load (path:str)\n",
       "\n",
       "*Load the PyTorch model from a file.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str | Only the path to the folder is needed, not the file itself |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L295){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.load\n",
       "\n",
       ">      SGDBaseAgent.load (path:str)\n",
       "\n",
       "*Load the PyTorch model from a file.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str | Only the path to the folder is needed, not the file itself |"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NVBaseAgent(SGDBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Base agent for the Newsvendor problem implementing\n",
    "    the loss function for the Empirical Risk Minimization (ERM) approach\n",
    "    based on quantile loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                obsprocessors: list | None = None,      # default: []\n",
    "                torch_obsprocessors: list | None = None,  # default: []\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = None,\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\", \n",
    "                ):\n",
    "\n",
    "        cu = self.convert_to_numpy_array(cu)\n",
    "        co = self.convert_to_numpy_array(co)\n",
    "        \n",
    "        self.sl = cu / (cu + co) # ensure this works if cu and co are Parameters\n",
    "        self.cu = cu\n",
    "        self.co = co\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            optimizer_params=optimizer_params,\n",
    "            learning_rate_scheduler=learning_rate_scheduler,\n",
    "            dataloader_params=dataloader_params,\n",
    "            obsprocessors=obsprocessors,\n",
    "            torch_obsprocessors=torch_obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "        )   \n",
    "        \n",
    "    def set_loss_function(self):\n",
    "        \n",
    "        \"\"\"Set the loss function for the model to the quantile loss. For training\n",
    "        the model uses quantile loss and not the pinball loss with specific cu and \n",
    "        co values to ensure similar scale of the feedback signal during training.\"\"\"\n",
    "\n",
    "        if self.loss_function == \"quantile\":\n",
    "            self.loss_function_params = {\"quantile\": self.sl}\n",
    "            self.loss_function = TorchQuantileLoss(reduction=\"mean\")\n",
    "            logging.debug(f\"Loss function set to {self.loss_function}\")\n",
    "\n",
    "        elif self.loss_function == \"pinball\":\n",
    "            self.loss_function_params = {\"underage\": self.cu, \"overage\": self.co}\n",
    "            self.loss_function = TorchPinballLoss(reduction=\"mean\")\n",
    "            logging.debug(f\"Loss function set to {self.loss_function}\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Loss function {self.loss_function} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L319){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NVBaseAgent\n",
       "\n",
       ">      NVBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                   dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                   cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                   co:numpy.ndarray|ddopnew.utils.Parameter, input_shape:Tuple,\n",
       ">                   output_shape:Tuple, optimizer_params:dict|None=None,\n",
       ">                   learning_rate_scheduler=None,\n",
       ">                   dataloader_params:dict|None=None,\n",
       ">                   obsprocessors:list|None=None,\n",
       ">                   torch_obsprocessors:list|None=None, device:str='cpu',\n",
       ">                   agent_name:str|None=None, test_batch_size:int=1024,\n",
       ">                   receive_batch_dim:bool=False,\n",
       ">                   loss_function:Literal['quantile','pinball']='quantile')\n",
       "\n",
       "*Base agent for the Newsvendor problem implementing\n",
       "the loss function for the Empirical Risk Minimization (ERM) approach\n",
       "based on quantile loss.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| torch_obsprocessors | list \\| None | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L319){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NVBaseAgent\n",
       "\n",
       ">      NVBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                   dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                   cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                   co:numpy.ndarray|ddopnew.utils.Parameter, input_shape:Tuple,\n",
       ">                   output_shape:Tuple, optimizer_params:dict|None=None,\n",
       ">                   learning_rate_scheduler=None,\n",
       ">                   dataloader_params:dict|None=None,\n",
       ">                   obsprocessors:list|None=None,\n",
       ">                   torch_obsprocessors:list|None=None, device:str='cpu',\n",
       ">                   agent_name:str|None=None, test_batch_size:int=1024,\n",
       ">                   receive_batch_dim:bool=False,\n",
       ">                   loss_function:Literal['quantile','pinball']='quantile')\n",
       "\n",
       "*Base agent for the Newsvendor problem implementing\n",
       "the loss function for the Empirical Risk Minimization (ERM) approach\n",
       "based on quantile loss.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| torch_obsprocessors | list \\| None | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NVBaseAgent, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L372){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NVBaseAgent.set_loss_function\n",
       "\n",
       ">      NVBaseAgent.set_loss_function ()\n",
       "\n",
       "*Set the loss function for the model to the quantile loss. For training\n",
       "the model uses quantile loss and not the pinball loss with specific cu and \n",
       "co values to ensure similar scale of the feedback signal during training.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L372){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NVBaseAgent.set_loss_function\n",
       "\n",
       ">      NVBaseAgent.set_loss_function ()\n",
       "\n",
       "*Set the loss function for the model to the quantile loss. For training\n",
       "the model uses quantile loss and not the pinball loss with specific cu and \n",
       "co values to ensure similar scale of the feedback signal during training.*"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NVBaseAgent.set_loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorlERMAgent(NVBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
    "    based on a linear (regression) model. Note that this implementation finds\n",
    "    the optimal regression parameters via SGD.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "                model_params: dict | None = None,  # default: {\"relu_output\": False}\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                torch_obsprocessors: list | None = None,  # default: [FlattenTimeDim(allow_2d=False)]\n",
    "                device: str = \"cpu\",  # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = \"lERM\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\", \n",
    "                ):\n",
    "\n",
    "        # Handle mutable defaults unique to this class\n",
    "        default_model_params = {\n",
    "            \"relu_output\": False\n",
    "            }\n",
    "\n",
    "        self.model_params = self.update_model_params(default_model_params, model_params or {})\n",
    "\n",
    "        # By default automatically flatten the time dimension of data, if it is not already 2D\n",
    "        torch_obsprocessors = [FlattenTimeDim(allow_2d=True)] if torch_obsprocessors is None else torch_obsprocessors\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            optimizer_params=optimizer_params,\n",
    "            learning_rate_scheduler=learning_rate_scheduler,\n",
    "            dataloader_params=dataloader_params,\n",
    "            obsprocessors=obsprocessors,\n",
    "            torch_obsprocessors=torch_obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )\n",
    "    def set_model(self, input_shape, output_shape):\n",
    "\n",
    "        \"\"\"Set the model for the agent to a linear model\"\"\"\n",
    "\n",
    "        from ddopnew.approximators import LinearModel\n",
    "\n",
    "        # flatten time dim of input\n",
    "        input_size = np.prod(input_shape)\n",
    "        output_size = output_shape[0]\n",
    "\n",
    "        self.model = LinearModel(input_size=input_size, output_size=output_size, **self.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L392){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NewsvendorlERMAgent\n",
       "\n",
       ">      NewsvendorlERMAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                           dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                           cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                           co:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                           input_shape:Tuple, output_shape:Tuple,\n",
       ">                           optimizer_params:dict|None=None,\n",
       ">                           learning_rate_scheduler=None,\n",
       ">                           model_params:dict|None=None,\n",
       ">                           dataloader_params:dict|None=None,\n",
       ">                           obsprocessors:list|None=None,\n",
       ">                           torch_obsprocessors:list|None=None,\n",
       ">                           device:str='cpu', agent_name:str|None='lERM',\n",
       ">                           test_batch_size:int=1024,\n",
       ">                           receive_batch_dim:bool=False, loss_function:Literal[\n",
       ">                           'quantile','pinball']='quantile')\n",
       "\n",
       "*Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
       "based on a linear (regression) model. Note that this implementation finds\n",
       "the optimal regression parameters via SGD.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| model_params | dict \\| None | None | default: {\"relu_output\": False} |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| torch_obsprocessors | list \\| None | None | default: [FlattenTimeDim(allow_2d=False)] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | lERM |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L392){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NewsvendorlERMAgent\n",
       "\n",
       ">      NewsvendorlERMAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                           dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                           cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                           co:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                           input_shape:Tuple, output_shape:Tuple,\n",
       ">                           optimizer_params:dict|None=None,\n",
       ">                           learning_rate_scheduler=None,\n",
       ">                           model_params:dict|None=None,\n",
       ">                           dataloader_params:dict|None=None,\n",
       ">                           obsprocessors:list|None=None,\n",
       ">                           torch_obsprocessors:list|None=None,\n",
       ">                           device:str='cpu', agent_name:str|None='lERM',\n",
       ">                           test_batch_size:int=1024,\n",
       ">                           receive_batch_dim:bool=False, loss_function:Literal[\n",
       ">                           'quantile','pinball']='quantile')\n",
       "\n",
       "*Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
       "based on a linear (regression) model. Note that this implementation finds\n",
       "the optimal regression parameters via SGD.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| model_params | dict \\| None | None | default: {\"relu_output\": False} |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| torch_obsprocessors | list \\| None | None | default: [FlattenTimeDim(allow_2d=False)] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | lERM |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NewsvendorlERMAgent, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further information:   \n",
    "   \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    .. [1] Gah-Yi Ban, Cynthia Rudin, \"The Big Data Newsvendor: Practical Insights\n",
    "        from Machine Learning\", 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L449){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NewsvendorlERMAgent.set_model\n",
       "\n",
       ">      NewsvendorlERMAgent.set_model (input_shape, output_shape)\n",
       "\n",
       "*Set the model for the agent to a linear model*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L449){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NewsvendorlERMAgent.set_model\n",
       "\n",
       ">      NewsvendorlERMAgent.set_model (input_shape, output_shape)\n",
       "\n",
       "*Set the model for the agent to a linear model*"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NewsvendorlERMAgent.set_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Network architecture:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LinearModel                              [1, 1, 1]                 --\n",
      "├─Linear: 1-1                            [1, 1, 1]                 3\n",
      "├─Identity: 1-2                          [1, 1, 1]                 --\n",
      "==========================================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "==========================================================================================\n",
      "tensor([[0.1306, 0.6514]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m environment\u001b[38;5;241m.\u001b[39mtest()\n\u001b[1;32m     36\u001b[0m agent\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 38\u001b[0m R, J \u001b[38;5;241m=\u001b[39m \u001b[43mtest_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(R, J)\n\u001b[1;32m     42\u001b[0m run_experiment(agent, environment, \u001b[38;5;241m2\u001b[39m, run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# fit agent via run_experiment function\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/02_PhD/Other_python_projects/00_ddop_new/ddopnew/ddopnew/experiment_functions.py:177\u001b[0m, in \u001b[0;36mtest_agent\u001b[0;34m(agent, env, return_dataset, tracking, eval_step_info)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mTests the agent on the environment for a single episode\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# TODO make it possible to save dataset via tracking tool\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Run the test episode\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mrun_test_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_step_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Calculate the score\u001b[39;00m\n\u001b[1;32m    180\u001b[0m R, J \u001b[38;5;241m=\u001b[39m calculate_score(dataset, env)\n",
      "File \u001b[0;32m~/Documents/02_PhD/Other_python_projects/00_ddop_new/ddopnew/ddopnew/experiment_functions.py:216\u001b[0m, in \u001b[0;36mrun_test_episode\u001b[0;34m(env, agent, eval_step_info)\u001b[0m\n\u001b[1;32m    211\u001b[0m horizon \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mmdp_info\u001b[38;5;241m.\u001b[39mhorizon\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m finished:\n\u001b[1;32m    214\u001b[0m     \n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# Sample action from agent\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# Take a step in the environment\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/Documents/02_PhD/Other_python_projects/00_ddop_new/ddopnew/ddopnew/agents/base.py:63\u001b[0m, in \u001b[0;36mBaseAgent.draw_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     60\u001b[0m         observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch_dim(observation)\n\u001b[1;32m     61\u001b[0m         batch_added \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_action_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "Cell \u001b[0;32mIn[48], line 186\u001b[0m, in \u001b[0;36mSGDBaseAgent.draw_action_\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_action_\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray: \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    Draw an action based on the fitted model (see predict method)\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "Cell \u001b[0;32mIn[48], line 212\u001b[0m, in \u001b[0;36mSGDBaseAgent.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    210\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_printoptions(sci_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    216\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ddopnew.envs.inventory.single_period import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.experiment_functions import run_experiment, test_agent\n",
    "\n",
    "val_index_start = 800 #90_000\n",
    "test_index_start = 900 #100_000\n",
    "\n",
    "X = np.random.rand(1000, 2)\n",
    "Y = np.random.rand(1000, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start)\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    ")\n",
    "\n",
    "agent = NewsvendorlERMAgent(environment.mdp_info,\n",
    "                            dataloader,\n",
    "                            cu=np.array([0.42857]),\n",
    "                            co=np.array([1.0]),\n",
    "                            input_shape=(2,),\n",
    "                            output_shape=(1,),\n",
    "                            optimizer_params= {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}, # other optimizers: \"SGD\", \"RMSprop\"\n",
    "                            learning_rate_scheduler = None, # TODO add base class for learning rate scheduler for typing\n",
    "                            model_params = {\"relu_output\": False}, #\n",
    "                            dataloader_params={\"batch_size\": 32, \"shuffle\": True},\n",
    "                            torch_obsprocessors = [],\n",
    "                            device = \"cpu\", # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "run_experiment(agent, environment, 2, run_id = \"test\") # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorDLAgent(NVBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
    "    based on a deep learning model. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "                \n",
    "                # parameters in yaml file\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                model_params: dict | None = None,  # default: {\"hidden_layers\": [64, 64], \"drop_prob\": 0.0, \"batch_norm\": False, \"relu_output\": False}\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                torch_obsprocessors: list | None = None,  # default: [FlattenTimeDim(allow_2d=False)]\n",
    "                agent_name: str | None = \"DLNV\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\",\n",
    "                ):\n",
    "\n",
    "        # Handle mutable defaults unique to this class\n",
    "        default_model_params = {\n",
    "            \"hidden_layers\": [64, 64],\n",
    "            \"drop_prob\": 0.0,\n",
    "            \"batch_norm\": False,\n",
    "            \"relu_output\": False\n",
    "            }\n",
    "\n",
    "        self.model_params = self.update_model_params(default_model_params, model_params or {})\n",
    "        \n",
    "        torch_obsprocessors = [FlattenTimeDim(allow_2d=True)] if torch_obsprocessors is None else torch_obsprocessors\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            optimizer_params=optimizer_params,\n",
    "            learning_rate_scheduler=learning_rate_scheduler,\n",
    "            dataloader_params=dataloader_params,\n",
    "            obsprocessors=obsprocessors,\n",
    "            torch_obsprocessors=torch_obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )\n",
    "        \n",
    "    def set_model(self, input_shape, output_shape):\n",
    "        \n",
    "        \"\"\"Set the model for the agent to an MLP\"\"\"\n",
    "\n",
    "\n",
    "        # flatten time dim of input\n",
    "        input_size = np.prod(input_shape)\n",
    "        output_size = output_shape[0]\n",
    "\n",
    "        from ddopnew.approximators import MLP\n",
    "        self.model = MLP(input_size=input_size, output_size=output_size, **self.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L462){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NewsvendorDLAgent\n",
       "\n",
       ">      NewsvendorDLAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                         dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                         cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                         co:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                         input_shape:Tuple, output_shape:Tuple,\n",
       ">                         learning_rate_scheduler=None,\n",
       ">                         optimizer_params:dict|None=None,\n",
       ">                         model_params:dict|None=None,\n",
       ">                         dataloader_params:dict|None=None, device:str='cpu',\n",
       ">                         obsprocessors:list|None=None,\n",
       ">                         torch_obsprocessors:list|None=None,\n",
       ">                         agent_name:str|None='DLNV', test_batch_size:int=1024,\n",
       ">                         receive_batch_dim:bool=False, loss_function:Literal['q\n",
       ">                         uantile','pinball']='quantile')\n",
       "\n",
       "*Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
       "based on a deep learning model.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| learning_rate_scheduler | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| model_params | dict \\| None | None | default: {\"hidden_layers\": [64, 64], \"drop_prob\": 0.0, \"batch_norm\": False, \"relu_output\": False} |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| torch_obsprocessors | list \\| None | None | default: [FlattenTimeDim(allow_2d=False)] |\n",
       "| agent_name | str \\| None | DLNV |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L462){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NewsvendorDLAgent\n",
       "\n",
       ">      NewsvendorDLAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                         dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                         cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                         co:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                         input_shape:Tuple, output_shape:Tuple,\n",
       ">                         learning_rate_scheduler=None,\n",
       ">                         optimizer_params:dict|None=None,\n",
       ">                         model_params:dict|None=None,\n",
       ">                         dataloader_params:dict|None=None, device:str='cpu',\n",
       ">                         obsprocessors:list|None=None,\n",
       ">                         torch_obsprocessors:list|None=None,\n",
       ">                         agent_name:str|None='DLNV', test_batch_size:int=1024,\n",
       ">                         receive_batch_dim:bool=False, loss_function:Literal['q\n",
       ">                         uantile','pinball']='quantile')\n",
       "\n",
       "*Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
       "based on a deep learning model.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| learning_rate_scheduler | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| model_params | dict \\| None | None | default: {\"hidden_layers\": [64, 64], \"drop_prob\": 0.0, \"batch_norm\": False, \"relu_output\": False} |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| torch_obsprocessors | list \\| None | None | default: [FlattenTimeDim(allow_2d=False)] |\n",
       "| agent_name | str \\| None | DLNV |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NewsvendorDLAgent, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further information:   \n",
    "   \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    .. [1] Afshin Oroojlooyjadid, Lawrence V. Snyder, Martin Takáˇc,\n",
    "            \"Applying Deep Learning to the Newsvendor Problem\", 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L523){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NewsvendorDLAgent.set_model\n",
       "\n",
       ">      NewsvendorDLAgent.set_model (input_shape, output_shape)\n",
       "\n",
       "*Set the model for the agent to an MLP*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L523){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NewsvendorDLAgent.set_model\n",
       "\n",
       ">      NewsvendorDLAgent.set_model (input_shape, output_shape)\n",
       "\n",
       "*Set the model for the agent to an MLP*"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NewsvendorDLAgent.set_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Network architecture:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MLP                                      [1, 1, 1]                 --\n",
      "├─Sequential: 1-1                        [1, 1, 1]                 --\n",
      "│    └─Linear: 2-1                       [1, 1, 64]                192\n",
      "│    └─ReLU: 2-2                         [1, 1, 64]                --\n",
      "│    └─Dropout: 2-3                      [1, 1, 64]                --\n",
      "│    └─Linear: 2-4                       [1, 1, 64]                4,160\n",
      "│    └─ReLU: 2-5                         [1, 1, 64]                --\n",
      "│    └─Dropout: 2-6                      [1, 1, 64]                --\n",
      "│    └─Linear: 2-7                       [1, 1, 1]                 65\n",
      "│    └─Identity: 2-8                     [1, 1, 1]                 --\n",
      "==========================================================================================\n",
      "Total params: 4,417\n",
      "Trainable params: 4,417\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting experiment\n",
      "INFO:root:Initial evaluation: R=-30.661606375234683, J=-29.172413616244395\n",
      "INFO:root:Starting training with epochs fit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-28.32507302843735 -26.925014300122214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 1268.83it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 1417.78it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 35.11it/s]\n",
      "INFO:root:Finished training with epochs fit\n",
      "INFO:root:Evaluation after training: R=-16.24489721946409, J=-15.456595366749202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.04781734604064 -14.300095962755433\n"
     ]
    }
   ],
   "source": [
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start)\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    ")\n",
    "\n",
    "model_params = {\n",
    "    \"hidden_layers\": [64, 64],\n",
    "}\n",
    "\n",
    "agent = NewsvendorDLAgent(environment.mdp_info,\n",
    "                            dataloader,\n",
    "                            cu=np.array([0.42857]),\n",
    "                            co=np.array([1.0]),\n",
    "                            input_shape=(2,),\n",
    "                            output_shape=(1,),\n",
    "                            optimizer_params= {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}, # other optimizers: \"SGD\", \"RMSprop\"\n",
    "                            learning_rate_scheduler = None, # TODO add base class for learning rate scheduler for typing\n",
    "                            model_params = model_params, #\n",
    "                            dataloader_params={\"batch_size\": 32, \"shuffle\": True},\n",
    "                            torch_obsprocessors = [],\n",
    "                            device = \"cpu\" # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "run_experiment(agent, environment, 2, run_id = \"test\") # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseMetaAgent():\n",
    "\n",
    "    def set_meta_dataloader(\n",
    "        self, \n",
    "        dataloader: BaseDataLoader,\n",
    "        dataloader_params, # dict with keys: batch_size, shuffle\n",
    "        draw_parameter_function: callable, # function to draw parameters from distribution\n",
    "        distribution: str, # distribution for params during training\n",
    "        bounds_low: Union[int, float], # lower bound for params during training\n",
    "        bounds_high: Union[int, float], # upper bound for params during training\n",
    "        obsprocessor: callable, # function to process observations\n",
    "        parameter_names: List[str] = None, # names of parameters\n",
    "        ) -> None: \n",
    "\n",
    "        \"\"\" \"\"\"\n",
    "\n",
    "        # check if class already have a dataloader\n",
    "\n",
    "        print(\"setting meta datloader\")\n",
    "\n",
    "        dataset = DatasetWrapperMeta(\n",
    "            dataloader = dataloader,\n",
    "            draw_parameter_function = draw_parameter_function,\n",
    "            distribution = distribution,\n",
    "            bounds_low = bounds_low,\n",
    "            bounds_high = bounds_high,\n",
    "            obsprocessor = obsprocessor,\n",
    "            parameter_names = parameter_names,\n",
    "            )\n",
    "\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset, **dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorlERMMetaAgent(NewsvendorlERMAgent, BaseMetaAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
    "    based on a linear (regression) model. In addition to the features, the agent\n",
    "    also gets the sl as input to be able to forecast the optimal order quantity\n",
    "    for different sl values. Depending on the training pipeline, this model can be \n",
    "    adapted to become a full meta-learning algorithm cross products and cross sls.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                # Parameters for meta Agent\n",
    "                dataset_meta_params: dict, # Parameters for meta dataloader\n",
    "\n",
    "                # Parameters for lERM agent\n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "                model_params: dict | None = None,  # default: {\"relu_output\": False}\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                torch_obsprocessors: list | None = None,  # default: [FlattenTimeDim(allow_2d=False)]\n",
    "                device: str = \"cpu\",  # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = \"lERMMeta\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\",\n",
    "                ):\n",
    "\n",
    "        self.set_meta_dataloader(dataloader, dataloader_params, **dataset_meta_params)\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            optimizer_params=optimizer_params,\n",
    "            learning_rate_scheduler=learning_rate_scheduler,\n",
    "            model_params=model_params,\n",
    "            dataloader_params=dataloader_params,\n",
    "            obsprocessors=obsprocessors,\n",
    "            torch_obsprocessors=torch_obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim = receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorDLMetaAgent(NewsvendorDLAgent, BaseMetaAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
    "    based on a Neural Network. In addition to the features, the agent\n",
    "    also gets the sl as input to be able to forecast the optimal order quantity\n",
    "    for different sl values. Depending on the training pipeline, this model can be \n",
    "    adapted to become a full meta-learning algorithm cross products and cross sls.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                # Parameters for meta Agent\n",
    "                dataset_meta_params: dict, # Parameters for meta dataloader\n",
    "\n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "                \n",
    "                # parameters in yaml file\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                model_params: dict | None = None,  # default: {\"hidden_layers\": [64, 64], \"drop_prob\": 0.0, \"batch_norm\": False, \"relu_output\": False}\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                torch_obsprocessors: list | None = None,  # default: [FlattenTimeDim(allow_2d=False)]\n",
    "                agent_name: str | None = \"DLNV\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\",\n",
    "                ):\n",
    "\n",
    "        self.set_meta_dataloader(dataloader, dataloader_params, **dataset_meta_params)\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            learning_rate_scheduler=learning_rate_scheduler,\n",
    "\n",
    "            optimizer_params=optimizer_params,\n",
    "            model_params=model_params,\n",
    "            dataloader_params=dataloader_params,\n",
    "            device=device,\n",
    "\n",
    "            obsprocessors=obsprocessors,\n",
    "            torch_obsprocessors=torch_obsprocessors,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorDLTransformerAgent(NVBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO ADD DOCSTRING\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "                \n",
    "                # parameters in yaml file\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                model_params: dict | None = None,  # default: {\"hidden_layers\": [64, 64], \"drop_prob\": 0.0, \"batch_norm\": False, \"relu_output\": False}\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                torch_obsprocessors: list | None = None,  # default: [FlattenTimeDim(allow_2d=False)]\n",
    "                agent_name: str | None = \"DLNV\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\",\n",
    "                ):\n",
    "\n",
    "        # Handle mutable defaults unique to this class\n",
    "        default_model_params = {\n",
    "            \"hidden_layers\": [64, 64],\n",
    "            \"drop_prob\": 0.0,\n",
    "            \"batch_norm\": False,\n",
    "            \"relu_output\": False\n",
    "            }\n",
    "\n",
    "        self.model_params = self.update_model_params(default_model_params, model_params or {})\n",
    "        \n",
    "        torch_obsprocessors = [FlattenTimeDim(allow_2d=True)] if torch_obsprocessors is None else torch_obsprocessors\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            optimizer_params=optimizer_params,\n",
    "            learning_rate_scheduler=learning_rate_scheduler,\n",
    "            dataloader_params=dataloader_params,\n",
    "            obsprocessors=obsprocessors,\n",
    "            torch_obsprocessors=torch_obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )\n",
    "        \n",
    "    def set_model(self, input_shape, output_shape):\n",
    "        \n",
    "        \"\"\"Set the model for the agent to an MLP\"\"\"\n",
    "\n",
    "        if len(input_shape) == 1:\n",
    "            raise ValueError(\"Input shape must be at least 2D for Transformer model\")\n",
    "\n",
    "        output_size = output_shape[0]\n",
    "\n",
    "        from ddopnew.approximators import Transformer\n",
    "        self.model = Transformer(input_size=input_size, output_size=output_size, **self.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
