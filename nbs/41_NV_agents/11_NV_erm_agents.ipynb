{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory Management Environments\n",
    "\n",
    "> To be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.newsvendor.erm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.base import BaseAgent\n",
    "from ddopnew.utils import MDPInfo, Parameter, DatasetWrapper\n",
    "from ddopnew.loss_functions import TorchQuantileLoss\n",
    "\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SGDBaseAgent(BaseAgent):\n",
    "\n",
    "    train_mode = \"epochs_fit\"\n",
    "    \n",
    "    def __init__(self, \n",
    "            environment_info: MDPInfo,\n",
    "            dataloader: BaseDataLoader,\n",
    "            optimizer_params: dict = {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}, # other optimizers: \"SGD\", \"RMSprop\"\n",
    "            learning_rate_scheduler = None, # TODO add base class for learning rate scheduler for typing\n",
    "            dataloader_params: dict = {\"batch_size\": 32, \"shuffle\": True},\n",
    "            device: str = \"cpu\" # \"cuda\" or \"cpu\"\n",
    "            ):\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.set_dataloader(dataloader, dataloader_params)\n",
    "        self.set_model()\n",
    "        self.set_loss_function()\n",
    "        self.set_optimizer(optimizer_params)\n",
    "        self.set_learning_rate_scheduler(learning_rate_scheduler)\n",
    "\n",
    "        super().__init__(environment_info)\n",
    "\n",
    "    def set_dataloader(self, dataloader, dataloader_params):\n",
    "        dataset = DatasetWrapper(dataloader)\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset, **dataloader_params)\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_loss_function(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_model(self):\n",
    "        pass\n",
    "\n",
    "    def set_optimizer(self, optimizer_params):\n",
    "        optimizer = optimizer_params[\"optimizer\"]\n",
    "        optimizer_params_copy = optimizer_params.copy()\n",
    "        del optimizer_params_copy[\"optimizer\"]\n",
    "\n",
    "        if optimizer == \"Adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), **optimizer_params_copy)\n",
    "        elif optimizer == \"SGD\":\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), **optimizer_params_copy)\n",
    "        elif optimizer == \"RMSprop\":\n",
    "            self.optimizer = torch.optim.RMSprop(self.model.parameters(), **optimizer_params_copy)\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer {optimizer} not supported\")\n",
    "        \n",
    "    def set_learning_rate_scheduler(self, learning_rate_scheduler):\n",
    "        if learning_rate_scheduler is not None:\n",
    "            raise NotImplementedError(\"Learning rate scheduler not implemented yet\")\n",
    "        else:\n",
    "            self.learning_rate_scheduler = None\n",
    "\n",
    "    def fit_epoch(self, X, Y):\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        self.model.train()\n",
    "        total_loss=0\n",
    "\n",
    "        for i, output in enumerate(self.dataloader):\n",
    "            \n",
    "            X, y = output\n",
    "\n",
    "            # convert X and y to float32\n",
    "            X = X.type(torch.float32)\n",
    "            y = y.type(torch.float32)\n",
    "            \n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            y_pred = self.model(X)\n",
    "\n",
    "            if self.loss_function_params==None:\n",
    "                loss = self.loss_function(y_pred, y)\n",
    "            else:\n",
    "                loss = self.loss_function(y_pred, y, **self.loss_function_params) # TODO: add reduction param when defining loss function\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def draw_action(self, X):\n",
    "        y_pred = self.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # TODO handle if X is larger than some size, then split into batches\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "            y_pred = self.model(X)\n",
    "\n",
    "\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def to(self, device):\n",
    "        self.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NVBaseAgent(SGDBaseAgent):\n",
    "\n",
    "    def __init__(self, \n",
    "        environment_info: MDPInfo,\n",
    "        dataloader: BaseDataLoader,\n",
    "        cu: Union[np.ndarray, Parameter],\n",
    "        co: Union[np.ndarray, Parameter],\n",
    "        optimizer_params: dict = {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}, # other optimizers: \"SGD\", \"RMSprop\"\n",
    "        learning_rate_scheduler = None, # TODO add base class for learning rate scheduler for typing\n",
    "        dataloader_params: dict = {\"batch_size\": 32, \"shuffle\": True},\n",
    "        device: str = \"cpu\" # \"cuda\" or \"cpu\"\n",
    "        ):\n",
    "\n",
    "        self.sl = cu / (cu + co) # ensure this works if cu and co are Parameters\n",
    "\n",
    "        super().__init__(environment_info, dataloader, optimizer_params, learning_rate_scheduler, dataloader_params, device)\n",
    "\n",
    "\n",
    "    def set_loss_function(self):\n",
    "\n",
    "        self.loss_function_params = {\"quantile\": self.sl}\n",
    "        self.loss_function = TorchQuantileLoss(reduction=\"mean\")\n",
    "        \n",
    "        logging.debug(f\"Loss function set to {self.loss_function}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorlERMAgent(NVBaseAgent):\n",
    "\n",
    "    def __init__(self, \n",
    "        environment_info: MDPInfo,\n",
    "        dataloader: BaseDataLoader,\n",
    "        cu: Union[np.ndarray, Parameter],\n",
    "        co: Union[np.ndarray, Parameter],\n",
    "        optimizer_params: dict = {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}, # other optimizers: \"SGD\", \"RMSprop\"\n",
    "        learning_rate_scheduler = None, # TODO add base class for learning rate scheduler for typing\n",
    "        model_params: dict = {\"input_size\": 1, \"output_size\": 1, \"relu_output\": False}, #\n",
    "        dataloader_params: dict = {\"batch_size\": 32, \"shuffle\": True},\n",
    "        device: str = \"cpu\" # \"cuda\" or \"cpu\"\n",
    "        ):\n",
    "\n",
    "        self.model_params = model_params\n",
    "\n",
    "        super().__init__(environment_info, dataloader, cu, co, optimizer_params, learning_rate_scheduler, dataloader_params, device)\n",
    "    \n",
    "    def set_model(self):\n",
    "        from ddopnew.approximators import LinearModel\n",
    "        self.model = LinearModel(**self.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddopnew.envs.inventory import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.experiment_functions import run_experiment, test_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100\n",
      "-18.841829378284025 -17.90628799781712\n",
      "Step 100\n",
      "Step 100\n",
      "Step 100\n",
      "Step 100\n",
      "-15.13178600787327 -14.426744918943673\n"
     ]
    }
   ],
   "source": [
    "val_index_start = 800 #90_000\n",
    "test_index_start = 900 #100_000\n",
    "\n",
    "X = np.random.rand(1000, 2)\n",
    "Y = np.random.rand(1000, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start)\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    ")\n",
    "\n",
    "agent = NewsvendorlERMAgent(environment.mdp_info,\n",
    "                            dataloader,\n",
    "                            cu=np.array([0.42857]),\n",
    "                            co=np.array([1.0]),\n",
    "                            optimizer_params= {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}, # other optimizers: \"SGD\", \"RMSprop\"\n",
    "                            learning_rate_scheduler = None, # TODO add base class for learning rate scheduler for typing\n",
    "                            model_params = {\"input_size\": 2, \"output_size\": 1, \"relu_output\": False}, #\n",
    "                            dataloader_params={\"batch_size\": 32, \"shuffle\": True},\n",
    "                            device = \"cpu\" # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "run_experiment(agent, environment, 2, run_id = \"test\") # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
