{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory Management Environments\n",
    "\n",
    "> To be written.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.newsvendor.erm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional, List\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.base import BaseAgent\n",
    "from ddopnew.utils import MDPInfo, Parameter, DatasetWrapper\n",
    "from ddopnew.torch_utils.loss_functions import TorchQuantileLoss\n",
    "from ddopnew.torch_utils.preprocessors import FlattenTimeDim\n",
    "\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SGDBaseAgent(BaseAgent):\n",
    "\n",
    "    train_mode = \"epochs_fit\"\n",
    "    \n",
    "    def __init__(self, \n",
    "            environment_info: MDPInfo,\n",
    "            dataloader: BaseDataLoader,\n",
    "            optimizer_params: Optional[dict] = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "            learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "            dataloader_params: Optional[dict] = None, # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "            preprocessors: Optional[List] = None,     # default: []\n",
    "            postprocessors: Optional[List] = None,     # default: []\n",
    "            torch_preprocessors: Optional[List] = None,     # default: []\n",
    "            device: str = \"cpu\" # \"cuda\" or \"cpu\"\n",
    "            ):\n",
    "\n",
    "        # Initialize default values for mutable arguments\n",
    "        optimizer_params = optimizer_params or {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "        dataloader_params = dataloader_params or {\"batch_size\": 32, \"shuffle\": True}\n",
    "        self.torch_preprocessors = torch_preprocessors or []\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.set_dataloader(dataloader, dataloader_params)\n",
    "        self.set_model()\n",
    "        self.set_loss_function()\n",
    "        self.set_optimizer(optimizer_params)\n",
    "        self.set_learning_rate_scheduler(learning_rate_scheduler)\n",
    "\n",
    "        super().__init__(environment_info, preprocessors, postprocessors)\n",
    "\n",
    "    def set_dataloader(self, dataloader, dataloader_params):\n",
    "        dataset = DatasetWrapper(dataloader)\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset, **dataloader_params)\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_loss_function(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_model(self):\n",
    "        pass\n",
    "\n",
    "    def set_optimizer(self, optimizer_params):\n",
    "        optimizer = optimizer_params[\"optimizer\"]\n",
    "        optimizer_params_copy = optimizer_params.copy()\n",
    "        del optimizer_params_copy[\"optimizer\"]\n",
    "\n",
    "        if optimizer == \"Adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), **optimizer_params_copy)\n",
    "        elif optimizer == \"SGD\":\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), **optimizer_params_copy)\n",
    "        elif optimizer == \"RMSprop\":\n",
    "            self.optimizer = torch.optim.RMSprop(self.model.parameters(), **optimizer_params_copy)\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer {optimizer} not supported\")\n",
    "        \n",
    "    def set_learning_rate_scheduler(self, learning_rate_scheduler):\n",
    "        if learning_rate_scheduler is not None:\n",
    "            raise NotImplementedError(\"Learning rate scheduler not implemented yet\")\n",
    "        else:\n",
    "            self.learning_rate_scheduler = None\n",
    "\n",
    "    def fit_epoch(self):\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        self.model.train()\n",
    "        total_loss=0\n",
    "\n",
    "        for i, output in enumerate(self.dataloader):\n",
    "            \n",
    "            X, y = output\n",
    "\n",
    "            # convert X and y to float32\n",
    "            X = X.type(torch.float32)\n",
    "            y = y.type(torch.float32)\n",
    "\n",
    "            for torch_preprocessor in self.torch_preprocessors:\n",
    "                X = torch_preprocessor(X)\n",
    "            \n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            y_pred = self.model(X)\n",
    "\n",
    "            if self.loss_function_params==None:\n",
    "                loss = self.loss_function(y_pred, y)\n",
    "            else:\n",
    "                loss = self.loss_function(y_pred, y, **self.loss_function_params) # TODO: add reduction param when defining loss function\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def draw_action_(self, observation):\n",
    "        \n",
    "        action = self.predict(observation)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # TODO handle if X is larger than some size, then split into batches\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        self.model.eval()\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        for torch_preprocessor in self.torch_preprocessors:\n",
    "            X = torch_preprocessor(X)\n",
    "        X = X.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            y_pred = self.model(X)\n",
    "\n",
    "\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def to(self, device):\n",
    "        self.model.to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def update_model_params(default_params, custom_params):\n",
    "        updated_params = default_params.copy()\n",
    "        updated_params.update(custom_params)\n",
    "        return updated_params\n",
    "\n",
    "    def save(self, path: str, overwrite=True):\n",
    "        \"\"\"\n",
    "        Save the PyTorch model to a file in the specified directory.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str): The directory where the model file will be saved.\n",
    "        - overwrite (bool): If True, the file will be overwritten if it already exists. \n",
    "                            If False, a FileExistsError will be raised if the file exists.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the model attribute is not set.\n",
    "        - FileExistsError: If the file already exists and overwrite is set to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            raise AttributeError(\"Model is not defined in the class.\")\n",
    "\n",
    "        # Create the directory path if it does not exist\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Construct the file path using os.path.join for better cross-platform compatibility\n",
    "        full_path = os.path.join(path, \"model.pth\")\n",
    "\n",
    "        if os.path.exists(full_path):\n",
    "            if not overwrite:\n",
    "                raise FileExistsError(f\"The file {full_path} already exists and will not be overwritten.\")\n",
    "            else:\n",
    "                logging.info(f\"Overwriting file {full_path}\") # Only log with info as during training we will continuously overwrite the model\n",
    "        \n",
    "        # Save the model's state_dict using torch.save\n",
    "        torch.save(self.model.state_dict(), full_path)\n",
    "        logging.info(f\"Model saved successfully to {full_path}\")\n",
    "\n",
    "    def load(self, path: str):\n",
    "        \"\"\"\n",
    "        Load the PyTorch model from a file.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str): The directory where the model file is located.\n",
    "\n",
    "        Raises:\n",
    "        - FileNotFoundError: If the file does not exist.\n",
    "        - AttributeError: If the model attribute is not set.\n",
    "        - RuntimeError: If the model state cannot be loaded due to shape or parameter mismatch.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            raise AttributeError(\"Model is not defined in the class.\")\n",
    "\n",
    "        # Construct the file path\n",
    "        full_path = os.path.join(path, \"model.pth\")\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            raise FileNotFoundError(f\"The file {full_path} does not exist.\")\n",
    "\n",
    "        try:\n",
    "            # Load the model's state_dict using torch.load\n",
    "            self.model.load_state_dict(torch.load(full_path))\n",
    "            logging.info(f\"Model loaded successfully from {full_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred while loading the model: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NVBaseAgent(SGDBaseAgent):\n",
    "\n",
    "    def __init__(self, \n",
    "        environment_info: MDPInfo,\n",
    "        dataloader: BaseDataLoader,\n",
    "        cu: Union[np.ndarray, Parameter],\n",
    "        co: Union[np.ndarray, Parameter],\n",
    "        optimizer_params: Optional[dict] = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "        learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "        dataloader_params: Optional[dict] = None, # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "        preprocessors: Optional[List] = None,     # default: []\n",
    "        postprocessors: Optional[List] = None,     # default: []\n",
    "        torch_preprocessors: Optional[List] = None,     # default: []\n",
    "        device: str = \"cpu\"  # \"cuda\" or \"cpu\"\n",
    "        ):\n",
    "        \n",
    "        self.sl = cu / (cu + co) # ensure this works if cu and co are Parameters\n",
    "\n",
    "        super().__init__(environment_info, dataloader, optimizer_params, learning_rate_scheduler, dataloader_params, preprocessors, postprocessors,torch_preprocessors, device)\n",
    "\n",
    "\n",
    "    def set_loss_function(self):\n",
    "\n",
    "        self.loss_function_params = {\"quantile\": self.sl}\n",
    "        self.loss_function = TorchQuantileLoss(reduction=\"mean\")\n",
    "        \n",
    "        logging.debug(f\"Loss function set to {self.loss_function}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorlERMAgent(NVBaseAgent):\n",
    "\n",
    "    def __init__(self, \n",
    "        environment_info: MDPInfo,\n",
    "        dataloader: BaseDataLoader,\n",
    "        cu: Union[np.ndarray, Parameter],\n",
    "        co: Union[np.ndarray, Parameter],\n",
    "        optimizer_params: Optional[dict] = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "        learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "        model_params: Optional[dict] = None,      # default: {\"input_size\": 1, \"output_size\": 1, \"relu_output\": False}\n",
    "        dataloader_params: Optional[dict] = None, # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "        preprocessors: Optional[List] = None,     # default: \n",
    "        postprocessors: Optional[List] = None,     # default: []\n",
    "        torch_preprocessors: Optional[List] = None,     # default: [FlattenTimeDim(allow_2d=False)]\n",
    "        device: str = \"cpu\"  # \"cuda\" or \"cpu\"\n",
    "        ):\n",
    "\n",
    "\n",
    "        # Handle mutable defaults unique to this class\n",
    "        default_model_params = {\n",
    "            \"input_size\": 1,\n",
    "            \"output_size\": 1,\n",
    "            \"relu_output\": False\n",
    "            }\n",
    "\n",
    "        self.model_params = self.update_model_params(default_model_params, model_params or {})\n",
    "\n",
    "        torch_preprocessors = [FlattenTimeDim(allow_2d=True)] if torch_preprocessors is None else torch_preprocessors\n",
    "\n",
    "        super().__init__(environment_info, dataloader, cu, co, optimizer_params, learning_rate_scheduler, dataloader_params, preprocessors, postprocessors, torch_preprocessors, device)\n",
    "    \n",
    "    def set_model(self):\n",
    "        from ddopnew.approximators import LinearModel\n",
    "        self.model = LinearModel(**self.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddopnew.envs.inventory import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.experiment_functions import run_experiment, test_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.051547487461786 -17.19908573287191\n",
      "-14.24499509513067 -13.575595456292424\n"
     ]
    }
   ],
   "source": [
    "val_index_start = 800 #90_000\n",
    "test_index_start = 900 #100_000\n",
    "\n",
    "X = np.random.rand(1000, 2)\n",
    "Y = np.random.rand(1000, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start)\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    ")\n",
    "\n",
    "agent = NewsvendorlERMAgent(environment.mdp_info,\n",
    "                            dataloader,\n",
    "                            cu=np.array([0.42857]),\n",
    "                            co=np.array([1.0]),\n",
    "                            optimizer_params= {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}, # other optimizers: \"SGD\", \"RMSprop\"\n",
    "                            learning_rate_scheduler = None, # TODO add base class for learning rate scheduler for typing\n",
    "                            model_params = {\"input_size\": 2, \"output_size\": 1, \"relu_output\": False}, #\n",
    "                            dataloader_params={\"batch_size\": 32, \"shuffle\": True},\n",
    "                            torch_preprocessors = [],\n",
    "                            device = \"cpu\" # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "run_experiment(agent, environment, 2, run_id = \"test\") # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorDLAgent(NVBaseAgent):\n",
    "\n",
    "    def __init__(self, \n",
    "        environment_info: MDPInfo,\n",
    "        dataloader: BaseDataLoader,\n",
    "        cu: Union[np.ndarray, Parameter],\n",
    "        co: Union[np.ndarray, Parameter],\n",
    "        optimizer_params: Optional[dict] = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "        learning_rate_scheduler = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "        model_params: Optional[dict] = None,      # default: {\"input_size\": 1, \"output_size\": 1, \"relu_output\": False}\n",
    "        dataloader_params: Optional[dict] = None, # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "        preprocessors: Optional[List] = None,     # default: \n",
    "        postprocessors: Optional[List] = None,     # default: []\n",
    "        torch_preprocessors: Optional[List] = None,     # default: [FlattenTimeDim(allow_2d=False)]\n",
    "        device: str = \"cpu\"  # \"cuda\" or \"cpu\"\n",
    "        ):\n",
    "\n",
    "        # Handle mutable defaults unique to this class\n",
    "        default_model_params = {\n",
    "            \"input_size\": 1,\n",
    "            \"output_size\": 1,\n",
    "            \"hidden_layers\": [64, 64],\n",
    "            \"drop_prob\": 0.0,\n",
    "            \"batch_norm\": False,\n",
    "            \"relu_output\": False\n",
    "            }\n",
    "\n",
    "        self.model_params = self.update_model_params(default_model_params, model_params or {})\n",
    "        \n",
    "        torch_preprocessors = [FlattenTimeDim(allow_2d=True)] if torch_preprocessors is None else torch_preprocessors\n",
    "\n",
    "        super().__init__(environment_info, dataloader, cu, co, optimizer_params, learning_rate_scheduler, dataloader_params, preprocessors, postprocessors, torch_preprocessors, device)\n",
    "    \n",
    "    def set_model(self):\n",
    "        from ddopnew.approximators import MLP\n",
    "        self.model = MLP(**self.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.911026013892176 -17.999486975878526\n",
      "-13.953989446869352 -13.277391185500251\n"
     ]
    }
   ],
   "source": [
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start)\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    ")\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    \"input_size\": 2,\n",
    "}\n",
    "\n",
    "agent = NewsvendorDLAgent(environment.mdp_info,\n",
    "                            dataloader,\n",
    "                            cu=np.array([0.42857]),\n",
    "                            co=np.array([1.0]),\n",
    "                            optimizer_params= {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}, # other optimizers: \"SGD\", \"RMSprop\"\n",
    "                            learning_rate_scheduler = None, # TODO add base class for learning rate scheduler for typing\n",
    "                            model_params = model_params, #\n",
    "                            dataloader_params={\"batch_size\": 32, \"shuffle\": True},\n",
    "                            torch_preprocessors = [],\n",
    "                            device = \"cpu\" # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "run_experiment(agent, environment, 2, run_id = \"test\") # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
