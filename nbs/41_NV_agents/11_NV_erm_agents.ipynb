{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERM agents\n",
    "\n",
    "> Newsvendor agents based on Empirical Risk Minimization (ERM) principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.newsvendor.erm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional, List, Tuple, Literal, Callable, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython import get_ipython\n",
    "\n",
    "from ddopnew.envs.base import BaseEnvironment\n",
    "from ddopnew.agents.base import BaseAgent\n",
    "from ddopnew.utils import MDPInfo, Parameter, DatasetWrapper, DatasetWrapperMeta\n",
    "from ddopnew.torch_utils.loss_functions import TorchQuantileLoss, TorchPinballLoss\n",
    "from ddopnew.obsprocessors import FlattenTimeDimNumpy\n",
    "from ddopnew.dataloaders.base import BaseDataLoader\n",
    "from ddopnew.ml_utils import LRSchedulerPerStep\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SGDBaseAgent(BaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Base class for Agents that are trained using Stochastic Gradient Descent (SGD) on PyTorch models.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Remove input shapes as input end get from MDPInfo\n",
    "\n",
    "    train_mode = \"epochs_fit\"\n",
    "    \n",
    "    def __init__(self, \n",
    "            environment_info: MDPInfo,\n",
    "            dataloader: BaseDataLoader,\n",
    "            input_shape: Tuple,\n",
    "            output_shape: Tuple,\n",
    "            dataset_params: Optional[dict] = None, # parameters needed to convert the dataloader to a torch dataset\n",
    "            dataloader_params: Optional[dict] = None, # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "            optimizer_params: Optional[dict] = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "            learning_rate_scheduler_params: Dict | None = None, # default: None. If dict, then first key is \"scheduler\" and the rest are the parameters\n",
    "            obsprocessors: Optional[List] = None,     # default: []\n",
    "            device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "            agent_name: str | None = None,\n",
    "            test_batch_size: int = 1024,\n",
    "            receive_batch_dim: bool = False,\n",
    "            ):\n",
    "\n",
    "        # Initialize default values for mutable arguments\n",
    "        optimizer_params = optimizer_params or {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "        dataloader_params = dataloader_params or {\"batch_size\": 32, \"shuffle\": True}\n",
    "        dataset_params = dataset_params or {}\n",
    "\n",
    "        self.device = self.set_device(device)\n",
    "        \n",
    "        self.set_dataloader(dataloader, dataset_params, dataloader_params)\n",
    "\n",
    "        self.set_model(input_shape, output_shape)\n",
    "        self.loss_function_params=None # default\n",
    "        self.set_loss_function()\n",
    "        self.set_optimizer(optimizer_params)\n",
    "        self.set_learning_rate_scheduler(learning_rate_scheduler_params)\n",
    "        self.test_batch_size = test_batch_size\n",
    "\n",
    "        super().__init__(environment_info = environment_info, obsprocessors = obsprocessors, agent_name = agent_name, receive_batch_dim = receive_batch_dim)\n",
    "\n",
    "        batch_dim = 1\n",
    "        logging.info(\"Network architecture:\")\n",
    "        if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "\n",
    "            self.model.eval()\n",
    "            if any(isinstance(obsprocessor, FlattenTimeDimNumpy) for obsprocessor in self.obsprocessors):\n",
    "                input_size = (batch_dim, int(np.prod(input_shape)))\n",
    "            else:\n",
    "                input_size = (batch_dim, *input_shape)\n",
    "\n",
    "            input_tensor = torch.randn(*input_size).to(self.device)\n",
    "            input_tuple = (input_tensor,)\n",
    "\n",
    "            if get_ipython() is not None:\n",
    "                print(summary(self.model, input_data=input_tuple, device=self.device))\n",
    "            else:\n",
    "                summary(self.model, input_data=input_tuple, device=self.device)\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def set_device(self, device: str):\n",
    "\n",
    "        \"\"\" Set the device for the model \"\"\"\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            if torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "            else:\n",
    "                logging.warning(\"CUDA is not available. Using CPU instead.\")\n",
    "                return \"cpu\"\n",
    "        elif device == \"cpu\":\n",
    "            return \"cpu\"\n",
    "        else:\n",
    "            raise ValueError(f\"Device {device} not currently not supported, use 'cuda' or 'cpu'\")\n",
    "\n",
    "\n",
    "    def set_dataloader(self,\n",
    "                        dataloader: BaseDataLoader,\n",
    "                        dataset_params: dict,\n",
    "                        dataloader_params: dict, # dict with keys: batch_size, shuffle\n",
    "                        ) -> None: \n",
    "\n",
    "        \"\"\"\n",
    "        Set the dataloader for the agent by wrapping it into a Torch Dataset\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # check if class already have a dataloader\n",
    "        if not hasattr(self, 'dataloader'):\n",
    "\n",
    "            dataset = DatasetWrapper(dataloader, **dataset_params)\n",
    "            self.dataloader = torch.utils.data.DataLoader(dataset, **dataloader_params)\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_loss_function(self):\n",
    "        \"\"\" Set loss function for the model \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_model(self, input_shape: Tuple, output_shape: Tuple):\n",
    "        \"\"\" Set the model for the agent \"\"\"\n",
    "        pass\n",
    "\n",
    "    def set_optimizer(self, optimizer_params: dict): # dict with keys: optimizer, lr, weight_decay\n",
    "        \n",
    "        \"\"\" Set the optimizer for the model \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'optimizer'):\n",
    "            \n",
    "            optimizer = optimizer_params[\"optimizer\"]\n",
    "            optimizer_params_copy = optimizer_params.copy()\n",
    "            del optimizer_params_copy[\"optimizer\"]\n",
    "\n",
    "            if optimizer == \"Adam\":\n",
    "                self.optimizer = torch.optim.Adam(self.model.parameters(), **optimizer_params_copy)\n",
    "            elif optimizer == \"SGD\":\n",
    "                self.optimizer = torch.optim.SGD(self.model.parameters(), **optimizer_params_copy)\n",
    "            elif optimizer == \"RMSprop\":\n",
    "                self.optimizer = torch.optim.RMSprop(self.model.parameters(), **optimizer_params_copy)\n",
    "            else:\n",
    "                raise ValueError(f\"Optimizer {optimizer} not supported\")\n",
    "        \n",
    "    def set_learning_rate_scheduler(self, learning_rate_scheduler_params): #\n",
    "        \"\"\" Set learning rate scheudler (can be None) \"\"\"\n",
    "\n",
    "        if learning_rate_scheduler_params is not None:\n",
    "\n",
    "            params = learning_rate_scheduler_params.copy()\n",
    "            scheduler_type = params[\"scheduler\"]\n",
    "            del params[\"scheduler\"]\n",
    "            if scheduler_type == \"LRSchedulerPerStep\":\n",
    "                self.learning_rate_scheduler = LRSchedulerPerStep(self.optimizer, **params)\n",
    "            else:\n",
    "                raise ValueError(f\"Learning rate scheduler {scheduler_type} not supported\")\n",
    "\n",
    "        else:\n",
    "            self.learning_rate_scheduler = None\n",
    "\n",
    "    def fit_epoch(self):\n",
    "\n",
    "        \"\"\" Fit the model for one epoch using the dataloader \"\"\"\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        self.model.train()\n",
    "        total_loss=0\n",
    "\n",
    "        for i, output in enumerate(tqdm(self.dataloader)):\n",
    "            \n",
    "            if len(output)==3:\n",
    "                X, y, loss_function_params = output\n",
    "            else:\n",
    "                X, y = output\n",
    "                loss_function_params = None\n",
    "\n",
    "            # convert X and y to float32\n",
    "            X = X.type(torch.float32)\n",
    "            y = y.type(torch.float32)\n",
    "            \n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            y_pred = self.model(X)\n",
    "\n",
    "            if loss_function_params is not None:\n",
    "                loss = self.loss_function(y_pred, y, **loss_function_params)\n",
    "            elif self.loss_function_params is not None:\n",
    "                loss = self.loss_function(y_pred, y, **self.loss_function_params)\n",
    "            else:\n",
    "                loss = self.loss_function(y_pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.learning_rate_scheduler is not None:\n",
    "                self.learning_rate_scheduler.step()\n",
    "        \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def draw_action_(self, observation: np.ndarray) -> np.ndarray: #\n",
    "        \n",
    "        \"\"\" \n",
    "        Draw an action based on the fitted model (see predict method)\n",
    "        \"\"\"\n",
    "        \n",
    "        action = self.predict(observation)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_into_batches(X: np.ndarray, batch_size: int) -> List[np.ndarray]: #\n",
    "        \"\"\" Split the input into batches of the specified size \"\"\"\n",
    "        return [X[i:i+batch_size] for i in range(0, len(X), batch_size)]\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray: #\n",
    "        \"\"\" Do one forward pass of the model and return the prediction \"\"\"\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        self.model.eval()\n",
    "\n",
    "        batches = self.split_into_batches(X, self.test_batch_size)\n",
    "\n",
    "        y_pred_full = []\n",
    "        for batch in batches:\n",
    "\n",
    "            X = batch\n",
    "\n",
    "            X = torch.tensor(X, dtype=torch.float32)\n",
    "            X = X.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                y_pred = self.model(X)\n",
    "\n",
    "                # check if y_pred is not finite:\n",
    "                if not torch.all(torch.isfinite(y_pred)):\n",
    "\n",
    "                    print(y_pred)\n",
    "\n",
    "                    # check if X is not finite:\n",
    "                    if not torch.all(torch.isfinite(X)):\n",
    "\n",
    "                        print(\"X is not finite\")\n",
    "                        print(\"total X_shape: \", X.shape)\n",
    "                        print(\"non-finite indices: \", torch.nonzero(~torch.isfinite(X)))\n",
    "                        print(X)\n",
    "\n",
    "\n",
    "                    raise ValueError(\"Predicted values are not finite\")\n",
    "\n",
    "            y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "            y_pred_full.append(y_pred)\n",
    "        \n",
    "        y_pred_full = np.concatenate(y_pred_full, axis=0)\n",
    "\n",
    "        return y_pred_full\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"set the internal state of the agent and its model to train\"\"\"\n",
    "        self.mode = \"train\"\n",
    "        self.model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"set the internal state of the agent and its model to eval\"\"\"\n",
    "        self.mode = \"eval\"\n",
    "        self.model.eval()\n",
    "\n",
    "    def to(self, device: str): #\n",
    "        \"\"\"Move the model to the specified device\"\"\"\n",
    "        self.model.to(device)\n",
    "\n",
    "    def save(self,\n",
    "                path: str, # The directory where the file will be saved.\n",
    "                overwrite: bool=True): # Allow overwriting; if False, a FileExistsError will be raised if the file exists.\n",
    "        \n",
    "        \"\"\"\n",
    "        Save the PyTorch model to a file in the specified directory.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            raise AttributeError(\"Model is not defined in the class.\")\n",
    "\n",
    "        # Create the directory path if it does not exist\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Construct the file path using os.path.join for better cross-platform compatibility\n",
    "        full_path = os.path.join(path, \"model.pth\")\n",
    "\n",
    "        if os.path.exists(full_path):\n",
    "            if not overwrite:\n",
    "                raise FileExistsError(f\"The file {full_path} already exists and will not be overwritten.\")\n",
    "            else:\n",
    "                logging.debug(f\"Overwriting file {full_path}\") # Only log with info as during training we will continuously overwrite the model\n",
    "        \n",
    "        # Save the model's state_dict using torch.save\n",
    "        torch.save(self.model.state_dict(), full_path)\n",
    "        logging.debug(f\"Model saved successfully to {full_path}\")\n",
    "\n",
    "    def load(self, path: str): # Only the path to the folder is needed, not the file itself\n",
    " \n",
    "        \"\"\"\n",
    "        Load the PyTorch model from a file.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            raise AttributeError(\"Model is not defined in the class.\")\n",
    "\n",
    "        # Construct the file path\n",
    "        full_path = os.path.join(path, \"model.pth\")\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            raise FileNotFoundError(f\"The file {full_path} does not exist.\")\n",
    "\n",
    "        try:\n",
    "            # Load the model's state_dict using torch.load\n",
    "            if self.device == \"cuda\":\n",
    "                self.model.load_state_dict(torch.load(full_path))\n",
    "            else:\n",
    "                self.model.load_state_dict(torch.load(full_path, map_location=torch.device('cpu')))\n",
    "            logging.debug(f\"Model loaded successfully from {full_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred while loading the model: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L31){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## SGDBaseAgent\n",
       "\n",
       ">      SGDBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                    dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                    input_shape:Tuple, output_shape:Tuple,\n",
       ">                    dataset_params:Optional[dict]=None,\n",
       ">                    dataloader_params:Optional[dict]=None,\n",
       ">                    optimizer_params:Optional[dict]=None,\n",
       ">                    learning_rate_scheduler_params:Optional[Dict]=None,\n",
       ">                    obsprocessors:Optional[List]=None, device:str='cpu',\n",
       ">                    agent_name:str|None=None, test_batch_size:int=1024,\n",
       ">                    receive_batch_dim:bool=False)\n",
       "\n",
       "*Base class for Agents that are trained using Stochastic Gradient Descent (SGD) on PyTorch models.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| dataset_params | Optional | None | parameters needed to convert the dataloader to a torch dataset |\n",
       "| dataloader_params | Optional | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| optimizer_params | Optional | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler_params | Optional | None | default: None. If dict, then first key is \"scheduler\" and the rest are the parameters |\n",
       "| obsprocessors | Optional | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L31){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## SGDBaseAgent\n",
       "\n",
       ">      SGDBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                    dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                    input_shape:Tuple, output_shape:Tuple,\n",
       ">                    dataset_params:Optional[dict]=None,\n",
       ">                    dataloader_params:Optional[dict]=None,\n",
       ">                    optimizer_params:Optional[dict]=None,\n",
       ">                    learning_rate_scheduler_params:Optional[Dict]=None,\n",
       ">                    obsprocessors:Optional[List]=None, device:str='cpu',\n",
       ">                    agent_name:str|None=None, test_batch_size:int=1024,\n",
       ">                    receive_batch_dim:bool=False)\n",
       "\n",
       "*Base class for Agents that are trained using Stochastic Gradient Descent (SGD) on PyTorch models.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| dataset_params | Optional | None | parameters needed to convert the dataloader to a torch dataset |\n",
       "| dataloader_params | Optional | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| optimizer_params | Optional | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler_params | Optional | None | default: None. If dict, then first key is \"scheduler\" and the rest are the parameters |\n",
       "| obsprocessors | Optional | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important notes:\n",
    "\n",
    "SGD-based agents are all agents that are trained via SGD such as Linear Models or Neural Networks. Some specific requirements are necessary to make them interface properly with the environment.\n",
    "\n",
    "**Torch perprocessors**:\n",
    "\n",
    "* In addition to the general Numpy-based pre-processor, we also provide pre-processors that work on tensor level within the ```fit_epoch``` method and the ```predict``` method. They can be used in addition to the numpy-based pre-processors or instead of them. It's important to ensure that the shape of observations (after pre-processing) is the same for those from the environemnt and those from the dataloader during training.\n",
    "\n",
    "**Dataloader**:\n",
    "\n",
    "* As for normal supervised learning via Torch, we make use of the Torch dataloader to load the data. Instead of defining a custom dataset class, we provide a Wrapper that can be used around our dataloader to make its output and interface the same as a Torch dataset. The dataloader is then initialized when the agent is created such that the agent has access to the same dataloader as the environment.\n",
    " \n",
    "**Training process**:\n",
    "\n",
    "* The outper loop of the training process (epochs) is handled outside the agent by the ```run_experiment```functions (or can also be customized). The agent needs to have a ```fit_epoch``` method that tells the agent what to do within an epoch. \n",
    "This includes:\n",
    "    * Getting the data from the dataloader\n",
    "    * Pre-processing the data\n",
    "    * Forward pass\n",
    "    * Loss calculation\n",
    "    * Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_dataloader\n",
       "\n",
       ">      SGDBaseAgent.set_dataloader\n",
       ">                                   (dataloader:ddopnew.dataloaders.base.BaseDat\n",
       ">                                   aLoader, dataset_params:dict,\n",
       ">                                   dataloader_params:dict)\n",
       "\n",
       "*Set the dataloader for the agent by wrapping it into a Torch Dataset*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dataloader | BaseDataLoader |  |\n",
       "| dataset_params | dict |  |\n",
       "| dataloader_params | dict | dict with keys: batch_size, shuffle |\n",
       "| **Returns** | **None** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_dataloader\n",
       "\n",
       ">      SGDBaseAgent.set_dataloader\n",
       ">                                   (dataloader:ddopnew.dataloaders.base.BaseDat\n",
       ">                                   aLoader, dataset_params:dict,\n",
       ">                                   dataloader_params:dict)\n",
       "\n",
       "*Set the dataloader for the agent by wrapping it into a Torch Dataset*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dataloader | BaseDataLoader |  |\n",
       "| dataset_params | dict |  |\n",
       "| dataloader_params | dict | dict with keys: batch_size, shuffle |\n",
       "| **Returns** | **None** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.set_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_loss_function\n",
       "\n",
       ">      SGDBaseAgent.set_loss_function ()\n",
       "\n",
       "*Set loss function for the model*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_loss_function\n",
       "\n",
       ">      SGDBaseAgent.set_loss_function ()\n",
       "\n",
       "*Set loss function for the model*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.set_loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L135){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_model\n",
       "\n",
       ">      SGDBaseAgent.set_model (input_shape:Tuple, output_shape:Tuple)\n",
       "\n",
       "*Set the model for the agent*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L135){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_model\n",
       "\n",
       ">      SGDBaseAgent.set_model (input_shape:Tuple, output_shape:Tuple)\n",
       "\n",
       "*Set the model for the agent*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.set_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L139){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_optimizer\n",
       "\n",
       ">      SGDBaseAgent.set_optimizer (optimizer_params:dict)\n",
       "\n",
       "*Set the optimizer for the model*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| optimizer_params | dict | dict with keys: optimizer, lr, weight_decay |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L139){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_optimizer\n",
       "\n",
       ">      SGDBaseAgent.set_optimizer (optimizer_params:dict)\n",
       "\n",
       "*Set the optimizer for the model*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| optimizer_params | dict | dict with keys: optimizer, lr, weight_decay |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.set_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L158){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_learning_rate_scheduler\n",
       "\n",
       ">      SGDBaseAgent.set_learning_rate_scheduler (learning_rate_scheduler_params)\n",
       "\n",
       "*Set learning rate scheudler (can be None)*\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| learning_rate_scheduler_params |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L158){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.set_learning_rate_scheduler\n",
       "\n",
       ">      SGDBaseAgent.set_learning_rate_scheduler (learning_rate_scheduler_params)\n",
       "\n",
       "*Set learning rate scheudler (can be None)*\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| learning_rate_scheduler_params |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.set_learning_rate_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L174){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.fit_epoch\n",
       "\n",
       ">      SGDBaseAgent.fit_epoch ()\n",
       "\n",
       "*Fit the model for one epoch using the dataloader*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L174){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.fit_epoch\n",
       "\n",
       ">      SGDBaseAgent.fit_epoch ()\n",
       "\n",
       "*Fit the model for one epoch using the dataloader*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.fit_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L219){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.draw_action_\n",
       "\n",
       ">      SGDBaseAgent.draw_action_ (observation:numpy.ndarray)\n",
       "\n",
       "*Draw an action based on the fitted model (see predict method)*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| observation | ndarray |  |\n",
       "| **Returns** | **ndarray** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L219){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.draw_action_\n",
       "\n",
       ">      SGDBaseAgent.draw_action_ (observation:numpy.ndarray)\n",
       "\n",
       "*Draw an action based on the fitted model (see predict method)*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| observation | ndarray |  |\n",
       "| **Returns** | **ndarray** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.draw_action_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L234){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.predict\n",
       "\n",
       ">      SGDBaseAgent.predict (X:numpy.ndarray)\n",
       "\n",
       "*Do one forward pass of the model and return the prediction*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | ndarray |  |\n",
       "| **Returns** | **ndarray** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L234){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.predict\n",
       "\n",
       ">      SGDBaseAgent.predict (X:numpy.ndarray)\n",
       "\n",
       "*Do one forward pass of the model and return the prediction*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | ndarray |  |\n",
       "| **Returns** | **ndarray** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L262){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.train\n",
       "\n",
       ">      SGDBaseAgent.train ()\n",
       "\n",
       "*set the internal state of the agent and its model to train*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L262){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.train\n",
       "\n",
       ">      SGDBaseAgent.train ()\n",
       "\n",
       "*set the internal state of the agent and its model to train*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L267){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.eval\n",
       "\n",
       ">      SGDBaseAgent.eval ()\n",
       "\n",
       "*set the internal state of the agent and its model to eval*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L267){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.eval\n",
       "\n",
       ">      SGDBaseAgent.eval ()\n",
       "\n",
       "*set the internal state of the agent and its model to eval*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L272){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.to\n",
       "\n",
       ">      SGDBaseAgent.to (device:str)\n",
       "\n",
       "*Move the model to the specified device*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| device | str |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L272){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.to\n",
       "\n",
       ">      SGDBaseAgent.to (device:str)\n",
       "\n",
       "*Move the model to the specified device*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| device | str |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L276){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.save\n",
       "\n",
       ">      SGDBaseAgent.save (path:str, overwrite:bool=True)\n",
       "\n",
       "*Save the PyTorch model to a file in the specified directory.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| path | str |  | The directory where the file will be saved. |\n",
       "| overwrite | bool | True | Allow overwriting; if False, a FileExistsError will be raised if the file exists. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L276){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.save\n",
       "\n",
       ">      SGDBaseAgent.save (path:str, overwrite:bool=True)\n",
       "\n",
       "*Save the PyTorch model to a file in the specified directory.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| path | str |  | The directory where the file will be saved. |\n",
       "| overwrite | bool | True | Allow overwriting; if False, a FileExistsError will be raised if the file exists. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L304){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.load\n",
       "\n",
       ">      SGDBaseAgent.load (path:str)\n",
       "\n",
       "*Load the PyTorch model from a file.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str | Only the path to the folder is needed, not the file itself |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L304){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGDBaseAgent.load\n",
       "\n",
       ">      SGDBaseAgent.load (path:str)\n",
       "\n",
       "*Load the PyTorch model from a file.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str | Only the path to the folder is needed, not the file itself |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SGDBaseAgent.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NVBaseAgent(SGDBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Base agent for the Newsvendor problem implementing\n",
    "    the loss function for the Empirical Risk Minimization (ERM) approach\n",
    "    based on quantile loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                learning_rate_scheduler_params = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "                dataset_params: dict | None = None, # parameters needed to convert the dataloader to a torch dataset\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                obsprocessors: list | None = None,      # default: []\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = None,\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\", \n",
    "                ):\n",
    "\n",
    "        cu = self.convert_to_numpy_array(cu)\n",
    "        co = self.convert_to_numpy_array(co)\n",
    "        \n",
    "        self.sl = cu / (cu + co) # ensure this works if cu and co are Parameters\n",
    "        self.cu = cu\n",
    "        self.co = co\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            optimizer_params=optimizer_params,\n",
    "            learning_rate_scheduler_params=learning_rate_scheduler_params,\n",
    "            dataset_params=dataset_params,\n",
    "            dataloader_params=dataloader_params,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "        )   \n",
    "        \n",
    "    def set_loss_function(self):\n",
    "        \n",
    "        \"\"\"Set the loss function for the model to the quantile loss. For training\n",
    "        the model uses quantile loss and not the pinball loss with specific cu and \n",
    "        co values to ensure similar scale of the feedback signal during training.\"\"\"\n",
    "\n",
    "        if self.loss_function == \"quantile\":\n",
    "            self.loss_function_params = {\"quantile\": self.sl}\n",
    "            self.loss_function = TorchQuantileLoss(reduction=\"mean\")\n",
    "            logging.debug(f\"Loss function set to {self.loss_function}\")\n",
    "\n",
    "        elif self.loss_function == \"pinball\":\n",
    "            self.loss_function_params = {\"underage\": self.cu, \"overage\": self.co}\n",
    "            self.loss_function = TorchPinballLoss(reduction=\"mean\")\n",
    "            logging.debug(f\"Loss function set to {self.loss_function}\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Loss function {self.loss_function} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L328){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NVBaseAgent\n",
       "\n",
       ">      NVBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                   dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                   cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                   co:numpy.ndarray|ddopnew.utils.Parameter, input_shape:Tuple,\n",
       ">                   output_shape:Tuple, optimizer_params:dict|None=None,\n",
       ">                   learning_rate_scheduler_params=None,\n",
       ">                   dataset_params:dict|None=None,\n",
       ">                   dataloader_params:dict|None=None,\n",
       ">                   obsprocessors:list|None=None, device:str='cpu',\n",
       ">                   agent_name:str|None=None, test_batch_size:int=1024,\n",
       ">                   receive_batch_dim:bool=False,\n",
       ">                   loss_function:Literal['quantile','pinball']='quantile')\n",
       "\n",
       "*Base agent for the Newsvendor problem implementing\n",
       "the loss function for the Empirical Risk Minimization (ERM) approach\n",
       "based on quantile loss.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler_params | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| dataset_params | dict \\| None | None | parameters needed to convert the dataloader to a torch dataset |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L328){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NVBaseAgent\n",
       "\n",
       ">      NVBaseAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                   dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                   cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                   co:numpy.ndarray|ddopnew.utils.Parameter, input_shape:Tuple,\n",
       ">                   output_shape:Tuple, optimizer_params:dict|None=None,\n",
       ">                   learning_rate_scheduler_params=None,\n",
       ">                   dataset_params:dict|None=None,\n",
       ">                   dataloader_params:dict|None=None,\n",
       ">                   obsprocessors:list|None=None, device:str='cpu',\n",
       ">                   agent_name:str|None=None, test_batch_size:int=1024,\n",
       ">                   receive_batch_dim:bool=False,\n",
       ">                   loss_function:Literal['quantile','pinball']='quantile')\n",
       "\n",
       "*Base agent for the Newsvendor problem implementing\n",
       "the loss function for the Empirical Risk Minimization (ERM) approach\n",
       "based on quantile loss.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler_params | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| dataset_params | dict \\| None | None | parameters needed to convert the dataloader to a torch dataset |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | None |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NVBaseAgent, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L381){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NVBaseAgent.set_loss_function\n",
       "\n",
       ">      NVBaseAgent.set_loss_function ()\n",
       "\n",
       "*Set the loss function for the model to the quantile loss. For training\n",
       "the model uses quantile loss and not the pinball loss with specific cu and \n",
       "co values to ensure similar scale of the feedback signal during training.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L381){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NVBaseAgent.set_loss_function\n",
       "\n",
       ">      NVBaseAgent.set_loss_function ()\n",
       "\n",
       "*Set the loss function for the model to the quantile loss. For training\n",
       "the model uses quantile loss and not the pinball loss with specific cu and \n",
       "co values to ensure similar scale of the feedback signal during training.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NVBaseAgent.set_loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorlERMAgent(NVBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
    "    based on a linear (regression) model. Note that this implementation finds\n",
    "    the optimal regression parameters via SGD.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                learning_rate_scheduler_params = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "                model_params: dict | None = None,  # default: {\"relu_output\": False}\n",
    "                dataset_params: dict | None = None, # parameters needed to convert the dataloader to a torch dataset\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                device: str = \"cpu\",  # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = \"lERM\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\", \n",
    "                ):\n",
    "\n",
    "        # Handle mutable defaults unique to this class\n",
    "        default_model_params = {\n",
    "            \"relu_output\": False\n",
    "            }\n",
    "\n",
    "        self.model_params = self.update_model_params(default_model_params, model_params or {})\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            optimizer_params=optimizer_params,\n",
    "            learning_rate_scheduler_params=learning_rate_scheduler_params,\n",
    "            dataloader_params=dataloader_params,\n",
    "            dataset_params=dataset_params,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )\n",
    "    def set_model(self, input_shape, output_shape):\n",
    "\n",
    "        \"\"\"Set the model for the agent to a linear model\"\"\"\n",
    "\n",
    "        from ddopnew.approximators import LinearModel\n",
    "\n",
    "        # flatten time dim of input\n",
    "        print(\"input shape\", input_shape)\n",
    "        input_size = np.prod(input_shape)\n",
    "        output_size = output_shape[0]\n",
    "\n",
    "        self.model = LinearModel(input_size=input_size, output_size=output_size, **self.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L401){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NewsvendorlERMAgent\n",
       "\n",
       ">      NewsvendorlERMAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                           dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                           cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                           co:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                           input_shape:Tuple, output_shape:Tuple,\n",
       ">                           optimizer_params:dict|None=None,\n",
       ">                           learning_rate_scheduler_params=None,\n",
       ">                           model_params:dict|None=None,\n",
       ">                           dataset_params:dict|None=None,\n",
       ">                           dataloader_params:dict|None=None,\n",
       ">                           obsprocessors:list|None=None, device:str='cpu',\n",
       ">                           agent_name:str|None='lERM',\n",
       ">                           test_batch_size:int=1024,\n",
       ">                           receive_batch_dim:bool=False, loss_function:Literal[\n",
       ">                           'quantile','pinball']='quantile')\n",
       "\n",
       "*Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
       "based on a linear (regression) model. Note that this implementation finds\n",
       "the optimal regression parameters via SGD.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler_params | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| model_params | dict \\| None | None | default: {\"relu_output\": False} |\n",
       "| dataset_params | dict \\| None | None | parameters needed to convert the dataloader to a torch dataset |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | lERM |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L401){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NewsvendorlERMAgent\n",
       "\n",
       ">      NewsvendorlERMAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                           dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                           cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                           co:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                           input_shape:Tuple, output_shape:Tuple,\n",
       ">                           optimizer_params:dict|None=None,\n",
       ">                           learning_rate_scheduler_params=None,\n",
       ">                           model_params:dict|None=None,\n",
       ">                           dataset_params:dict|None=None,\n",
       ">                           dataloader_params:dict|None=None,\n",
       ">                           obsprocessors:list|None=None, device:str='cpu',\n",
       ">                           agent_name:str|None='lERM',\n",
       ">                           test_batch_size:int=1024,\n",
       ">                           receive_batch_dim:bool=False, loss_function:Literal[\n",
       ">                           'quantile','pinball']='quantile')\n",
       "\n",
       "*Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
       "based on a linear (regression) model. Note that this implementation finds\n",
       "the optimal regression parameters via SGD.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| learning_rate_scheduler_params | NoneType | None | TODO: add base class for learning rate scheduler for typing |\n",
       "| model_params | dict \\| None | None | default: {\"relu_output\": False} |\n",
       "| dataset_params | dict \\| None | None | parameters needed to convert the dataloader to a torch dataset |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| agent_name | str \\| None | lERM |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NewsvendorlERMAgent, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further information:   \n",
    "   \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    .. [1] Gah-Yi Ban, Cynthia Rudin, \"The Big Data Newsvendor: Practical Insights\n",
    "        from Machine Learning\", 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L455){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NewsvendorlERMAgent.set_model\n",
       "\n",
       ">      NewsvendorlERMAgent.set_model (input_shape, output_shape)\n",
       "\n",
       "*Set the model for the agent to a linear model*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L455){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NewsvendorlERMAgent.set_model\n",
       "\n",
       ">      NewsvendorlERMAgent.set_model (input_shape, output_shape)\n",
       "\n",
       "*Set the model for the agent to a linear model*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NewsvendorlERMAgent.set_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Network architecture:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape (2,)\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LinearModel                              [1, 1]                    --\n",
      "├─Linear: 1-1                            [1, 1]                    3\n",
      "├─Identity: 1-2                          [1, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting experiment\n",
      "INFO:root:Initial evaluation: R=-53.74981173191295, J=-51.13917631562847\n",
      "INFO:root:Starting training with epochs fit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-52.66597676734078 -50.12757372931984\n",
      "Experiment directory: results/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 1853.75it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 1872.96it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 46.54it/s]\n",
      "INFO:root:Finished training with epochs fit\n",
      "INFO:root:Evaluation after training: R=-16.2603253581593, J=-15.494811098433305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.83217868015756 -14.120542478739525\n"
     ]
    }
   ],
   "source": [
    "from ddopnew.envs.inventory.single_period import NewsvendorEnv\n",
    "from ddopnew.dataloaders.tabular import XYDataLoader\n",
    "from ddopnew.experiment_functions import run_experiment, test_agent\n",
    "\n",
    "val_index_start = 800 #90_000\n",
    "test_index_start = 900 #100_000\n",
    "\n",
    "X = np.random.rand(1000, 2)\n",
    "Y = np.random.rand(1000, 1)\n",
    "\n",
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start)\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    ")\n",
    "\n",
    "agent = NewsvendorlERMAgent(environment.mdp_info,\n",
    "                            dataloader,\n",
    "                            cu=np.array([0.42857]),\n",
    "                            co=np.array([1.0]),\n",
    "                            input_shape=(2,),\n",
    "                            output_shape=(1,),\n",
    "                            optimizer_params= {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}, # other optimizers: \"SGD\", \"RMSprop\"\n",
    "                            learning_rate_scheduler_params = None, # TODO add base class for learning rate scheduler for typing\n",
    "                            model_params = {\"relu_output\": False}, #\n",
    "                            dataloader_params={\"batch_size\": 32, \"shuffle\": True},\n",
    "                            device = \"cpu\", # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "run_experiment(agent, environment, 2, run_id = \"test\") # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorDLAgent(NVBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
    "    based on a deep learning model. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                learning_rate_scheduler_params: Dict | None = None,  \n",
    "                \n",
    "                # parameters in yaml file\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                model_params: dict | None = None,  # default: {\"hidden_layers\": [64, 64], \"drop_prob\": 0.0, \"batch_norm\": False, \"relu_output\": False}\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                dataset_params: dict | None = None, # parameters needed to convert the dataloader to a torch dataset\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                agent_name: str | None = \"DLNV\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\",\n",
    "                ):\n",
    "\n",
    "        # Handle mutable defaults unique to this class\n",
    "        default_model_params = {\n",
    "            \"hidden_layers\": [64, 64],\n",
    "            \"drop_prob\": 0.0,\n",
    "            \"batch_norm\": False,\n",
    "            \"relu_output\": False\n",
    "            }\n",
    "\n",
    "        self.model_params = self.update_model_params(default_model_params, model_params or {})\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            optimizer_params=optimizer_params,\n",
    "            learning_rate_scheduler_params=learning_rate_scheduler_params,\n",
    "            dataloader_params=dataloader_params,\n",
    "            dataset_params=dataset_params,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )\n",
    "        \n",
    "    def set_model(self, input_shape, output_shape):\n",
    "        \n",
    "        \"\"\"Set the model for the agent to an MLP\"\"\"\n",
    "\n",
    "        # flatten time dim of input\n",
    "        print(\"input shape\", input_shape)\n",
    "        input_size = np.prod(input_shape)\n",
    "        output_size = output_shape[0]\n",
    "\n",
    "        from ddopnew.approximators import MLP\n",
    "        self.model = MLP(input_size=input_size, output_size=output_size, **self.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L469){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NewsvendorDLAgent\n",
       "\n",
       ">      NewsvendorDLAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                         dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                         cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                         co:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                         input_shape:Tuple, output_shape:Tuple,\n",
       ">                         learning_rate_scheduler_params:Optional[Dict]=None,\n",
       ">                         optimizer_params:dict|None=None,\n",
       ">                         model_params:dict|None=None,\n",
       ">                         dataloader_params:dict|None=None,\n",
       ">                         dataset_params:dict|None=None, device:str='cpu',\n",
       ">                         obsprocessors:list|None=None,\n",
       ">                         agent_name:str|None='DLNV', test_batch_size:int=1024,\n",
       ">                         receive_batch_dim:bool=False, loss_function:Literal['q\n",
       ">                         uantile','pinball']='quantile')\n",
       "\n",
       "*Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
       "based on a deep learning model.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| learning_rate_scheduler_params | Optional | None |  |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| model_params | dict \\| None | None | default: {\"hidden_layers\": [64, 64], \"drop_prob\": 0.0, \"batch_norm\": False, \"relu_output\": False} |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| dataset_params | dict \\| None | None | parameters needed to convert the dataloader to a torch dataset |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| agent_name | str \\| None | DLNV |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L469){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## NewsvendorDLAgent\n",
       "\n",
       ">      NewsvendorDLAgent (environment_info:ddopnew.utils.MDPInfo,\n",
       ">                         dataloader:ddopnew.dataloaders.base.BaseDataLoader,\n",
       ">                         cu:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                         co:numpy.ndarray|ddopnew.utils.Parameter,\n",
       ">                         input_shape:Tuple, output_shape:Tuple,\n",
       ">                         learning_rate_scheduler_params:Optional[Dict]=None,\n",
       ">                         optimizer_params:dict|None=None,\n",
       ">                         model_params:dict|None=None,\n",
       ">                         dataloader_params:dict|None=None,\n",
       ">                         dataset_params:dict|None=None, device:str='cpu',\n",
       ">                         obsprocessors:list|None=None,\n",
       ">                         agent_name:str|None='DLNV', test_batch_size:int=1024,\n",
       ">                         receive_batch_dim:bool=False, loss_function:Literal['q\n",
       ">                         uantile','pinball']='quantile')\n",
       "\n",
       "*Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
       "based on a deep learning model.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| environment_info | MDPInfo |  |  |\n",
       "| dataloader | BaseDataLoader |  |  |\n",
       "| cu | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| co | numpy.ndarray \\| ddopnew.utils.Parameter |  |  |\n",
       "| input_shape | Tuple |  |  |\n",
       "| output_shape | Tuple |  |  |\n",
       "| learning_rate_scheduler_params | Optional | None |  |\n",
       "| optimizer_params | dict \\| None | None | default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0} |\n",
       "| model_params | dict \\| None | None | default: {\"hidden_layers\": [64, 64], \"drop_prob\": 0.0, \"batch_norm\": False, \"relu_output\": False} |\n",
       "| dataloader_params | dict \\| None | None | default: {\"batch_size\": 32, \"shuffle\": True} |\n",
       "| dataset_params | dict \\| None | None | parameters needed to convert the dataloader to a torch dataset |\n",
       "| device | str | cpu | \"cuda\" or \"cpu\" |\n",
       "| obsprocessors | list \\| None | None | default: [] |\n",
       "| agent_name | str \\| None | DLNV |  |\n",
       "| test_batch_size | int | 1024 |  |\n",
       "| receive_batch_dim | bool | False |  |\n",
       "| loss_function | Literal | quantile |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NewsvendorDLAgent, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further information:   \n",
    "   \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    .. [1] Afshin Oroojlooyjadid, Lawrence V. Snyder, Martin Takáˇc,\n",
    "            \"Applying Deep Learning to the Newsvendor Problem\", 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L528){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NewsvendorDLAgent.set_model\n",
       "\n",
       ">      NewsvendorDLAgent.set_model (input_shape, output_shape)\n",
       "\n",
       "*Set the model for the agent to an MLP*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/opimwue/ddopnew/blob/main/ddopnew/agents/newsvendor/erm.py#L528){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NewsvendorDLAgent.set_model\n",
       "\n",
       ">      NewsvendorDLAgent.set_model (input_shape, output_shape)\n",
       "\n",
       "*Set the model for the agent to an MLP*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NewsvendorDLAgent.set_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Network architecture:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape (2,)\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MLP                                      [1, 1]                    --\n",
      "├─Sequential: 1-1                        [1, 1]                    --\n",
      "│    └─Linear: 2-1                       [1, 64]                   192\n",
      "│    └─ReLU: 2-2                         [1, 64]                   --\n",
      "│    └─Dropout: 2-3                      [1, 64]                   --\n",
      "│    └─Linear: 2-4                       [1, 64]                   4,160\n",
      "│    └─ReLU: 2-5                         [1, 64]                   --\n",
      "│    └─Dropout: 2-6                      [1, 64]                   --\n",
      "│    └─Linear: 2-7                       [1, 1]                    65\n",
      "│    └─Identity: 2-8                     [1, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 4,417\n",
      "Trainable params: 4,417\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting experiment\n",
      "INFO:root:Initial evaluation: R=-28.597681257306558, J=-27.220163205759015\n",
      "INFO:root:Starting training with epochs fit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-26.989019584755987 -25.663410208519274\n",
      "Experiment directory: results/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 1103.13it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 1337.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 30.57it/s]\n",
      "INFO:root:Finished training with epochs fit\n",
      "INFO:root:Evaluation after training: R=-16.16906666127553, J=-15.414261035575393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.93922111198494 -14.254037954757864\n"
     ]
    }
   ],
   "source": [
    "dataloader = XYDataLoader(X, Y, val_index_start, test_index_start)\n",
    "\n",
    "environment = NewsvendorEnv(\n",
    "    dataloader = dataloader,\n",
    "    underage_cost = 0.42857,\n",
    "    overage_cost = 1.0,\n",
    "    gamma = 0.999,\n",
    "    horizon_train = 365,\n",
    ")\n",
    "\n",
    "model_params = {\n",
    "    \"hidden_layers\": [64, 64],\n",
    "}\n",
    "\n",
    "agent = NewsvendorDLAgent(environment.mdp_info,\n",
    "                            dataloader,\n",
    "                            cu=np.array([0.42857]),\n",
    "                            co=np.array([1.0]),\n",
    "                            input_shape=(2,),\n",
    "                            output_shape=(1,),\n",
    "                            optimizer_params= {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}, # other optimizers: \"SGD\", \"RMSprop\"\n",
    "                            learning_rate_scheduler_params = None, # TODO add base class for learning rate scheduler for typing\n",
    "                            model_params = model_params, #\n",
    "                            dataloader_params={\"batch_size\": 32, \"shuffle\": True},\n",
    "                            device = \"cpu\" # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)\n",
    "\n",
    "run_experiment(agent, environment, 2, run_id = \"test\") # fit agent via run_experiment function\n",
    "\n",
    "environment.test()\n",
    "agent.eval()\n",
    "\n",
    "R, J = test_agent(agent, environment)\n",
    "\n",
    "print(R, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseMetaAgent():\n",
    "\n",
    "    def set_meta_dataloader(\n",
    "        self, \n",
    "        dataloader: BaseDataLoader,\n",
    "        dataset_params: dict, # parameters needed to convert the dataloader to a torch dataset\n",
    "        dataloader_params: dict, # dict with keys: batch_size, shuffle\n",
    "        ) -> None:\n",
    "\n",
    "        \"\"\" \"\"\"\n",
    "\n",
    "        dataset = DatasetWrapperMeta(dataloader, **dataset_params)\n",
    "\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset, **dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorlERMMetaAgent(NewsvendorlERMAgent, BaseMetaAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
    "    based on a linear (regression) model. In addition to the features, the agent\n",
    "    also gets the sl as input to be able to forecast the optimal order quantity\n",
    "    for different sl values. Depending on the training pipeline, this model can be \n",
    "    adapted to become a full meta-learning algorithm cross products and cross sls.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "\n",
    "                # Parameters for lERM agent\n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                learning_rate_scheduler_params = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "                model_params: dict | None = None,  # default: {\"relu_output\": False}\n",
    "                dataset_params: dict | None = None, # parameters needed to convert the dataloader to a torch dataset\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                device: str = \"cpu\",  # \"cuda\" or \"cpu\"\n",
    "                agent_name: str | None = \"lERMMeta\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\",\n",
    "                ):\n",
    "\n",
    "        self.set_meta_dataloader(dataloader, dataset_params, dataloader_params)\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            optimizer_params=optimizer_params,\n",
    "            learning_rate_scheduler_params=learning_rate_scheduler_params,\n",
    "            model_params=model_params,\n",
    "            dataloader_params=dataloader_params,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim = receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorDLMetaAgent(NewsvendorDLAgent, BaseMetaAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
    "    based on a Neural Network. In addition to the features, the agent\n",
    "    also gets the sl as input to be able to forecast the optimal order quantity\n",
    "    for different sl values. Depending on the training pipeline, this model can be \n",
    "    adapted to become a full meta-learning algorithm cross products and cross sls.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                learning_rate_scheduler_params = None,  # TODO: add base class for learning rate scheduler for typing\n",
    "                \n",
    "                # parameters in yaml file\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                model_params: dict | None = None,  # default: {\"hidden_layers\": [64, 64], \"drop_prob\": 0.0, \"batch_norm\": False, \"relu_output\": False}\n",
    "                dataset_params: dict | None = None, # parameters needed to convert the dataloader to a torch dataset\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                agent_name: str | None = \"DLNV\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\",\n",
    "                ):\n",
    "\n",
    "        self.set_meta_dataloader(dataloader, dataset_params, dataloader_params)\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            learning_rate_scheduler_params=learning_rate_scheduler_params,\n",
    "\n",
    "            optimizer_params=optimizer_params,\n",
    "            model_params=model_params,\n",
    "            dataloader_params=dataloader_params,\n",
    "            device=device,\n",
    "\n",
    "            obsprocessors=obsprocessors,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorDLTransformerAgent(NVBaseAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
    "    based on a deep learning model with a Transformer architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                learning_rate_scheduler_params: Dict | None = None,\n",
    "                \n",
    "                # parameters in yaml file\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                model_params: dict | None = None,  # default: {\"max_context_length\": 128, \"n_layer\": 3, \"n_head\": 8, \"n_embd_per_head\": 32, \"rope_scaling\": None, \"min_multiple\": 256, \"gating\": True, \"drop_prob\": 0.0, \"final_activation\": \"identity\"}\n",
    "                dataset_params: dict | None = None, # parameters needed to convert the dataloader to a torch dataset\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                agent_name: str | None = \"DLNV\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\",\n",
    "                ):\n",
    "\n",
    "        # Handle mutable defaults unique to this class\n",
    "        default_model_params = {\n",
    "            \"max_context_length\": 128,\n",
    "            \"n_layer\": 3,\n",
    "            \"n_head\": 8,\n",
    "            \"n_embd_per_head\": 32,\n",
    "            \"rope_scaling\": None,\n",
    "\n",
    "            \"min_multiple\": 256,\n",
    "            \"gating\": True,\n",
    "\n",
    "            \"drop_prob\": 0.0,\n",
    "            \"final_activation\": \"identity\",\n",
    "            }\n",
    "        \n",
    "        self.model_params = self.update_model_params(default_model_params, model_params or {})\n",
    "\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            optimizer_params=optimizer_params,\n",
    "            learning_rate_scheduler_params=learning_rate_scheduler_params,\n",
    "            dataset_params=dataset_params,\n",
    "            dataloader_params=dataloader_params,\n",
    "            obsprocessors=obsprocessors,\n",
    "            device=device,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )\n",
    "         \n",
    "    def set_model(self, input_shape, output_shape):\n",
    "        \n",
    "        \"\"\"Set the model for the agent to an MLP\"\"\"\n",
    "\n",
    "        if len(input_shape) == 1:\n",
    "            raise ValueError(\"Input shape must be at least 2D for Transformer model\")\n",
    "\n",
    "        output_size = output_shape[0]\n",
    "\n",
    "        from ddopnew.approximators import Transformer\n",
    "        self.model = Transformer(input_size=input_shape, output_size=output_size, **self.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NewsvendorDLTransformerMetaAgent(NewsvendorDLTransformerAgent, BaseMetaAgent):\n",
    "\n",
    "    \"\"\"\n",
    "    Newsvendor agent implementing Empirical Risk Minimization (ERM) approach \n",
    "    based on a Neural Network using the attention mechanism. In addition to the features,\n",
    "    the agent also gets the sl as input to be able to forecast the optimal order quantity\n",
    "    for different sl values. Depending on the training pipeline, this model can be \n",
    "    adapted to become a full meta-learning algorithm cross products and cross sls.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                environment_info: MDPInfo,\n",
    "                dataloader: BaseDataLoader,\n",
    "                cu: np.ndarray | Parameter,\n",
    "                co: np.ndarray | Parameter,\n",
    "                input_shape: Tuple,\n",
    "                output_shape: Tuple,\n",
    "                learning_rate_scheduler_params: Dict | None = None, \n",
    "                \n",
    "                # parameters in yaml file\n",
    "                optimizer_params: dict | None = None,  # default: {\"optimizer\": \"Adam\", \"lr\": 0.01, \"weight_decay\": 0.0}\n",
    "                model_params: dict | None = None,  # default: {\"hidden_layers\": [64, 64], \"drop_prob\": 0.0, \"batch_norm\": False, \"relu_output\": False}\n",
    "                dataset_params: dict | None = None, # parameters needed to convert the dataloader to a torch dataset\n",
    "                dataloader_params: dict | None = None,  # default: {\"batch_size\": 32, \"shuffle\": True}\n",
    "                device: str = \"cpu\", # \"cuda\" or \"cpu\"\n",
    "\n",
    "                obsprocessors: list | None = None,  # default: []\n",
    "                agent_name: str | None = \"DLNV\",\n",
    "                test_batch_size: int = 1024,\n",
    "                receive_batch_dim: bool = False,\n",
    "                loss_function: Literal[\"quantile\", \"pinball\"] = \"quantile\",\n",
    "                ):\n",
    "\n",
    "        self.set_meta_dataloader(dataloader, dataset_params, dataloader_params)\n",
    "\n",
    "        super().__init__(\n",
    "            environment_info=environment_info,\n",
    "            dataloader=dataloader,\n",
    "            cu=cu,\n",
    "            co=co,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape,\n",
    "            learning_rate_scheduler_params=learning_rate_scheduler_params,\n",
    "\n",
    "            optimizer_params=optimizer_params,\n",
    "            model_params=model_params,\n",
    "            dataloader_params=dataloader_params,\n",
    "            device=device,\n",
    "\n",
    "            obsprocessors=obsprocessors,\n",
    "            agent_name=agent_name,\n",
    "            test_batch_size=test_batch_size,\n",
    "            receive_batch_dim=receive_batch_dim,\n",
    "            loss_function=loss_function,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
