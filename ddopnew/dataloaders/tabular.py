# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/10_dataloaders/12_tabular_dataloaders.ipynb.

# %% auto 0
__all__ = ['XYDataLoader', 'MultiShapeLoader']

# %% ../../nbs/10_dataloaders/12_tabular_dataloaders.ipynb 3
import logging
logging.basicConfig(level=logging.INFO)

import numpy as np
from abc import ABC, abstractmethod
from typing import Union, Tuple, List
import pandas as pd

from .base import BaseDataLoader

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# %% ../../nbs/10_dataloaders/12_tabular_dataloaders.ipynb 4
class XYDataLoader(BaseDataLoader):

    """

    A class for datasets with the typicall X, Y structure. Both X
    and Y are numpy arrays. X may be of shape (datapoints, features) or (datapoints, sequence_length, features) 
    if lag features are used. The prep_lag_features can be used to create those lag features. Y is of shape
    (datapoints, units).

    """
    
    def __init__(self,
        X: np.ndarray,
        Y: np.ndarray,
        val_index_start: Union[int, None] = None, 
        test_index_start: Union[int, None] = None, 
        lag_window_params: Union[dict] = None, # default: {'lag_window': 0, 'include_y': False, 'pre_calc': False}
        normalize_features: Union[dict] = None, # default: {'normalize': True, 'ignore_one_hot': True}
    ):

        self.X = X
        self.Y = Y

        self.val_index_start = val_index_start
        self.test_index_start = test_index_start

        # train index ends either at the start of the validation set, the start of the test set or at the end of the dataset
        if self.val_index_start is not None:
            self.train_index_end = self.val_index_start-1
        elif self.test_index_start is not None:
            self.train_index_end = self.test_index_start-1
        else:
            self.train_index_end = len(Y)-1

        self.dataset_type = "train"

        normalize_features = normalize_features or {'normalize': True, 'ignore_one_hot': True}
        lag_window_params = lag_window_params or {'lag_window': 0, 'include_y': False, 'pre_calc': False}

        self.normalize_features(**normalize_features, initial_normalization=True)
        self.prep_lag_features(**lag_window_params)

        # X must at least have datapoint and feature dimension
        if len(X.shape) == 1:
            self.X = X.reshape(-1, 1)
        
        # Y must have at least datapoint and unit dimension (even if only one unit is present)
        if len(Y.shape) == 1:
            self.Y = Y.reshape(-1, 1)

        assert len(X) == len(Y), 'X and Y must have the same length'

        self.num_units = Y.shape[1] # shape 0 is alsways time, shape 1 is the number of units (e.g., SKUs)

        super().__init__()

    def normalize_features(self,
        normalize: bool = True,
        ignore_one_hot: bool = True,
        initial_normalization=False # Flag if it is set before having added lag features
        ):

        """
        Normalize features using a standard scaler. If ignore_one_hot is true, one-hot encoded features are not normalized.

        """

        if normalize:

            scaler = StandardScaler()

            if initial_normalization:

                if len(self.X.shape) == 3:
                    raise ValueError('Normalization not possible with lag features. Please set initial_normalization=False')
            
                scaler.fit(self.X[:self.train_index_end+1]) # +1 to include the last training point
                scaler.transform(self.X)

                if initial_normalization:
                    return
                else:
                    raise NotImplementedError('Normalization after lag features have been set not implemented yet')

                    # Idea:
                        # remove time dimension
                        # normalize features
                        # add time_dimension back
                    # Problem:
                        # usage of prep_lag_features needs to ensure y is not added a second time

    def prep_lag_features(self,
        lag_window: int = 0, # length of the lage window
        include_y: bool = False, # if lag demand shall be included as feature
        pre_calc: bool = False # if all lags are pre-calculated for the entire dataset
        ):

        """
        Create lag feature for the dataset. If "inlcude_y" is true, then a lag-1 of of the target variable is added as a feature.
        If lag-window is > 0, the lag features are added as middle dimension to X. Note that this, e.g., means that with a lag
        window of 1, the data will include 2 time steps, the current features including lag-1 demand and the lag-1 features
        including lag-2 demand. If pre-calc is true, all these calculations are performed on the entire dataset reduce
        computation time later on at the expense of increases memory usage. 

        """
        # to be discussed: Do we need option to only provide lag demand wihtout lag features?
        self.lag_window = lag_window
        self.pre_calc = pre_calc
        self.include_y = include_y
        
        if self.pre_calc:
            if self.include_y:
                # add additional column to X with demand shifted by 1
                self.X = np.concatenate((self.X, np.roll(self.Y, 1, axis=0)), axis=1)
                self.X = self.X[1:] # remove first row
                self.Y = self.Y[1:] # remove first row
                
                self.val_index_start = self.val_index_start-1
                self.test_index_start = self.test_index_start-1
                self.train_index_end  = self.train_index_end-1
        
            if self.lag_window is not None and self.lag_window > 0:

                # add lag features as dimention 2 to X (making it dimension (datapoints, sequence_length, features))
                X_lag = np.zeros((self.X.shape[0], self.lag_window+1, self.X.shape[1]))
                for i in range(self.lag_window+1):
                    if i == 0:
                        features = self.X
                    else:    
                        features = self.X[:-i, :]
                    X_lag[i:, self.lag_window-i, :] = features
                self.X = X_lag[self.lag_window:]
                self.Y = self.Y[self.lag_window:]

                self.val_index_start = self.val_index_start-self.lag_window
                self.test_index_start = self.test_index_start-self.lag_window
                self.train_index_end  = self.train_index_end-self.lag_window

        else:
            self.lag_window = None
            self.include_y = False
            # add time dimension to X

    def update_lag_features(self,
        lag_window: int,
        ):

        """ Update lag window parameters for dataloader object that is already initialized """

        raise NotImplementedError('Not implemented yet')

        # Problem: updating lag_features naively would shorten the dataset each time it is called

    def __getitem__(self, idx): 

        """ get item by index, depending on the dataset type (train, val, test)"""

        if self.dataset_type == "train":
            if idx > self.train_index_end:
                raise IndexError(f'index {idx} out of range{self.train_index_end}')
            idx = idx

        elif self.dataset_type == "val":
            idx = idx + self.val_index_start
            
            if idx >= self.test_index_start:
                raise IndexError(f'index{idx} out of range{self.test_index_start}')
            
        elif self.dataset_type == "test":
            idx = idx + self.test_index_start
            
            if idx >= len(self.X):
                raise IndexError(f'index{idx} out of range{len(self.X)}')
        
        else:
            raise ValueError('dataset_type not set')

        return self.X[idx], self.Y[idx]

    def __len__(self):
        return len(self.X)
    
    @property
    def X_shape(self):
        return self.X.shape
    
    @property
    def Y_shape(self):
        return self.Y.shape

    @property
    def len_train(self):
        return self.train_index_end+1

    @property
    def len_val(self):
        if self.val_index_start is None:
            raise ValueError('no validation set defined')
        return self.test_index_start-self.val_index_start

    @property
    def len_test(self):
        if self.test_index_start is None:
            raise ValueError('no test set defined')
        return len(self.Y)-self.test_index_start

    def get_all_X(self,
                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'
                ): 

        """
        Returns the entire features dataset.
        Return either the train, val, test, or all data.
        """

        if dataset_type == 'train':
            return self.X[:self.val_index_start].copy() if self.X is not None else None
        elif dataset_type == 'val':
            return self.X[self.val_index_start:self.test_index_start].copy() if self.X is not None else None
        elif dataset_type == 'test':
            return self.X[self.test_index_start:].copy() if self.X is not None else None
        elif dataset_type == 'all':
            return self.X.copy() if self.X is not None else None
        else:
            raise ValueError('dataset_type not recognized')

    def get_all_Y(self,
                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'
                ): 

        """
        Returns the entire target dataset.
        Return either the train, val, test, or all data.
        """

        if dataset_type == 'train':
            return self.Y[:self.val_index_start].copy() if self.Y is not None else None
        elif dataset_type == 'val':
            return self.Y[self.val_index_start:self.test_index_start].copy() if self.Y is not None else None
        elif dataset_type == 'test':
            return self.Y[self.test_index_start:].copy() if self.Y is not None else None
        elif dataset_type == 'all':
            return self.Y.copy() if self.Y is not None else None
        else:
            raise ValueError('dataset_type not recognized')
        

# %% ../../nbs/10_dataloaders/12_tabular_dataloaders.ipynb 18
class MultiShapeLoader(BaseDataLoader):

    """
    A class designed for comlex datasets with mutlipe feature types. The class is more
    memory-efficient than the XYDataLoader, as it separate the storeage of SKU-specific
    feature, time-specific features, and time-SKU-specific features. The class works generically
    as long as those feature classes are provided during pre-processing. The class is designed 
    to handle classic learning, but able to work in a meta-learning pipeline where no SKU-dimension
    is present and the model needs to make prediction on SKU-time level without knowhing the
    specific SKU.
    """
    
    def __init__(self,
        demand: pd.DataFrame, # Demand data of shape time x SKU
        SKU_features: pd.DataFrame, # Features constant over time of shape SKU x SKU_features
        time_features: pd.DataFrame, # Features constant over SKU of shape time x time_features
        time_SKU_features: pd.DataFrame, # Features varying over time and SKU of shape time x (time_SKU_features*SKU) with double index
        mask: pd.DataFrame, # Mask of shape time x SKU telling which SKUs are available at which time (can be used as mask during trainig or added to features)
        
        val_index_start: Union[int, None] = None, # Validation index start on the time dimension
        test_index_start: Union[int, None] = None, # Test index start on the time dimension
        in_sample_val_test_SKUs: List = None, # SKUs in the training set to be used for validation and testing, out-of-sample w.r.t. time dimension
        out_of_sample_val_SKUs: List = None, # SKUs to be hold-out for validation (can be same as test if no validation on out-of-sample SKUs required)
        out_of_sample_test_SKUs: List = None, # SKUs to be hold-out for testing
        lag_window_params: Union[dict] = None, # default: {'lag_window': 0, 'include_y': False, 'pre_calc': True}
        normalize_features: Union[dict] = None, # default: {'normalize': True, 'ignore_one_hot': True}
        engineered_SKU_features: Union[dict] = None, # default: ["mean_demand", "std_demand", "kurtosis_demand", "skewness_demand", "percentile_10_demand", "percentile_30_demand", "median_demand", "percentile_70_demand", "percentile_90_demand", "inter_quartile_range"]
        include_non_available: bool = False, # if timestep/SKU combination where the SKU was not available for sale shall be included. If included, it will be used as feature, otherwise as mask.
        train_subset: int = False ,# if only a subset of SKUs is used for training. Will always contain in_sample_val_test_SKUs and then fills the rest with random SKUs
        train_subset_SKUs: List = None, # if train_subset is set, specific SKUs can be provided
        SKU_as_batch: bool = False # if get_index during training gets an index for the time dimension (in batch) or from time*SKU dimension
    ):

        normalize_features = normalize_features or {'normalize': True, 'ignore_one_hot': True}
        lag_window_params = lag_window_params or {'lag_window': 0, 'include_y': False, 'pre_calc': False}
        self.lag_window_params = lag_window_params
        self.train_index_start = self.lag_window_params["lag_window"] # start index for training data
        self.train_index_start += self.lag_window_params["include_y"] # if lag demand is included as feature need one more timestep
        engineered_SKU_features = engineered_SKU_features or ["mean_demand", "std_demand", "kurtosis_demand", "skewness_demand", "percentile_10_demand", "percentile_30_demand", "median_demand", "percentile_70_demand", "percentile_90_demand", "inter_quartile_range"]

        self.demand = demand
        self.SKU_features = SKU_features
        self.time_features = time_features
        self.time_SKU_features = time_SKU_features
        self.mask = mask
        self.num_time_SKU_features = len(self.time_SKU_features.columns.get_level_values(0).unique())
        self.num_units = len(self.demand.columns)
        self.num_features = len(self.SKU_features.columns) + len(self.time_features.columns) + self.num_time_SKU_features
        if engineered_SKU_features is not None:
            self.num_features += len(engineered_SKU_features)
        if lag_window_params["include_y"]:
            self.num_features += 1
        if include_non_available:
            self.num_features += 1

        self.normalized_in_sample_SKUs = False
        self.normalized_out_of_sample_val_SKUs = False
        self.normalized_out_of_sample_test_SKUs = False

        self.include_non_available = include_non_available
        self.train_subset = train_subset
        self.train_subset_SKUs = train_subset_SKUs
        self.SKU_as_batch = SKU_as_batch

        self.SKU_type = "in_sample" # or "out_of_sample_val" or "out_of_sample_test" # affecting the SKU-dimension
        self.dataset_type = "train" # or "val" or "test", affecting the time-dimension

        logging.info("Setting indices for validation and test set")
        self.val_index_start = val_index_start
        self.test_index_start = test_index_start

        # train index ends either at the start of the validation set, the start of the test set or at the end of the dataset
        if self.val_index_start is not None:
            self.train_index_end = self.val_index_start-1
        elif self.test_index_start is not None:
            self.train_index_end = self.test_index_start-1
        else:
            self.train_index_end = len(self.demand)-1
        
        logging.info("Setting out-of-sample SKUs")
        # print("Number of SKUs in dataset:", self.demand.shape[1])
        for sku, attr_suffix in [(out_of_sample_val_SKUs, 'val'), (out_of_sample_test_SKUs, 'test')]:
            if sku is not None:
                setattr(self, f'demand_out_of_sample_{attr_suffix}', self.demand.loc[:, sku])
                setattr(self, f'SKU_features_out_of_sample_{attr_suffix}', self.SKU_features.loc[sku])
                setattr(self, f'time_SKU_features_out_of_sample_{attr_suffix}', # here SKU are in columns on index level 2
                        self.time_SKU_features.loc[:, pd.IndexSlice[:, sku]])   
                setattr(self, f'mask_out_of_sample_{attr_suffix}', self.mask.loc[:, sku])
                # time_features are independent of SKU

                self.demand.drop(columns=sku, inplace=True)
                self.SKU_features.drop(index=sku, inplace=True)
                for single_sku in sku if isinstance(sku, list) else [sku]:
                    columns_to_drop = self.time_SKU_features.columns.get_loc_level(single_sku, level=1)
                    self.time_SKU_features.drop(columns=self.time_SKU_features.columns[columns_to_drop[0]], inplace=True)
                self.mask.drop(columns=sku, inplace=True)
        self.in_sample_val_test_SKUs = in_sample_val_test_SKUs
        
        logging.info("Identifying training SKUs")
        self.identify_train_SKUs()

        logging.info("Creating engineered SKU features for training data")
        engineered_SKU_features = self.build_engineered_SKU_features(engineered_SKU_features, self.demand.iloc[:self.train_index_end+1]) # only for training data initially
        self.SKU_features = pd.concat([self.SKU_features, engineered_SKU_features.transpose()], axis=1)
        
        logging.info("Normalizing in-sample SKU features (based on training timesteps)")
        self.normalize_features_in_sample(**normalize_features, initial_normalization=True)

        # store row and column indices of demand, SKU_features time_features mask and then convert to numpy array

        self.demand_indices = self.save_indices(self.demand)
        self.SKU_features_indices = self.save_indices(self.SKU_features)
        self.time_features_indices = self.save_indices(self.time_features)
        self.time_SKU_features_indices = self.save_indices(self.time_SKU_features)
        self.mask_indices = self.save_indices(self.mask)

        self.demand = self.demand.to_numpy()
        self.SKU_features = self.SKU_features.to_numpy()
        self.time_features = self.time_features.to_numpy()
        self.time_SKU_features = self.time_SKU_features.to_numpy()
        self.mask = self.mask.to_numpy()

        self.len_train_time = self.train_index_end-self.train_index_start+1
        if SKU_as_batch:
            logging.info("Creating time-SKU index for training data")
            self.sku_time_index = [(i, j) for i in range(self.train_SKUs_indices.shape[0]) for j in range(self.len_train_time)]

        super().__init__()

    def identify_train_SKUs(self):
        """ determine which SKUs are used for training, validation and testing """

        if self.train_subset:

            if self.train_subset_SKUs is not None:
                if len(self.train_subset_SKUs) != self.train_subset:
                    raise ValueError('train_subset_SKUs must have the same length as train_subset')
                train_SKUs = self.train_subset_SKUs
                # check that all train_SKUs are in demand.collumns
                if not set(train_SKUs).issubset(self.demand.columns):
                    raise ValueError('train_subset_SKUs must be a subset of all training SKUs')
                if self.in_sample_val_test_SKUs is not None:
                    if not set(self.in_sample_val_test_SKUs).issubset(train_SKUs):
                        raise ValueError('train_subset_SKUs must contain in_sample_val_test_SKUs')
            else:
                if self.in_sample_val_test_SKUs is not None and self.train_subset <= len(self.in_sample_val_test_SKUs):
                    raise ValueError('train_subset must be equal or larger than the number of in_sample_val_test_SKUs')
                train_SKUs = self.in_sample_val_test_SKUs if self.in_sample_val_test_SKUs is not None else []
                remaining_SKUs = self.demand.columns.difference(train_SKUs)
                additional_SKUs = np.random.choice(remaining_SKUs, self.train_subset-len(train_SKUs), replace=False)
                train_SKUs = np.concatenate((train_SKUs, additional_SKUs))
    
        else:
            train_SKUs = self.demand.columns # val and test SKUs have been removed before, only training SKUs remain
        
        self.train_SKUs = train_SKUs

        self.train_SKUs_indices = self.demand.columns.get_indexer(self.train_SKUs)
        if self.in_sample_val_test_SKUs is not None:
            self.in_sample_val_test_SKUs_indices = self.demand.columns.get_indexer(self.in_sample_val_test_SKUs)
        

    @staticmethod
    def build_engineered_SKU_features(engineered_SKU_features: List, demand: pd.DataFrame):

        """
        Create engineered features for each SKU
        """

        feature_names = []
        feature_values = []

        for feature in engineered_SKU_features:

            if feature == "mean_demand":
                mean_demand = demand.mean(axis=0)
            elif feature == "std_demand":
                std_demand = demand.std(axis=0)
            elif feature == "kurtosis_demand":
                kurtosis_demand = demand.kurtosis(axis=0)
            elif feature == "skewness_demand":
                skewness_demand = demand.skew(axis=0)
            elif feature == "percentile_10_demand":
                percentile_10_demand = demand.quantile(0.1, axis=0)
            elif feature == "percentile_30_demand":
                percentile_30_demand = demand.quantile(0.3, axis=0)
            elif feature == "median_demand":
                median_demand = demand.median(axis=0)
            elif feature == "percentile_70_demand":
                percentile_70_demand = demand.quantile(0.7, axis=0)
            elif feature == "percentile_90_demand":
                percentile_90_demand = demand.quantile(0.9, axis=0)
            elif feature == "inter_quartile_range":
                inter_quartile_range = demand.quantile(0.75, axis=0) - demand.quantile(0.25, axis=0)
            else:  
                raise ValueError(f'Feature {feature} not recognized')
            
            feature_names.append(feature)
            feature_values.append(locals()[feature])

        return pd.DataFrame(feature_values, columns=demand.columns, index=feature_names)
    
    def normalize_features_in_sample(self,
        normalize: bool = True,
        ignore_one_hot: bool = True,
        initial_normalization = False # Flag if it is set before having added lag features
        ):

        """
        Normalize features using a standard scaler. If ignore_one_hot is true, one-hot encoded features are not normalized.
        """

        if normalize:

            if self.normalized_in_sample_SKUs:
                raise ValueError('Features already normalized')

            self.scaler_demand = StandardScaler()
            self.scaler_SKU_features = StandardScaler()
            self.scaler_time_features = StandardScaler()
            self.scaler_time_SKU_features= [StandardScaler() for _ in range(self.num_time_SKU_features)]

            if initial_normalization:
            
                logging.info("--Normalizing demand")
                self.scaler_demand.fit(self.demand[:self.train_index_end+1])
                transformed_demand = self.scaler_demand.transform(self.demand)
                self.demand.iloc[:,:] = transformed_demand

                logging.info("--Normalizing SKU features")
                continuous_features = [col for col in self.SKU_features.columns if not self.is_one_hot(self.SKU_features[col])]
                if len(continuous_features) > 0:
                    self.scaler_SKU_features.fit(self.SKU_features[continuous_features]) # SKU features are already calculated based on training index
                    transformed_SKU_features = self.scaler_SKU_features.transform(self.SKU_features[continuous_features])
                    self.SKU_features[continuous_features] = transformed_SKU_features

                logging.info("--Normalizing time features")
                continuous_features = [col for col in self.time_features.columns if not self.is_one_hot(self.time_features[col])]
                if len(continuous_features) > 0:
                    self.scaler_time_features.fit(self.time_features.loc[:self.train_index_end+1,continuous_features]) # each column to be normalized
                    transformed_time_features = self.scaler_time_features.transform(self.time_features.loc[:,continuous_features])
                    self.time_features.loc[:,continuous_features] = transformed_time_features

                logging.info("--Normalizing time-SKU features")
                # Normalize time-SKU features (double-indexed)
                for i, feature in enumerate(self.time_SKU_features.columns.get_level_values(0).unique()):
                    # Select all columns corresponding to the current feature in level 0
                    feature_df = self.time_SKU_features.xs(key=feature, axis=1, level=0)
                    if not self.is_one_hot_across_skus(feature_df):
                        self.scaler_time_SKU_features[i].fit(feature_df[:self.train_index_end+1])
                        transformed_feature_df = self.scaler_time_SKU_features[i].transform(feature_df)
                        self.time_SKU_features.loc[:, (feature, slice(None))] = transformed_feature_df
            
                self.normalized_in_sample_SKUs = True

            else:
                raise NotImplementedError('Training data can only normalized during initialization - later normlization not implemented yet')

    def update_lag_features(self,
        lag_window: int,
        ):

        """ Update lag window parameters for dataloader object that is already initialized """

        raise NotImplementedError('Not implemented yet')

        # Problem: updating lag_features naively would shorten the dataset each time it is called

    def get_time_SKU_idx(self, idx):

        """ get time and SKU index by index, depending on the dataset type (train, val, test)"""

        if self.dataset_type == "train":

            if self.SKU_as_batch:
                if idx >= len(self.sku_time_index):
                    raise IndexError(f'index {idx} out of range{len(self.sku_time_index)}')
                idx_sku, idx_time, = self.sku_time_index[idx]
                idx_skus = [idx_sku]

                print(idx_time, idx_sku)

            else:
                if idx+self.train_index_start > self.train_index_end:
                    raise IndexError(f'index {idx} out of range{self.train_index_end-self.train_index_start}')
                idx_skus = self.train_SKUs_indices
                idx_time = idx
            idx_time += self.train_index_start

        elif self.dataset_type == "val":
            idx_time = idx + self.val_index_start
            if self.in_sample_val_test_SKUs is not None:
                idx_skus = self.in_sample_val_test_SKUs_indices
            else:
                idx_skus = self.train_SKUs_indices
            
            if idx >= self.test_index_start:
                raise IndexError(f'index{idx} out of range{self.test_index_start}')
        elif self.dataset_type == "test":
            idx_time = idx + self.test_index_start
            if self.in_sample_val_test_SKUs is not None:
                idx_skus = self.in_sample_val_test_SKUs_indices
            else:
                idx_skus = self.train_SKUs_indices
            
            if idx >= len(self.demand):
                raise IndexError(f'index{idx} out of range{len(self.demand)}')
        else:
            raise ValueError('dataset_type not set')

        return idx_time, idx_skus

    def __getitem__(self, idx):

        """ get item by index, depending on the dataset type (train, val, test)"""

        lag_window = self.lag_window_params["lag_window"]
        include_y = self.lag_window_params["include_y"]

        idx_time, idx_skus = self.get_time_SKU_idx(idx)
        num_skus = len(idx_skus)

        print(idx_time, idx_skus)
        demand = self.demand[idx_time, idx_skus]

        item = np.empty((1,lag_window+1, self.num_features, num_skus))

        for t in range(lag_window+1):

            item_t = np.empty((1, self.num_features, num_skus))
            idx_time_t = idx_time-t

            if include_y:
                assert idx_time_t-1 >= 0
                lag_demand = self.demand[idx_time_t-1, idx_skus]

            SKU_features = self.SKU_features[idx_skus].transpose()
            time_features = self.time_features[idx_time_t]
            # repeate time_SKU_features for all SKUs with SKU as last dimension 
            time_features = np.repeat(time_features[:, np.newaxis], num_skus, axis=1)

            time_SKU_features = np.empty((self.num_time_SKU_features, num_skus))
            for i, idx_sku in enumerate(idx_skus):
                SKU_indices = [len(self.train_SKUs)*i+idx_sku for i in range(self.num_time_SKU_features)]
                time_SKU_features[:,i] = self.time_SKU_features[idx_time_t, SKU_indices]
            len_SKU_features = len(SKU_features)
            len_time_features = len(time_features)
            
            item_t[:,:len(SKU_features),:] = SKU_features
            item_t[:,len_SKU_features:(len_SKU_features+len_time_features),:] = time_features
            item_t[:,(len_SKU_features+len_time_features):(len_SKU_features+len_time_features+self.num_time_SKU_features),:] = time_SKU_features

            if self.include_non_available:
                if self.include_y:
                    item_t[:,-2,:] = lag_demand
                item_t[:,-1,:] = self.mask[idx_time,idx_skus]
            else:
                item_t[:,-1,:] = lag_demand

            item[:,t,:,:] = item_t
        
        if lag_window == 0:
            if item.shape[1] == 1:
                item = item.squeeze(1) 
            else:
                raise ValueError('Lag window is 0, but item has more than one time dimension')
        if self.SKU_as_batch:
            if item.shape[-1] == 1:
                item = item.squeeze(-1) 
            else:
                raise ValueError('SKU as batch, but item has more than one SKU dimension')
    
        return item, demand

    def __len__(self):
        return len(self.demand)
    
    @property
    def X_shape(self):
        return (len_train_time, self.num_features, self.num_units)
    
    @property
    def Y_shape(self):
        return (len_train_time, self.num_units)

    @property
    def len_train(self):
        if self.SKU_as_batch:
            return len(self.sku_time_index)
        else:
            return self.len_train_time

    @property
    def len_val(self):
        if self.val_index_start is None:
            raise ValueError('no validation set defined')
        return self.test_index_start-self.val_index_start

    @property
    def len_test(self):
        if self.test_index_start is None:
            raise ValueError('no test set defined')
        return len(self.demand)-self.test_index_start

    def get_all_X(self,
                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'
                ): 

        raise NotImplementedError('Not implemented yet')

        # """
        # Returns the entire features dataset.
        # Return either the train, val, test, or all data.
        # """

        # if dataset_type == 'train':
        #     return self.X[:self.val_index_start].copy() if self.X is not None else None
        # elif dataset_type == 'val':
        #     return self.X[self.val_index_start:self.test_index_start].copy() if self.X is not None else None
        # elif dataset_type == 'test':
        #     return self.X[self.test_index_start:].copy() if self.X is not None else None
        # elif dataset_type == 'all':
        #     return self.X.copy() if self.X is not None else None
        # else:
        #     raise ValueError('dataset_type not recognized')

    def get_all_Y(self,
                dataset_type: str = 'train' # can be 'train', 'val', 'test', 'all'
                ): 

        # """
        # Returns the entire target dataset.
        # Return either the train, val, test, or all data.
        # """

        raise NotImplementedError('Not implemented yet')

        # if dataset_type == 'train':
        #     return self.Y[:self.val_index_start].copy() if self.Y is not None else None
        # elif dataset_type == 'val':
        #     return self.Y[self.val_index_start:self.test_index_start].copy() if self.Y is not None else None
        # elif dataset_type == 'test':
        #     return self.Y[self.test_index_start:].copy() if self.Y is not None else None
        # elif dataset_type == 'all':
        #     return self.Y.copy() if self.Y is not None else None
        # else:
        #     raise ValueError('dataset_type not recognized')

    @staticmethod
    def is_one_hot(column):
        return set(column.unique()) <= {0, 1}

    @staticmethod
    def is_one_hot_across_skus(feature_df):
        """
        Check if the set of unique values in a feature across all SKU_ids is {0, 1}.
        feature_df: DataFrame slice for a specific feature with SKU_ids as columns.
        """
        flattened_values = feature_df.values.flatten()
        
        # Check if the unique values in this flattened array are exactly {0, 1}
        unique_values = set(flattened_values)
        
        return unique_values <= {0, 1}

    @staticmethod
    def save_indices(df):
        """
        Saves the row and column indices of a DataFrame.
        """
        return {
            'rows': df.index,
            'columns': df.columns
        }
