# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/60_approximators/11_approximators.ipynb.

# %% auto 0
__all__ = ['LinearModel', 'MLP']

# %% ../nbs/60_approximators/11_approximators.ipynb 4
from abc import ABC, abstractmethod
from typing import Union
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import time

# %% ../nbs/60_approximators/11_approximators.ipynb 5
class LinearModel(nn.Module):
    def __init__(self, input_size, output_size, relu_output=False):
        super().__init__()
        self.l1=nn.Linear(input_size, output_size)
        if relu_output:
            self.final_activation = nn.ReLU()
        else:
            self.final_activation = nn.Identity()
            
    def forward(self, x):
        out=self.l1(x)
        out=self.final_activation(out)
        return out

# %% ../nbs/60_approximators/11_approximators.ipynb 6
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_hidden_layers=3, drop_prob=0.0, batch_norm=False, relu_output=False, ):
        super().__init__()

        # List of layers
        layers = []

        # Input layer
        layers.append(nn.Linear(input_size, hidden_size))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(p=drop_prob))
        if batch_norm:
            layers.append(nn.BatchNorm1d(hidden_size))

        # Hidden layers
        for _ in range(num_hidden_layers-1): 
            layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(p=drop_prob))
            if batch_norm:
                layers.append(nn.BatchNorm1d(hidden_size))

        # Output layer
        layers.append(nn.Linear(hidden_size, output_size))
        if relu_output:
            layers.append(nn.ReLU()) # output is non-negative
        else:
            layers.append(nn.Identity())

        # Combine layers
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)
